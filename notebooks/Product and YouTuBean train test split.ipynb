{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import os\n",
    "import sys\n",
    "import random as rn\n",
    "rn.seed(42)\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from tdparse.helper import read_config, full_path\n",
    "from tdparse.parsers import semeval_14, hu_liu\n",
    "from tdparse.data_types import TargetCollection, Target\n",
    "from tdparse import write_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtubean = semeval_14(full_path(read_config('youtubean')))\n",
    "# Product reviews are made up of three different products: 1. Computer, 2. Router, and 3. Speaker\n",
    "product_reviews_folder = full_path(read_config('product_reviews_dir'))\n",
    "speaker_reviews = semeval_14(os.path.join(product_reviews_folder, 'Speaker.xml'))\n",
    "computer_reviews = semeval_14(os.path.join(product_reviews_folder, 'Computer.xml'))\n",
    "router_reviews = semeval_14(os.path.join(product_reviews_folder, 'Router.xml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "len(computer_reviews)": "354",
     "len(router_reviews)": "307",
     "len(speaker_reviews)": "440",
     "len(youtubean)": "798",
     "len(youtubean.stored_sentiments())": "3"
    }
   },
   "source": [
    "# Creating Training and Test sets for the YouTuBean and Product reviews datasets\n",
    "We show how we created the Training and Test sets for these two datasets.\n",
    "\n",
    "The original YouTuBean dataset can be found [here](https://github.com/epochx/opinatt), which is associated with this [paper](https://www.aclweb.org/anthology/W17-5213). It contains {{len(youtubean)}} target sentiment labels and has {{len(youtubean.stored_sentiments())}} unique sentiments. We split the dataset into 70% traning and 30% test set.\n",
    "\n",
    "The product review dataset of [Liu et al.](https://www.ijcai.org/Proceedings/15/Papers/186.pdf) is made up of three different Amazon product reviews (Number of target sentiments in brackets): 1. Speaker reviews ({{len(speaker_reviews)}}), 2. Computer reviews ({{len(computer_reviews)}}), and 3. Wireless Router reviews ({{len(router_reviews)}}). The original dataset can be found [here](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets). We are going to combine these reviews into one dataset of product reviews. We do this as all of the reviews are from the same text type which are product reviews. We then split this dataset into 70% train and 30% test where each of the products datasets are reprensted equally in the train and test splits, this removes the domain adaptation required as each domain is represented in the train and test splits.\n",
    "\n",
    "## YouTuBean train and test set creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.3)\n",
    "youtubean_data = np.asarray(youtubean.data_dict())\n",
    "youtubean_targets = np.asarray(youtubean.sentiment_data())\n",
    "for train_indexs, test_indexs in splitter.split(youtubean_data, youtubean_targets):\n",
    "    youtubean_data_train = youtubean_data[train_indexs]\n",
    "    youtubean_data_test = youtubean_data[test_indexs]\n",
    "convert_to_targets = lambda data: [Target(**target) for target in data]\n",
    "youtubean_train = TargetCollection(convert_to_targets(youtubean_data_train))\n",
    "youtubean_test = TargetCollection(convert_to_targets(youtubean_data_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "youtubean.no_targets_sentiment()": "{-1: 70, 0: 504, 1: 224}",
     "youtubean.ratio_targets_sentiment()": "{-1: 0.09, 0: 0.63, 1: 0.28}",
     "youtubean_test.no_targets_sentiment()": "{-1: 21, 0: 152, 1: 67}",
     "youtubean_test.ratio_targets_sentiment()": "{-1: 0.09, 0: 0.63, 1: 0.28}",
     "youtubean_train.no_targets_sentiment()": "{-1: 49, 0: 352, 1: 157}",
     "youtubean_train.ratio_targets_sentiment()": "{-1: 0.09, 0: 0.63, 1: 0.28}"
    }
   },
   "source": [
    "The dataset has now been split with respect to the class labels so each class label is represented equally in the train and test splits which can be shown here:\n",
    "\n",
    "Train Data ratio: **{{youtubean_train.ratio_targets_sentiment()}}**\n",
    "Train Data raw values: **{{youtubean_train.no_targets_sentiment()}}**\n",
    "\n",
    "Test Data ratio: **{{youtubean_test.ratio_targets_sentiment()}}**\n",
    "Test Data raw values: **{{youtubean_test.no_targets_sentiment()}}**\n",
    "\n",
    "Original Data ratio: **{{youtubean.ratio_targets_sentiment()}}**  \n",
    "Original Data raw values: **{{youtubean.no_targets_sentiment()}}**\n",
    "\n",
    "We now save the data back to it's original XML file style format which is the same as the SemEval data format. This is so that others can use this data without having to use this code base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data.semeval_14(full_path(read_config('youtubean_train')), youtubean_train)\n",
    "write_data.semeval_14(full_path(read_config('youtubean_test')), youtubean_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product review train and test set creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_datasets = {'speaker' : speaker_reviews, 'computer' : computer_reviews, \n",
    "                    'router' : router_reviews}\n",
    "train_test_datasets = {}\n",
    "for product_name, product_dataset in product_datasets.items():\n",
    "    product_data = np.asarray(product_dataset.data_dict())\n",
    "    targets = np.asarray(product_dataset.sentiment_data())\n",
    "    for train_indexs, test_indexs in splitter.split(product_data, targets):\n",
    "        train_data = product_data[train_indexs]\n",
    "        test_data = product_data[test_indexs]\n",
    "    train_data = TargetCollection(convert_to_targets(train_data))\n",
    "    test_data = TargetCollection(convert_to_targets(test_data))\n",
    "    train_test_datasets[product_name] = (train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "train_test_datasets['computer'][0].no_targets_sentiment()": "{-1: 59, 1: 188}",
     "train_test_datasets['computer'][1].no_targets_sentiment()": "{-1: 25, 1: 82}",
     "train_test_datasets['router'][0].no_targets_sentiment()": "{-1: 85, 1: 129}",
     "train_test_datasets['router'][1].no_targets_sentiment()": "{-1: 37, 1: 56}",
     "train_test_datasets['speaker'][0].no_targets_sentiment()": "{-1: 55, 1: 253}",
     "train_test_datasets['speaker'][1].no_targets_sentiment()": "{-1: 23, 1: 109}"
    }
   },
   "source": [
    "Speaker train data raw values: **{{train_test_datasets['speaker'][0].no_targets_sentiment()}}**\n",
    "Speaker test data raw values: **{{train_test_datasets['speaker'][1].no_targets_sentiment()}}**\n",
    "\n",
    "Computer train data raw values: **{{train_test_datasets['computer'][0].no_targets_sentiment()}}**\n",
    "Computer test data raw values: **{{train_test_datasets['computer'][1].no_targets_sentiment()}}**\n",
    "\n",
    "Wireless Router data raw values: **{{train_test_datasets['router'][0].no_targets_sentiment()}}**\n",
    "Wireless Router test data raw values: **{{train_test_datasets['router'][1].no_targets_sentiment()}}**\n",
    "\n",
    "We now combine the train and test datasets together so that each domain is reprensted equally in the train and test datasets and the sentiment values are also equally represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = [train_test[0] for name, train_test in train_test_datasets.items()]\n",
    "test_datasets = [train_test[1] for name, train_test in train_test_datasets.items()]\n",
    "product_train = TargetCollection.combine_collections(*train_datasets)\n",
    "product_test = TargetCollection.combine_collections(*test_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product train data raw values: **{{product_train.no_targets_sentiment()}}** ratio **{{product_train.ratio_targets_sentiment()}}**\n",
    "\n",
    "Product test data raw values: **{{product_test.no_targets_sentiment()}}** ratio **{{product_test.ratio_targets_sentiment()}}**\n",
    "\n",
    "We now save the data back to it's original XML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data.semeval_14(full_path(read_config('product_train')), product_train)\n",
    "write_data.semeval_14(full_path(read_config('product_test')), product_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
