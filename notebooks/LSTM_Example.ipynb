{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Models\n",
    "from tdparse.models.tdlstm import TLSTM\n",
    "# Tokenisers\n",
    "from tdparse.tokenisers import whitespace, ark_twokenize\n",
    "# Word Vectors\n",
    "from tdparse.word_vectors import PreTrained, GloveTwitterVectors\n",
    "# Get the data\n",
    "from tdparse.helper import read_config, full_path\n",
    "from tdparse.parsers import dong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "dong_train = dong(full_path(read_config('dong_twit_train_data')))\n",
    "dong_test = dong(full_path(read_config('dong_twit_test_data')))\n",
    "# Load the word vectors\n",
    "sswe_path = full_path(read_config('sswe_files')['vo_zhang'])\n",
    "sswe = PreTrained(sswe_path, name='sswe')\n",
    "#glove_50 = GloveTwitterVectors(50)\n",
    "#glove_100 = GloveTwitterVectors(100)\n",
    "#glove_200 = GloveTwitterVectors(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "This is an implementation of the LSTM model that is shown in [Tang et al. paper](https://aclanthology.info/papers/C16-1311/c16-1311).\n",
    "\n",
    "The LSTM is a single LSTM layer that outputs to a softmax function. The LSTM hidden layer dimension is the same as the embedding layer dimension. The optimiser is Stochastic Gradient Descent with a learning rate of 0.01.\n",
    "\n",
    "The number of epochs to run the model is unknown and Early Stopping is not mentioned in the paper.\n",
    "\n",
    "**Variables that can be changed through the contructor**\n",
    "1. Tokeniser used\n",
    "2. Pre-Trained Embedding e.g. Glove or SSWE (Sentiment Specific Word Embedding)\n",
    "3. Pad Size of the sentences\n",
    "4. To lower case the words. This should always be used as all the pre-trained word embeddings are trained on lower cased words\n",
    "5. Dimension of the LSTM hidden layer - The default for this is the size of the word embeddings e.g. If the word embedding is 50 dimensions then the output of the LSTM is 50 dimensions.\n",
    "6. optimiser to use. To change this provide a valid Keras [optimiser](https://keras.io/optimizers/)\n",
    "7. patience - Wether to use Early stopping or not default is not denoted by None. I would recomend to use it with a value of 10\n",
    "8. batch-size - default 32\n",
    "9. epochs - Number of epochs to train for default 100\n",
    "\n",
    "\n",
    "### Below is an example of how to perform Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3331 samples, validate on 833 samples\n",
      "Epoch 1/5\n",
      "3331/3331 [==============================] - 7s 2ms/step - loss: 0.9763 - acc: 0.5365 - val_loss: 0.9244 - val_acc: 0.5414\n",
      "Epoch 2/5\n",
      "3331/3331 [==============================] - 6s 2ms/step - loss: 0.9098 - acc: 0.5665 - val_loss: 0.9188 - val_acc: 0.5630\n",
      "Epoch 3/5\n",
      "3331/3331 [==============================] - 5s 2ms/step - loss: 0.8696 - acc: 0.6049 - val_loss: 0.8979 - val_acc: 0.5642\n",
      "Epoch 4/5\n",
      "3331/3331 [==============================] - 5s 2ms/step - loss: 0.8496 - acc: 0.6148 - val_loss: 0.8742 - val_acc: 0.5978\n",
      "Epoch 5/5\n",
      "3331/3331 [==============================] - 4s 1ms/step - loss: 0.8250 - acc: 0.6205 - val_loss: 0.9088 - val_acc: 0.5498\n",
      "Train on 3332 samples, validate on 834 samples\n",
      "Epoch 1/5\n",
      "3332/3332 [==============================] - 4s 1ms/step - loss: 0.9807 - acc: 0.5237 - val_loss: 0.9267 - val_acc: 0.5731\n",
      "Epoch 2/5\n",
      "3332/3332 [==============================] - 6s 2ms/step - loss: 0.9114 - acc: 0.5726 - val_loss: 0.9001 - val_acc: 0.5755\n",
      "Epoch 3/5\n",
      "3332/3332 [==============================] - 6s 2ms/step - loss: 0.8829 - acc: 0.5843 - val_loss: 0.8953 - val_acc: 0.5755\n",
      "Epoch 4/5\n",
      "3332/3332 [==============================] - 3s 1ms/step - loss: 0.8569 - acc: 0.6062 - val_loss: 0.8861 - val_acc: 0.5923\n",
      "Epoch 5/5\n",
      "3332/3332 [==============================] - 4s 1ms/step - loss: 0.8282 - acc: 0.6176 - val_loss: 0.8941 - val_acc: 0.5755\n",
      "Train on 3332 samples, validate on 834 samples\n",
      "Epoch 1/5\n",
      "3332/3332 [==============================] - 4s 1ms/step - loss: 0.9819 - acc: 0.5195 - val_loss: 0.9387 - val_acc: 0.5564\n",
      "Epoch 2/5\n",
      "3332/3332 [==============================] - 4s 1ms/step - loss: 0.9074 - acc: 0.5684 - val_loss: 0.9176 - val_acc: 0.5707\n",
      "Epoch 3/5\n",
      "3332/3332 [==============================] - 4s 1ms/step - loss: 0.8789 - acc: 0.5825 - val_loss: 0.9000 - val_acc: 0.5671\n",
      "Epoch 4/5\n",
      "3332/3332 [==============================] - 4s 1ms/step - loss: 0.8476 - acc: 0.6050 - val_loss: 0.8845 - val_acc: 0.6031\n",
      "Epoch 5/5\n",
      "3332/3332 [==============================] - 4s 1ms/step - loss: 0.8198 - acc: 0.6285 - val_loss: 0.9018 - val_acc: 0.5815: 1s - loss: 0.8118 - acc: 0.630 - ETA: 1s - loss: 0.8118 - acc: 0.63 - ETA: 1s - loss\n"
     ]
    }
   ],
   "source": [
    "lstm_model = TLSTM(whitespace, sswe, epochs=5, lower=True, optimiser='adam')\n",
    "predictions, scores = TLSTM. cross_val(dong_train.data_dict(), dong_train.sentiment_data(), \n",
    "                                       lstm_model, cv=3, scorer=accuracy_score,\n",
    "                                       reproducible=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.55950095969289826, 0.57925072046109505, 0.56964457252641687]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2, 1, 0, ..., 2, 2, 1]),\n",
       " array([2, 1, 0, ..., 0, 0, 0]),\n",
       " array([2, 1, 0, ..., 1, 0, 0])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores are accuracy scores per fold in the cross validation and the predicitions are the raw prediction values per fold.\n",
    "\n",
    "### Below is an example of how to fit and test a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4998 samples, validate on 1250 samples\n",
      "Epoch 1/200\n",
      "4998/4998 [==============================] - 6s 1ms/step - loss: 0.9864 - acc: 0.5150 - val_loss: 0.9738 - val_acc: 0.5232\n",
      "Epoch 2/200\n",
      "4998/4998 [==============================] - 5s 1ms/step - loss: 0.9327 - acc: 0.5498 - val_loss: 0.9077 - val_acc: 0.5640\n",
      "Epoch 3/200\n",
      "4998/4998 [==============================] - 6s 1ms/step - loss: 0.8992 - acc: 0.5778 - val_loss: 0.9045 - val_acc: 0.5712\n",
      "Epoch 4/200\n",
      "4998/4998 [==============================] - 12s 2ms/step - loss: 0.8795 - acc: 0.5904 - val_loss: 0.8832 - val_acc: 0.5824\n",
      "Epoch 5/200\n",
      "4998/4998 [==============================] - 7s 1ms/step - loss: 0.8600 - acc: 0.5976 - val_loss: 0.8990 - val_acc: 0.5712\n",
      "Epoch 6/200\n",
      "4998/4998 [==============================] - 7s 1ms/step - loss: 0.8475 - acc: 0.6048 - val_loss: 0.8792 - val_acc: 0.5944\n",
      "Epoch 7/200\n",
      "4998/4998 [==============================] - 10s 2ms/step - loss: 0.8278 - acc: 0.6265 - val_loss: 0.8783 - val_acc: 0.5776\n",
      "Epoch 8/200\n",
      "4998/4998 [==============================] - 6s 1ms/step - loss: 0.8112 - acc: 0.6327 - val_loss: 0.8763 - val_acc: 0.5920\n",
      "Epoch 9/200\n",
      "4998/4998 [==============================] - 6s 1ms/step - loss: 0.7874 - acc: 0.6455 - val_loss: 0.8827 - val_acc: 0.5888\n",
      "Epoch 10/200\n",
      "4998/4998 [==============================] - 5s 1ms/step - loss: 0.7721 - acc: 0.6519 - val_loss: 0.9010 - val_acc: 0.5944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.57947976878612717"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = TLSTM(whitespace, sswe, patience=2, epochs=200, optimiser='adam')\n",
    "lstm_model.fit(dong_train.data_dict(), dong_train.sentiment_data(), reproducible=True,\n",
    "               validation_size=0.2)\n",
    "predicted_values = np.argmax(lstm_model.predict(dong_test.data()), axis=1)\n",
    "test_res = np.argmax(keras.utils.to_categorical(dong_test.sentiment_data(), num_classes=3), axis=1)\n",
    "acc_score = accuracy_score(test_res, predicted_values)\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
