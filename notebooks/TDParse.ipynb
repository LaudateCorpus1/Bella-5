{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Notebook helper\n",
    "from tdparse.notebook_helper import get_json_data, write_json_data\n",
    "# Models\n",
    "from tdparse.models.tdparse import TDParseMinus, TDParse, TDParsePlus\n",
    "# Word Vector methods\n",
    "from tdparse.word_vectors import GensimVectors, PreTrained\n",
    "from tdparse.helper import read_config, full_path\n",
    "# Sentiment lexicons\n",
    "from tdparse import lexicons\n",
    "# Dependency parser\n",
    "from tdparse.dependency_parsers import tweebo, stanford\n",
    "# Data parsers\n",
    "from tdparse.parsers import dong, semeval_14, election\n",
    "# Evaluation metrics\n",
    "from tdparse import evaluation\n",
    "#from tdparse.evaluation import evaluation_results, scores, get_results, \\\n",
    "#                               save_results, combine_results, get_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder to store all of the TDParse results\n",
    "result_folder = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results', 'TDParse'))\n",
    "# Method to find the best C value for different TDParse models and datasets\n",
    "def dataset_best_c(train, model, word_vector, \n",
    "                   random_state, parser, senti_lexicon=None,\n",
    "                   result_path=None, dataset_name=None):\n",
    "    best_c = get_json_data(result_path, dataset_name)\n",
    "    if best_c is not None:\n",
    "        return best_c\n",
    "    # loading the data\n",
    "    data_train = train.data()\n",
    "    y_train = train.sentiment_data()\n",
    "\n",
    "    # Finding the best C value for the model on this dataset\n",
    "    c_grid_params = {'word_vectors' : [word_vector], 'random_state' : random_state,\n",
    "                     'parsers' : [parser]}\n",
    "    if sentiment_lexicon is not None:\n",
    "        c_grid_params['senti_lexicons'] = [senti_lexicon]\n",
    "    best_c = model.find_best_c(data_train, y_train, grid_params=c_grid_params,\n",
    "                               cv=5, n_jobs=5)\n",
    "    write_json_data(result_path, dataset_name, best_c)\n",
    "    return best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "dong_train = dong(full_path(read_config('dong_twit_train_data')))\n",
    "dong_test = dong(full_path(read_config('dong_twit_test_data')))\n",
    "election_train, election_test = election(full_path(read_config('election_folder_dir')))\n",
    "\n",
    "# Get word vectors\n",
    "w2v_path = full_path(read_config('word2vec_files')['vo_zhang'])\n",
    "w2v = GensimVectors(w2v_path, None, model='word2vec', name='w2v')\n",
    "sswe_path = full_path(read_config('sswe_files')['vo_zhang'])\n",
    "sswe = PreTrained(sswe_path, name='sswe')\n",
    "\n",
    "# Load the sentiment lexicons and remove all words that are not associated\n",
    "# to the Positive or Negative class and ensure the lexicons are lower case words\n",
    "subset_cats = {'positive', 'negative'}\n",
    "\n",
    "mpqa = lexicons.Mpqa(subset_cats=subset_cats)\n",
    "nrc = lexicons.NRC(subset_cats=subset_cats)\n",
    "hu_liu = lexicons.HuLiu(subset_cats=subset_cats)\n",
    "mpqa_huliu = lexicons.Lexicon.combine_lexicons(mpqa, hu_liu)\n",
    "all_three = lexicons.Lexicon.combine_lexicons(mpqa_huliu, nrc)\n",
    "# Lower cased lexicons\n",
    "mpqa_low = lexicons.Mpqa(subset_cats=subset_cats, lower=True)\n",
    "nrc_low = lexicons.NRC(subset_cats=subset_cats, lower=True)\n",
    "hu_liu_low = lexicons.HuLiu(subset_cats=subset_cats, lower=True)\n",
    "mpqa_huliu_low = lexicons.Lexicon.combine_lexicons(mpqa_low, hu_liu_low)\n",
    "all_three_low = lexicons.Lexicon.combine_lexicons(mpqa_huliu_low, nrc_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdparse_plus = TDParsePlus()\n",
    "std_model_parameters = {'word_vector' : [w2v, sswe], 'random_state' : 42, 'parser' : tweebo, 'C' : 0.007,\n",
    "                        'scale' : False, 'senti_lexicon' : all_three_low}\n",
    "params = tdparse_plus.get_params(**std_model_parameters)\n",
    "tdparse_plus.fit(dong_train.data(), dong_train.sentiment_data(), params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42195164085528941"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = tdparse_plus.predict(dong_test.data())\n",
    "f1_score(dong_test.sentiment_data(), preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdparse_plus = TDParsePlus()\n",
    "std_model_parameters = {'word_vector' : [w2v, sswe], 'random_state' : 42, 'parser' : tweebo, 'C' : 0.007,\n",
    "                        'scale' : True, 'senti_lexicon' : all_three_low}\n",
    "params = tdparse_plus.get_params(**std_model_parameters)\n",
    "tdparse_plus.fit(dong_train.data(), dong_train.sentiment_data(), params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68397504059612435"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = tdparse_plus.predict(dong_test.data())\n",
    "f1_score(dong_test.sentiment_data(), preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdparse_plus = TDParsePlus()\n",
    "std_model_parameters = {'word_vector' : [w2v, sswe], 'random_state' : 42, 'parser' : stanford, 'C' : 0.007,\n",
    "                        'scale' : True, 'senti_lexicon' : all_three_low}\n",
    "params = tdparse_plus.get_params(**std_model_parameters)\n",
    "tdparse_plus.fit(dong_train.data(), dong_train.sentiment_data(), params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67642860142139549"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = tdparse_plus.predict(dong_test.data())\n",
    "f1_score(dong_test.sentiment_data(), preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdparse_plus = TDParsePlus()\n",
    "std_model_parameters = {'word_vector' : [w2v, sswe], 'random_state' : 42, 'parser' : tweebo, 'C' : 0.0035,\n",
    "                        'scale' : False, 'senti_lexicon' : all_three_low}\n",
    "params = tdparse_plus.get_params(**std_model_parameters)\n",
    "tdparse_plus.fit(election_train.data(), election_train.sentiment_data(), params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28856947963787988"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = tdparse_plus.predict(election_test.data())\n",
    "f1_score(election_test.sentiment_data(), preds, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDParse models\n",
    "This notebook shows how to use the TDParse models and comparing the results of our implementation to the one in the original [paper](https://aclanthology.coli.uni-saarland.de/papers/E17-1046/e17-1046)\n",
    "\n",
    "The paper had 4 different models however we have implemented 3 of them as the fourth **TDParse+ (m)** we have incorporated the same target multi appearence solution into all of the impleneted models therefore **TDParse+ (m)** is now redudant as it equals **TDParse+** in our models. The reason we did this was that we did not know which target appearence they used e.g. if there was three apearance of the same target did they use the first, second and last appearence? this is the reason why the authors created the multi appearence solution, however they did not describe how they implmented the models they did not use the multi appearence solution.\n",
    "\n",
    "Models:\n",
    "1. **TDParse-** -- This only used the syntactic connected words of the target word as context.\n",
    "2. **TDParse** -- This used the syntactic connected words, left, right and target context.\n",
    "3. **TDParse+** -- This used the same as **TDParse** but had two more contexts left and right sentiment contexts which filtered out words that are not in the sentiment lexicons.\n",
    "\n",
    "The above models correspond to the following classes in our implementation:\n",
    "1. [TDParseMinus](../tdparse/models/tdparse.py), 2. [TDParse](../tdparse/models/tdparse.py), 3. [TDParsePlus](../tdparse/models/tdparse.py)\n",
    "\n",
    "The results reported are based on training and testing on the datasets of [Dong et al.](https://aclanthology.coli.uni-saarland.de/papers/P14-2009/p14-2009) and their own Election Twitter dataset which can be found [here](https://figshare.com/articles/EACL_2017_-_Multi-target_UK_election_Twitter_sentiment_corpus/4479563/1) as reported in the paper.\n",
    "\n",
    "The notebook is going to show the results of the original paper using our implementation. We are then going to look at the affects of lower casing and scaling with respect to those results as these parts of the implementation were not mentioned in the original paper but we show the importance of these implementation details.\n",
    "\n",
    "# Original Paper results\n",
    "For each of the datasets we require to find the C-value for the Support Vector Machine (SVM) as this was not reported in the paper but is an important hyper-parameter to tune for. Therefore the start of each dataset we are going to find the Best C value for each of the models keeping everything else constant. The defualt tokeniser is the [Ark tokeniser](http://ttic.uchicago.edu/~kgimpel/papers/gimpel+etal.acl11.pdf) of which we use this python port of the [tokeniser](https://github.com/Sentimentron/ark-twokenize-py). Also we default to lower casing all tokens as well. For the sentiment lexicon method **TDParse+** we use the following lexicons:\n",
    "1. [MPQA](http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/), 2. [NRC](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm), and 3. [Hu & Liu](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon)\n",
    "\n",
    "**NOTE** In the original paper they adopted approaches from *Vo and Zhang 2015* and have used the same pre-trained embeddings, pooling functions and tunned the C value over the same set of parameter values. However they did not state if they were going to use the same tokeniser *Ark tokeniser* or the same set of lexicons. However they did release their code which can be found [here](https://github.com/bluemonk482/tdparse) which shows that they do use the *Ark tokeniser* and the lexicons stated above which were the full set of lexicons in *Vo and Zhang 2015*.\n",
    "\n",
    "## Best C value Dong et al. Dataset\n",
    "\n",
    "We store the results of the C values as finding C values can take a long time > 3 hours using 5 processors and ~8GB of RAM. This is due to the fact we have to dependency parse each time we train and test as we do not cache pre-processing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "tdparse_minus = TDParseMinus()\n",
    "tdparse = TDParse()\n",
    "tdparse_plus = TDParsePlus()\n",
    "models = {'TDParse-' : tdparse_minus, 'TDParse' : tdparse, 'TDParse+' : tdparse_plus}\n",
    "# Model parameters\n",
    "std_model_parameters = {'word_vector' : [w2v, sswe], 'random_state' : 42, 'parser' : tweebo}\n",
    "sentiment_model_parameters = {**std_model_parameters, 'senti_lexicon' : all_three_low}\n",
    "# Model and parameters\n",
    "model_parameters = {'TDParse-' : std_model_parameters, 'TDParse' : std_model_parameters, \n",
    "                    'TDParse+' : sentiment_model_parameters}\n",
    "# Results folder/files for C values\n",
    "c_result_folder = os.path.join(result_folder, 'Best C value')\n",
    "os.makedirs(c_result_folder, exist_ok=True)\n",
    "add_folder = lambda res_file: os.path.join(c_result_folder, res_file)\n",
    "model_results_path = {'TDParse-' : add_folder('TDParse- C.json'),\n",
    "                      'TDParse' : add_folder('TDParse C.json'),\n",
    "                      'TDParse+' : add_folder('TDParse+ C.json')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c_value_dong = {}\n",
    "for model_name, model in models.items():\n",
    "    parameters = model_parameters[model_name]\n",
    "    result_path = model_results_path[model_name]\n",
    "    model_c_value_dong[model_name] = dataset_best_c(dong_train, model, result_path=result_path,\n",
    "                                                    dataset_name='Dong Twitter', **parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best C value for TDParse- is {{model_c_value_dong['TDParse-']}}, TDParse is {{model_c_value_dong['TDParse']}} and TDParse+ is {{model_c_value_dong['TDParse+']}}. We will use these C-Values for all of the other experiments on the Dong et al. Dataset\n",
    "\n",
    "## Best C value Elections dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c_value_elections = {}\n",
    "for model_name, model in models.items():\n",
    "    parameters = model_parameters[model_name]\n",
    "    result_path = model_results_path[model_name]\n",
    "    model_c_value_elections[model_name] = dataset_best_c(election_train, model, result_path=result_path,\n",
    "                                                         dataset_name='Election Twitter', **parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best C value for TDParse- is {{model_c_value_elections['TDParse-']}}, TDParse is {{model_c_value_elections['TDParse']}} and TDParse+ is {{model_c_value_elections['TDParse+']}}. We will use these C-Values for all of the other experiments on the Elections Dataset\n",
    "\n",
    "### Accuracy and Macro F1 scores on the Datasets\n",
    "\n",
    "Below trains and tests on the two datasets and then displays the results comparing to the original results stated in the paper\n",
    "**NOTE** Takes around 19 minutes to run on 6 processors and around 13 GB of RAM to reduce the amount of RAM use fewer processors in the Pool Instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_dataset(model, model_name, parameters, train, test, dataset_name):\n",
    "    model.fit(train.data(), train.sentiment_data(), params=parameters)\n",
    "    predictions = model.predict(test.data())\n",
    "    test_sentiment = test.sentiment_data()\n",
    "    num_classes = len(set(test_sentiment))\n",
    "    eval_results = evaluation.scores(test_sentiment, predictions, num_classes)\n",
    "    return model_name, dataset_name, eval_results, predictions\n",
    "\n",
    "datasets = {'Election Twitter' : (election_train, election_test), \n",
    "            'Dong Twitter' : (dong_train, dong_test)}\n",
    "score_dataset_params = []\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    train, test = dataset\n",
    "    for model_name, model in models.items():\n",
    "        parameters = {**model_parameters[model_name]}\n",
    "        if dataset_name == 'Dong Twitter':\n",
    "            parameters['C'] = model_c_value_dong[model_name]\n",
    "        else:\n",
    "            parameters['C'] = model_c_value_elections[model_name]\n",
    "        \n",
    "        parameters = model.get_params(**parameters)\n",
    "        score_dataset_params.append((model, model_name, parameters, \n",
    "                                     train, test, dataset_name))\n",
    "with Pool(6) as pool:\n",
    "    model_results = pool.starmap(scores_dataset, score_dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy (Paper)</th>\n",
       "      <th>Macro F1 2-Way</th>\n",
       "      <th>Macro F1 2-Way (Paper)</th>\n",
       "      <th>Macro F1 3-Way</th>\n",
       "      <th>Macro F1 3-Way (Paper)</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Macro Recall (Paper)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TDParse- Election Twitter</th>\n",
       "      <td>53.0</td>\n",
       "      <td>52.53</td>\n",
       "      <td>33.1</td>\n",
       "      <td>40.67</td>\n",
       "      <td>36.0</td>\n",
       "      <td>42.71</td>\n",
       "      <td>39.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDParse Election Twitter</th>\n",
       "      <td>55.1</td>\n",
       "      <td>56.45</td>\n",
       "      <td>35.6</td>\n",
       "      <td>43.43</td>\n",
       "      <td>40.8</td>\n",
       "      <td>46.09</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDParse+ Election Twitter</th>\n",
       "      <td>55.4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>38.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>44.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDParse- Dong Twitter</th>\n",
       "      <td>63.6</td>\n",
       "      <td>61.70</td>\n",
       "      <td>52.9</td>\n",
       "      <td>51.10</td>\n",
       "      <td>59.0</td>\n",
       "      <td>57.00</td>\n",
       "      <td>57.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDParse Dong Twitter</th>\n",
       "      <td>69.8</td>\n",
       "      <td>71.00</td>\n",
       "      <td>62.3</td>\n",
       "      <td>64.30</td>\n",
       "      <td>66.9</td>\n",
       "      <td>68.40</td>\n",
       "      <td>65.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDParse+ Dong Twitter</th>\n",
       "      <td>70.8</td>\n",
       "      <td>72.50</td>\n",
       "      <td>64.5</td>\n",
       "      <td>66.60</td>\n",
       "      <td>68.4</td>\n",
       "      <td>70.30</td>\n",
       "      <td>67.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Accuracy  Accuracy (Paper)  Macro F1 2-Way  \\\n",
       "Model Dataset                                                           \n",
       "TDParse- Election Twitter      53.0             52.53            33.1   \n",
       "TDParse Election Twitter       55.1             56.45            35.6   \n",
       "TDParse+ Election Twitter      55.4              0.00            38.1   \n",
       "TDParse- Dong Twitter          63.6             61.70            52.9   \n",
       "TDParse Dong Twitter           69.8             71.00            62.3   \n",
       "TDParse+ Dong Twitter          70.8             72.50            64.5   \n",
       "\n",
       "                           Macro F1 2-Way (Paper)  Macro F1 3-Way  \\\n",
       "Model Dataset                                                       \n",
       "TDParse- Election Twitter                   40.67            36.0   \n",
       "TDParse Election Twitter                    43.43            40.8   \n",
       "TDParse+ Election Twitter                    0.00            43.0   \n",
       "TDParse- Dong Twitter                       51.10            59.0   \n",
       "TDParse Dong Twitter                        64.30            66.9   \n",
       "TDParse+ Dong Twitter                       66.60            68.4   \n",
       "\n",
       "                           Macro F1 3-Way (Paper)  Macro Recall  \\\n",
       "Model Dataset                                                     \n",
       "TDParse- Election Twitter                   42.71          39.7   \n",
       "TDParse Election Twitter                    46.09          43.0   \n",
       "TDParse+ Election Twitter                    0.00          44.2   \n",
       "TDParse- Dong Twitter                       57.00          57.2   \n",
       "TDParse Dong Twitter                        68.40          65.6   \n",
       "TDParse+ Dong Twitter                       70.30          67.1   \n",
       "\n",
       "                           Macro Recall (Paper)  \n",
       "Model Dataset                                    \n",
       "TDParse- Election Twitter                     0  \n",
       "TDParse Election Twitter                      0  \n",
       "TDParse+ Election Twitter                     0  \n",
       "TDParse- Dong Twitter                         0  \n",
       "TDParse Dong Twitter                          0  \n",
       "TDParse+ Dong Twitter                         0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_scores = {'TDParse- Election Twitter' : [52.53, 42.71, 40.67, 0], \n",
    "                'TDParse Election Twitter' : [56.45, 46.09, 43.43, 0], \n",
    "                'TDParse+ Election Twitter' : [0, 0, 0, 0],\n",
    "                'TDParse- Dong Twitter' : [61.7, 57, 51.1, 0], \n",
    "                'TDParse Dong Twitter' : [71, 68.4, 64.3, 0], \n",
    "                'TDParse+ Dong Twitter' : [72.5, 70.3, 66.6, 0]}\n",
    "\n",
    "eval_measures = ['Accuracy', 'Macro F1 3-Way', 'Macro F1 2-Way', 'Macro Recall',\n",
    "                 'Accuracy (Paper)', 'Macro F1 3-Way (Paper)', \n",
    "                 'Macro F1 2-Way (Paper)', 'Macro Recall (Paper)']\n",
    "dataset_results_dict = {eval_measure : [] for eval_measure in eval_measures}\n",
    "model_datasets = []\n",
    "model_dataset_predictions = {}\n",
    "for model_result in model_results:\n",
    "    model_name, dataset_name, scores, predictions = model_result\n",
    "    model_dataset = '{} {}'.format(model_name, dataset_name)\n",
    "    model_datasets.append(model_dataset)\n",
    "    model_dataset_predictions[model_dataset] = predictions\n",
    "    acc, f1_3way, f1_2way, recall = scores\n",
    "    dataset_results_dict['Accuracy'].append(acc)\n",
    "    dataset_results_dict['Macro F1 3-Way'].append(f1_3way)\n",
    "    dataset_results_dict['Macro F1 2-Way'].append(f1_2way)\n",
    "    dataset_results_dict['Macro Recall'].append(recall)\n",
    "    acc, f1_3way, f1_2way, recall = paper_scores[model_dataset]\n",
    "    dataset_results_dict['Accuracy (Paper)'].append(acc)\n",
    "    dataset_results_dict['Macro F1 3-Way (Paper)'].append(f1_3way)\n",
    "    dataset_results_dict['Macro F1 2-Way (Paper)'].append(f1_2way)\n",
    "    dataset_results_dict['Macro Recall (Paper)'].append(recall)\n",
    "    \n",
    "dataset_results_dict['Model Dataset'] = model_datasets\n",
    "dataset_results_df = pd.DataFrame(dataset_results_dict)\n",
    "dataset_results_df = dataset_results_df.set_index('Model Dataset')\n",
    "dataset_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our results for the Dong dataset are fairly similar to those in the original paper. \n",
    "\n",
    "The Election dataset we could not get such similar results but this could be due to the fact that we could not parse the dataset the same as they did in the original paper. We could remove the 975 unhighlighted instances but we could not include the additional 688 highlighted instances due to the way the data is represented therefore these additional instances have not been trained/tested on. This could therefore explain the difference in results.\n",
    "\n",
    "Finally you can see that **TDParse+** was not run on the Election dataset from the original paper but we can show here how it is still better than the non-sentiment version **TDParse** showing that using a range of sentiment lexicons is still effective in the Political domain as well as the more general domain of the Dong et al. dataset.\n",
    "\n",
    "\n",
    "## The affects of different parameters on the model\n",
    "These parameters were not stated in the paper but they could be found in their [code base](https://github.com/bluemonk482/tdparse) and we thus show the affect of not stating these parameters.\n",
    "\n",
    "### The affect of lower casing\n",
    "\n",
    "We show below the affect of lower casing on the best model **TDParse+**. Lower casing happens in two parts. 1. The text that is input into the different contexts and is used as lookup for the word embeddings and 2. the text that is fed to the dependency parser, this is by default left in its normal state. Therefore we are going to do 4 experiments:\n",
    "1. Lower casing text to the word vectors and lower casing the text that is input to the dependency parser\n",
    "2. Lower casing text to the word vectors and not lower casing the text that is input to the dependency parser.\n",
    "3. Not lower casing text to the word vectors and not lower casing the text that is input to the dependency parser.\n",
    "4. Not lower casing text to the word vectors and lower casing the text that is input to the dependency parser.\n",
    "\n",
    "By default and the results thus far have used configuration 2. from the list above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1083.3295509815216"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = {'Dong Twitter' : (dong_train, dong_test), \n",
    "            'Election Twitter' : (election_train, election_test)}\n",
    "model_name = 'TDParse+'\n",
    "model = models[model_name]\n",
    "\n",
    "def scores_dataset(model, lower_name, parameters, train, test, dataset_name):\n",
    "    model.fit(train.data(), train.sentiment_data(), params=parameters)\n",
    "    predictions = model.predict(test.data())\n",
    "    acc_score = tdparse_plus.score(test.sentiment_data(),\n",
    "                                   predictions, accuracy_score)\n",
    "    return dataset_name, lower_name, acc_score\n",
    "\n",
    "lower_values = {'Lower' : True, 'Not Lower' : False}\n",
    "lower_params = []\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    train, test = dataset\n",
    "    for lower_name, lower_value in lower_values.items():\n",
    "        params = {**model_parameters[model_name]}\n",
    "        params['token_lower'] = lower_value\n",
    "        if dataset_name == 'Dong Twitter':\n",
    "            params['C'] = model_c_value_dong[model_name]\n",
    "        elif dataset_name == 'Election Twitter':\n",
    "            params['C'] = model_c_value_elections[model_name]\n",
    "        else:\n",
    "            raise ValueError('Dataset name does not exist {}'\\\n",
    "                             .format(dataset_name))\n",
    "        params = model.get_params(**params)\n",
    "        lower_params.append((model, lower_name, params, train, test, dataset_name))\n",
    "with Pool(4) as pool:\n",
    "    lower_results = pool.starmap(scores_dataset, lower_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dong Twitter</th>\n",
       "      <th>Election Twitter</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lower Cased</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Lower</th>\n",
       "      <td>70.23</td>\n",
       "      <td>55.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not Lower</th>\n",
       "      <td>69.51</td>\n",
       "      <td>54.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Dong Twitter  Election Twitter\n",
       "Lower Cased                                \n",
       "Lower               70.23             55.29\n",
       "Not Lower           69.51             54.35"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['Dong Twitter', 'Election Twitter']\n",
    "lower_results_df = pd.DataFrame(np.zeros((2, 2)), columns=columns)\n",
    "lower_results_df['Lower Cased'] = ['Lower', 'Not Lower']\n",
    "lower_results_df = lower_results_df.set_index('Lower Cased')\n",
    "for result in lower_results:\n",
    "    column, index, accuracy = result\n",
    "    lower_results_df[column][index] = accuracy\n",
    "lower_results_df = (lower_results_df * 100).round(2)\n",
    "dong_results = lower_results_df['Dong Twitter']\n",
    "dong_lower_affect = dong_results['Lower'] - dong_results['Not Lower']\n",
    "election_results = lower_results_df['Election Twitter']\n",
    "election_lower_affect = election_results['Lower'] - election_results['Not Lower']\n",
    "lower_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The affect of lower casing for the Dong Twitter dataset is around ~{{str(dong_lower_affect)}}% and for the Election Twitter dataset ~{{str(election_lower_affect)}}% which is very small. Showing that the amound of Out Of Vocabularly (OOV) terms due to not lower casing does not make much of an affect.\n",
    "\n",
    "### The Affect of scaling\n",
    "We show below the affect of scaling the word vectors on the best model **TDParse+**. To scale we use [MinMax](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) scaling. They did use a different Support Vector Machine Library [LibLinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/) however we actually use the [Scikit-learn interface](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html). Even though in the [Practical guide to LibLinear](https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf) it states that you should scale and they also show this in their [README.md](https://github.com/bluemonk482/tdparse) of their code base. We show the importance of scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'Dong Twitter' : (dong_train, dong_test), \n",
    "            'Election Twitter' : (election_train, election_test)}\n",
    "model_name = 'TDParse+'\n",
    "model = models[model_name]\n",
    "\n",
    "def scores_dataset(model, scaled_name, parameters, train, test, dataset_name):\n",
    "    model.fit(train.data(), train.sentiment_data(), params=parameters)\n",
    "    predictions = model.predict(test.data())\n",
    "    acc_score = tdparse_plus.score(test.sentiment_data(),\n",
    "                                   predictions, accuracy_score)\n",
    "    return dataset_name, scaled_name, acc_score\n",
    "scaled_values = {'Scaled' : True, 'Not Scaled' : False}\n",
    "scaled_params = []\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    train, test = dataset\n",
    "    for scaled_name, scaled_value in scaled_values.items():\n",
    "        params = {**model_parameters[model_name]}\n",
    "        params['scale'] = scaled_value\n",
    "        if dataset_name == 'Dong Twitter':\n",
    "            params['C'] = model_c_value_dong[model_name]\n",
    "        elif dataset_name == 'Election Twitter':\n",
    "            params['C'] = model_c_value_elections[model_name]\n",
    "        else:\n",
    "            raise ValueError('Dataset name does not exist {}'\\\n",
    "                             .format(dataset_name))\n",
    "        params = model.get_params(**params)\n",
    "        scaled_params.append((model, scaled_name, params, train, test, dataset_name))\n",
    "with Pool(4) as pool:\n",
    "    scaled_results = pool.starmap(scores_dataset, scaled_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dong Twitter</th>\n",
       "      <th>Election Twitter</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scaled</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Scaled</th>\n",
       "      <td>70.81</td>\n",
       "      <td>55.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not Scaled</th>\n",
       "      <td>43.35</td>\n",
       "      <td>34.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Dong Twitter  Election Twitter\n",
       "Scaled                                    \n",
       "Scaled             70.81             55.37\n",
       "Not Scaled         43.35             34.40"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['Dong Twitter', 'Election Twitter']\n",
    "scaled_results_df = pd.DataFrame(np.zeros((2, 2)), columns=columns)\n",
    "scaled_results_df['Scaled'] = ['Scaled', 'Not Scaled']\n",
    "scaled_results_df = scaled_results_df.set_index('Scaled')\n",
    "for result in scaled_results:\n",
    "    column, index, accuracy = result\n",
    "    scaled_results_df[column][index] = accuracy\n",
    "scaled_results_df = (scaled_results_df * 100).round(2)\n",
    "dong_results = scaled_results_df['Dong Twitter']\n",
    "dong_scaled_affect = dong_results['Scaled'] - dong_results['Not Scaled']\n",
    "election_results = scaled_results_df['Election Twitter']\n",
    "election_scaled_affect = election_results['Scaled'] - election_results['Not Scaled']\n",
    "scaled_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The affect of scaling for the Dong Twitter dataset is around ~{{str(dong_scaled_affect)}}% and for the Election Twitter dataset ~{{str(election_scaled_affect)}}% which is quite a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
