{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Models\n",
    "from tdparse.models.target import TargetInd\n",
    "from tdparse.models.target import TargetDepC\n",
    "from tdparse.models.target import TargetDep\n",
    "from tdparse.models.target import TargetDepSent\n",
    "# Word Vector methods\n",
    "from tdparse.word_vectors import GensimVectors\n",
    "from tdparse.word_vectors import PreTrained\n",
    "from tdparse.helper import read_config, full_path\n",
    "# Sentiment lexicons\n",
    "from tdparse import lexicons\n",
    "# Get the data\n",
    "from tdparse.parsers import dong, semeval_14\n",
    "\n",
    "from tdparse.tokenisers import stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target dependent models\n",
    "This notebook shows how to use the target dependent models and comparing the results of our implementation to the one in the original [paper](https://www.ijcai.org/Proceedings/15/Papers/194.pdf). We also show the problems we encountered when attempting to reproduce the methods from the description in the paper and the affects of not stating certain processes.\n",
    "\n",
    "The paper had four different models:\n",
    "1. **Target-Ind** -- Uses only the full Tweet as context.\n",
    "2. **Target-Dep-** -- Uses the left and right context of the target word as well as the target word as context.\n",
    "3. **Target-Dep** -- Uses all of the above contexts.\n",
    "4. **Target-Dep+** -- Uses all of the above as well as including two additional left and right contexts which ignores all words in the contexts unless they are part of the given sentiment lexicon (or any lexicon).\n",
    "\n",
    "The above models correspond to the following classes in our implementation:\n",
    "1. [TargetInd](../tdparse/models/target.py), 2. [TargetDepC](../tdparse/models/target.py), 3. [TargetDep](../tdparse/models/target.py), 4. [TargetDepSent](../tdparse/models/target.py)\n",
    "\n",
    "All of the results shown below are 5 fold cross validation over the training data of [Dong et al.](https://aclanthology.coli.uni-saarland.de/papers/P14-2009/p14-2009) or where appropriate on the test data as reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "dong_train = dong(full_path(read_config('dong_twit_train_data')))\n",
    "train_data = dong_train.data()\n",
    "train_y = dong_train.sentiment_data()\n",
    "\n",
    "# Get word vectors\n",
    "w2v_path = full_path(read_config('word2vec_files')['vo_zhang'])\n",
    "w2v = GensimVectors(w2v_path, None, model='word2vec', name='w2v')\n",
    "sswe_path = full_path(read_config('sswe_files')['vo_zhang'])\n",
    "sswe = PreTrained(sswe_path, name='sswe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the three base models\n",
    "\n",
    "In the paper the base models (target-ind, target-dep- and target-dep) using the the word2vec word vectors were compared after they found the best C-values therefore we are going to use the C-Values stated in the paper to compare our results to theres. **random_state** is used here to ensure that the results are reproducible, it stops the data from randomly shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances of the models\n",
    "target_ind = TargetInd()\n",
    "target_depc = TargetDepC()\n",
    "target_dep = TargetDep()\n",
    "# Getting the grid parameters for each model\n",
    "grid_params_ind = target_ind.get_cv_params(word_vectors=[[w2v]], random_state=42)\n",
    "grid_params_depc = target_depc.get_cv_params(word_vectors=[[w2v]], random_state=42)\n",
    "grid_params_dep = target_dep.get_cv_params(word_vectors=[[w2v]], random_state=42)\n",
    "# Running the grid search over 5 folds.\n",
    "results_ind = target_ind.grid_search(train_data, train_y, params=grid_params_ind, cv=5, n_jobs=5)\n",
    "results_depc = target_depc.grid_search(train_data, train_y, params=grid_params_depc, cv=5, n_jobs=5)\n",
    "results_dep = target_dep.grid_search(train_data, train_y, params=grid_params_dep, cv=5, n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Our results</th>\n",
       "      <th>Paper results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Target-Ind</th>\n",
       "      <td>60.98</td>\n",
       "      <td>59.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target-Dep-</th>\n",
       "      <td>65.69</td>\n",
       "      <td>65.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target-Dep</th>\n",
       "      <td>66.79</td>\n",
       "      <td>65.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Our results  Paper results\n",
       "Target-Ind         60.98          59.22\n",
       "Target-Dep-        65.69          65.38\n",
       "Target-Dep         66.79          65.72"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [results_ind['mean_test_score'], results_depc['mean_test_score'], results_dep['mean_test_score']]\n",
    "all_results = {'Our results' : [result.round(4)[0] * 100 for result in results]}\n",
    "all_results['Paper results'] = [59.22, 65.38, 65.72]\n",
    "index = ['Target-Ind', 'Target-Dep-', 'Target-Dep']\n",
    "base_model_df = pd.DataFrame(all_results, index=index)\n",
    "base_model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the results above that we get similar results and the order of the models stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target-Dep+ and sentiment lexicons\n",
    "The **Target-Dep+** model uses sentiment lexicons to remove words therefore in this section we compare:\n",
    "1. The statistics on the sentiment lexicons\n",
    "2. The results of the model using different lexicons\n",
    "\n",
    "All the experiments below again use the Word2Vec word embeddings.\n",
    "## Sentiment lexicon statistics\n",
    "\n",
    "Below we present the size of the sentiment lexicon once it has been processed and the size of that lexicon stated in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentiment lexicons and remove all words that are not associated\n",
    "# to the Positive or Negative class.\n",
    "subset_cats = {'positive', 'negative'}\n",
    "mpqa = lexicons.Mpqa(subset_cats=subset_cats)\n",
    "nrc = lexicons.NRC(subset_cats=subset_cats)\n",
    "hu_liu = lexicons.HuLiu(subset_cats=subset_cats)\n",
    "# Combine sentiment lexicons - Removes words that contradict each other.\n",
    "mpqa_huliu = lexicons.Lexicon.combine_lexicons(mpqa, hu_liu)\n",
    "all_three = lexicons.Lexicon.combine_lexicons(mpqa_huliu, nrc)\n",
    "\n",
    "# Load the sentiment lexicons but lower case all the words\n",
    "mpqa_low = lexicons.Mpqa(subset_cats=subset_cats, lower=True)\n",
    "nrc_low = lexicons.NRC(subset_cats=subset_cats, lower=True)\n",
    "hu_liu_low = lexicons.HuLiu(subset_cats=subset_cats, lower=True)\n",
    "mpqa_huliu_low = lexicons.Lexicon.combine_lexicons(mpqa_low, hu_liu_low)\n",
    "all_three_low = lexicons.Lexicon.combine_lexicons(mpqa_huliu_low, nrc_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper No. Positive</th>\n",
       "      <th>Ours No. Positive</th>\n",
       "      <th>Ours low No. Positive</th>\n",
       "      <th>Paper No. Negative</th>\n",
       "      <th>Ours No. Negative</th>\n",
       "      <th>Ours low No. Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MPQA</th>\n",
       "      <td>2289</td>\n",
       "      <td>2304</td>\n",
       "      <td>2304</td>\n",
       "      <td>4114</td>\n",
       "      <td>4154</td>\n",
       "      <td>4154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hu Liu</th>\n",
       "      <td>2003</td>\n",
       "      <td>2006</td>\n",
       "      <td>2006</td>\n",
       "      <td>4780</td>\n",
       "      <td>4783</td>\n",
       "      <td>4783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NRC</th>\n",
       "      <td>2231</td>\n",
       "      <td>2312</td>\n",
       "      <td>2312</td>\n",
       "      <td>3243</td>\n",
       "      <td>3324</td>\n",
       "      <td>3324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MPQA &amp; Hu Liu</th>\n",
       "      <td>2706</td>\n",
       "      <td>2726</td>\n",
       "      <td>2726</td>\n",
       "      <td>5069</td>\n",
       "      <td>5079</td>\n",
       "      <td>5075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All Three</th>\n",
       "      <td>3940</td>\n",
       "      <td>4036</td>\n",
       "      <td>4036</td>\n",
       "      <td>6490</td>\n",
       "      <td>6551</td>\n",
       "      <td>6547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Paper No. Positive  Ours No. Positive  Ours low No. Positive  \\\n",
       "MPQA                         2289               2304                   2304   \n",
       "Hu Liu                       2003               2006                   2006   \n",
       "NRC                          2231               2312                   2312   \n",
       "MPQA & Hu Liu                2706               2726                   2726   \n",
       "All Three                    3940               4036                   4036   \n",
       "\n",
       "               Paper No. Negative  Ours No. Negative  Ours low No. Negative  \n",
       "MPQA                         4114               4154                   4154  \n",
       "Hu Liu                       4780               4783                   4783  \n",
       "NRC                          3243               3324                   3324  \n",
       "MPQA & Hu Liu                5069               5079                   5075  \n",
       "All Three                    6490               6551                   6547  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_cat(lexicon, filter_cat):\n",
    "    return [word for word, cat in lexicon.lexicon if cat == filter_cat]\n",
    "\n",
    "all_lexicons = [mpqa, hu_liu, nrc, mpqa_huliu, all_three]\n",
    "num_positive = [len(filter_cat(lexicon, 'positive')) for lexicon in all_lexicons]\n",
    "num_negative = [len(filter_cat(lexicon, 'negative')) for lexicon in all_lexicons]\n",
    "\n",
    "all_lexicons_low = [mpqa_low, hu_liu_low, nrc_low, mpqa_huliu_low, all_three_low]\n",
    "num_positive_low = [len(filter_cat(lexicon, 'positive')) for lexicon in all_lexicons_low]\n",
    "num_negative_low = [len(filter_cat(lexicon, 'negative')) for lexicon in all_lexicons_low]\n",
    "\n",
    "columns = ['Paper No. Positive', 'Ours No. Positive', 'Ours low No. Positive', \n",
    "           'Paper No. Negative', 'Ours No. Negative', 'Ours low No. Negative']\n",
    "index = ['MPQA', 'Hu Liu', 'NRC', 'MPQA & Hu Liu', 'All Three']\n",
    "data = [[2289, 2003, 2231, 2706, 3940], num_positive, num_positive_low, \n",
    "        [4114, 4780, 3243, 5069, 6490], num_negative, num_negative_low]\n",
    "senti_info = dict(list(zip(columns, data)))\n",
    "pd.DataFrame(senti_info, columns=columns, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anti-American', 'anti-Semites', 'anti-Israeli', 'anti-US']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words that are shared between the MPQA and Hu Liu sentiment lexicons\n",
    "[word for word, cat in list(set(mpqa_huliu.lexicon).difference(set(mpqa_huliu_low.lexicon))) if cat == 'negative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we never agree on the number of words within the lexicons. We get the lexicons from the sources described in the paper. Intrestingly if we do not lower case the words in the lexicons we won't see the same similarities between the MPQA and Hu Liu sentiment lexicon as they both share the words above just the Hu Liu lexicon has the words lower cased already where as MPQA has not.\n",
    "\n",
    "## Showing the affect of using different sentiment lexicons in the Target-Dep+ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances of the model\n",
    "target_dep_plus = TargetDepSent()\n",
    "# Getting the grid parameters for each model\n",
    "grid_params_sent = target_dep_plus.get_cv_params(word_vectors=[[w2v]], senti_lexicons=all_lexicons_low,\n",
    "                                                 random_state=42)\n",
    "# Running the grid search over 5 folds.\n",
    "results_sent = target_dep_plus.grid_search(train_data, train_y, params=grid_params_sent, cv=5, n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sent_results = {'Paper results' : [65.72, 66.05, 67.24, 65.56, 67.40, 67.30],\n",
    "                    'Our results' : np.zeros(6)}\n",
    "index = ['Target-Dep', 'Target-Dep+: NRC', 'Target-Dep+: Hu Liu', 'Target-Dep+: MPQA',\n",
    "         'Target-Dep+: MPQA + Hu Liu', 'Target-Dep+: All Three']\n",
    "sent_results_df = pd.DataFrame(all_sent_results, index=index)\n",
    "sent_results_df['Our results']['Target-Dep'] = base_model_df['Our results']['Target-Dep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Our results</th>\n",
       "      <th>Paper results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Target-Dep</th>\n",
       "      <td>66.79</td>\n",
       "      <td>65.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target-Dep+: NRC</th>\n",
       "      <td>67.21</td>\n",
       "      <td>66.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target-Dep+: Hu Liu</th>\n",
       "      <td>68.63</td>\n",
       "      <td>67.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target-Dep+: MPQA</th>\n",
       "      <td>66.92</td>\n",
       "      <td>65.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target-Dep+: MPQA + Hu Liu</th>\n",
       "      <td>68.34</td>\n",
       "      <td>67.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target-Dep+: All Three</th>\n",
       "      <td>68.25</td>\n",
       "      <td>67.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Our results  Paper results\n",
       "Target-Dep                        66.79          65.72\n",
       "Target-Dep+: NRC                  67.21          66.05\n",
       "Target-Dep+: Hu Liu               68.63          67.24\n",
       "Target-Dep+: MPQA                 66.92          65.56\n",
       "Target-Dep+: MPQA + Hu Liu        68.34          67.40\n",
       "Target-Dep+: All Three            68.25          67.30"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_map = {'Mpqa' : 'Target-Dep+: MPQA', 'HuLiu' : 'Target-Dep+: Hu Liu', 'NRC' : 'Target-Dep+: NRC',\n",
    "            'Mpqa HuLiu' : 'Target-Dep+: MPQA + Hu Liu', 'Mpqa HuLiu NRC' : 'Target-Dep+: All Three'}\n",
    "results_sent['lexicon'] = results_sent['param_union__left_s__filter__lexicon'].apply(lambda lex: lex.name)\n",
    "for lex_name, model_name in name_map.items():\n",
    "    score = results_sent.loc[results_sent['lexicon'] == lex_name]['mean_test_score']\n",
    "    score = score.round(4) * 100\n",
    "    sent_results_df['Our results'][model_name] = score\n",
    "sent_results_df['Our results']['Target-Dep'] = base_model_df['Our results']['Target-Dep']\n",
    "sent_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results shown above we get different results but the results also have a different rank between the lexicons as in the best lexicon was **Hu and Liu** where as the papers original results show the combination of **MPQA and Hu & Liu** was the best. However in general we can see that it is better to use a sentiment lexicon than not. Also that both our implmentation and the original paper show that the best single sentiment lexicon is **Hu & Liu** and that using **all three** sentiment lexicons is worse than using **MPQA and Hu & Liu**.\n",
    "\n",
    "# Showing the affect of the different word vectors\n",
    "As presented in the paper they show the affect of using different word vectors accross the four models using the best sentiment lexicon for the sentiment dependent model. As we had different result for the sentiment lexicons compared to the original paper we will show the results of using **Hu & Liu** lexicon and using the combination of **Hu & Liu and MPQA**. The word vectors used are the following:\n",
    "1. Word2Vec - Which has been used throughout the previous experiments (100 dimensions)\n",
    "2. SSWE - Sentiment Specific Word Embeddings (50 dimensions)\n",
    "3. Concatenation of Word2vec and SSWE (150 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the results\n",
    "grid_params_ind = target_ind.get_cv_params(word_vectors=[[w2v], [sswe], [w2v, sswe]], random_state=42)\n",
    "grid_params_depc = target_depc.get_cv_params(word_vectors=[[w2v], [sswe], [w2v, sswe]], random_state=42)\n",
    "grid_params_dep = target_dep.get_cv_params(word_vectors=[[w2v], [sswe], [w2v, sswe]], random_state=42)\n",
    "grid_params_dep_sent = target_dep_plus.get_cv_params(word_vectors=[[w2v], [sswe], [w2v, sswe]], \n",
    "                                                     senti_lexicons=[hu_liu_low, mpqa_huliu_low], random_state=42)\n",
    "\n",
    "results_ind = target_ind.grid_search(train_data, train_y, params=grid_params_ind, cv=5, n_jobs=5)\n",
    "results_depc = target_depc.grid_search(train_data, train_y, params=grid_params_depc, cv=5, n_jobs=5)\n",
    "results_dep = target_dep.grid_search(train_data, train_y, params=grid_params_dep, cv=5, n_jobs=5)\n",
    "results_dep_sent = target_dep_plus.grid_search(train_data, train_y, params=grid_params_dep_sent, cv=5, n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target-Ind</th>\n",
       "      <th>Target-Dep-</th>\n",
       "      <th>Target-Dep</th>\n",
       "      <th>Target-Dep+: Hu Liu</th>\n",
       "      <th>Target-Dep+: MPQA + Hu Liu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word2vec</th>\n",
       "      <td>60.98</td>\n",
       "      <td>65.69</td>\n",
       "      <td>66.79</td>\n",
       "      <td>68.63</td>\n",
       "      <td>68.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sswe</th>\n",
       "      <td>60.18</td>\n",
       "      <td>66.71</td>\n",
       "      <td>66.36</td>\n",
       "      <td>67.96</td>\n",
       "      <td>67.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec + sswe</th>\n",
       "      <td>63.20</td>\n",
       "      <td>67.46</td>\n",
       "      <td>68.07</td>\n",
       "      <td>69.46</td>\n",
       "      <td>69.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Target-Ind  Target-Dep-  Target-Dep  Target-Dep+: Hu Liu  \\\n",
       "word2vec              60.98        65.69       66.79                68.63   \n",
       "sswe                  60.18        66.71       66.36                67.96   \n",
       "word2vec + sswe       63.20        67.46       68.07                69.46   \n",
       "\n",
       "                 Target-Dep+: MPQA + Hu Liu  \n",
       "word2vec                              68.34  \n",
       "sswe                                  67.67  \n",
       "word2vec + sswe                       69.11  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrangling the results\n",
    "results_dep_sent['lexicon'] = results_dep_sent['param_union__left_s__filter__lexicon'].apply(lambda lex: lex.name)\n",
    "results_dep_sent_hu = results_dep_sent[results_dep_sent['lexicon'] == 'HuLiu']\n",
    "results_dep_sent_hu_mpqa = results_dep_sent[results_dep_sent['lexicon'] == 'Mpqa HuLiu']\n",
    "grid_results = {'Target-Ind' : results_ind, 'Target-Dep-' : results_depc, 'Target-Dep' : results_dep, \n",
    "                'Target-Dep+: Hu Liu' : results_dep_sent_hu, \n",
    "                'Target-Dep+: MPQA + Hu Liu' : results_dep_sent_hu_mpqa}\n",
    "index = ['word2vec', 'sswe', 'word2vec + sswe']\n",
    "columns = list(grid_results.keys())\n",
    "name_map = {'w2v' : 'word2vec', 'sswe' : 'sswe', 'w2vsswe' : 'word2vec + sswe'}\n",
    "vector_results_df = pd.DataFrame(np.zeros((len(index), len(columns))), columns=columns, index=index)\n",
    "for model_name, result in grid_results.items():\n",
    "    vec_col = result.columns[result.columns.map(lambda x: 'vector' in x)==True][0]\n",
    "    get_vec_name = lambda vec_list: ''.join(map(lambda vec: vec.name, vec_list))\n",
    "    result['vector'] = result[vec_col].apply(get_vec_name)\n",
    "    for vec_name, index_name in name_map.items():\n",
    "        score = result.loc[result['vector'] == vec_name]['mean_test_score']\n",
    "        score = score.round(4) * 100\n",
    "        vector_results_df[model_name][index_name] = score\n",
    "vector_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results above using the combination of the two word vectors is best accross all models which is the finding in the original paper. Also that **Target-Dep+** > **Target-Dep** > **Target-Dep-** > **Target-Ind** which is also what the original paper found. However un-like the original paper we found that using the *SSWE* word vectors to be generally worse than using the *Word2Vec* vectors showing that using just semantic information is more important than using a vector model that was created by reducing the semantic and sentiment loss. Also we found that using **Hu & Liu** lexicon to be better than any other and any other combination of lexicons compared to the original paper which found using the combination of **MPQA and Hu & Liu** to be the best. Finally we can see that we got similar results to the original.\n",
    "\n",
    "# Results of the final models on the test data\n",
    "Here we show the affect of the **Target-Ind**, **Target-Dep**, and **Target-Dep+** models on the test data as reported in the paper where each model uses the best parameters found in the previous tests.\n",
    "\n",
    "For the **Target-Dep+** we show using **Hu & Liu** lexicon and using the combination of **MPQA and Hu & Liu** for direct comparison with the original paper as they found using **MPQA and Hu & Liu** to be better than **Hu & Liu** however we did not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dep_plus_mpqa = TargetDepSent()\n",
    "\n",
    "dong_test = dong(full_path(read_config('dong_twit_test_data')))\n",
    "test_data = dong_test.data()\n",
    "test_y = dong_test.sentiment_data()\n",
    "\n",
    "best_params_ind = target_ind.get_params(word_vector=[w2v, sswe], random_state=42)\n",
    "best_params_dep = target_dep.get_params(word_vector=[w2v, sswe], random_state=42)\n",
    "best_params_dep_sent_hu = target_dep_plus.get_params(word_vector=[w2v, sswe], senti_lexicon=hu_liu_low,\n",
    "                                                     random_state=42)\n",
    "best_params_dep_sent_mpqa = target_dep_plus_mpqa.get_params(word_vector=[w2v, sswe], senti_lexicon=mpqa_huliu_low,\n",
    "                                                       random_state=42)\n",
    "\n",
    "target_ind.fit(train_data, train_y, params=best_params_ind)\n",
    "target_dep.fit(train_data, train_y, params=best_params_dep)\n",
    "target_dep_plus.fit(train_data, train_y, params=best_params_dep_sent_hu)\n",
    "target_dep_plus_mpqa.fit(train_data, train_y, params=best_params_dep_sent_mpqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ind_res = target_ind.predict(test_data)\n",
    "target_dep_res = target_dep.predict(test_data)\n",
    "target_dep_plus_res_hu = target_dep_plus.predict(test_data)\n",
    "target_dep_plus_res_mpqa = target_dep_plus_mpqa.predict(test_data)\n",
    "\n",
    "results = [target_ind_res, target_dep_res, target_dep_plus_res_mpqa, target_dep_plus_res_hu]\n",
    "scorers = {'acc' : accuracy_score, 'F1' : f1_score}\n",
    "final_results_dict = {'Our results (Acc)' : [], 'Our results (Macro F1)' : []}\n",
    "for result in results:\n",
    "    for scorer_name, scorer in scorers.items():\n",
    "        if scorer_name == 'F1':\n",
    "            score = round(scorer(test_y, result, average='macro') * 100, 1)\n",
    "            final_results_dict['Our results (Macro F1)'].append(score)\n",
    "        else:\n",
    "            score = round(scorer(test_y, result) * 100, 1)\n",
    "            final_results_dict['Our results (Acc)'].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Our results (Acc)</th>\n",
       "      <th>Paper results (Acc)</th>\n",
       "      <th>Our results (Macro F1)</th>\n",
       "      <th>Paper results (Macro F1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Target-Ind</th>\n",
       "      <td>66.0</td>\n",
       "      <td>67.3</td>\n",
       "      <td>61.9</td>\n",
       "      <td>66.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target-Dep</th>\n",
       "      <td>69.7</td>\n",
       "      <td>69.7</td>\n",
       "      <td>66.7</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target-Dep+ (MPQA &amp; Hu Liu)</th>\n",
       "      <td>69.9</td>\n",
       "      <td>71.1</td>\n",
       "      <td>67.6</td>\n",
       "      <td>69.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target-Dep+ (Hu Liu)</th>\n",
       "      <td>70.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Our results (Acc)  Paper results (Acc)  \\\n",
       "Target-Ind                                66.0                 67.3   \n",
       "Target-Dep                                69.7                 69.7   \n",
       "Target-Dep+ (MPQA & Hu Liu)               69.9                 71.1   \n",
       "Target-Dep+ (Hu Liu)                      70.8                  0.0   \n",
       "\n",
       "                             Our results (Macro F1)  Paper results (Macro F1)  \n",
       "Target-Ind                                     61.9                      66.4  \n",
       "Target-Dep                                     66.7                      68.0  \n",
       "Target-Dep+ (MPQA & Hu Liu)                    67.6                      69.9  \n",
       "Target-Dep+ (Hu Liu)                           68.7                       0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = ['Target-Ind', 'Target-Dep', 'Target-Dep+ (MPQA & Hu Liu)', 'Target-Dep+ (Hu Liu)']\n",
    "final_results_dict['Paper results (Acc)'] = [67.3, 69.7, 71.1, 0.]\n",
    "final_results_dict['Paper results (Macro F1)'] = [66.4, 68.0, 69.9, 0.]\n",
    "final_results_df = pd.DataFrame(final_results_dict, index=index)\n",
    "final_results_df[['Our results (Acc)', 'Paper results (Acc)', \n",
    "                  'Our results (Macro F1)', 'Paper results (Macro F1)']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above our results are very close to those reported in the paper and are identical for **Target-Dep** model. Also as you can see that our results using the **Hu Liu** lexicon are much better and are closer to the results of **Target-Dep+** in the original paper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning\n",
    "We want to see if instead of using the C-Value reported in the paper, we fine tune our-seleves the C-Value where we use the combination of **Word2Vec** and **SSWE** embeddings which was never done/shown in the paper to see if we get values closer to those reported in the paper. We only do this for the best model (**Target-Dep+ (Hu Liu)**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_grid_params = {'word_vectors' : [[w2v, sswe]], 'random_state' : 42, 'senti_lexicons' : [hu_liu_low]}\n",
    "best_c, _ = target_dep_plus.find_best_c(train_data, train_y, grid_params=c_grid_params, cv=5, n_jobs=5)\n",
    "best_params = target_dep_plus.get_params(word_vector=[w2v, sswe], senti_lexicon=hu_liu_low,\n",
    "                                         random_state=42, C=best_c)\n",
    "target_dep_plus.fit(train_data, train_y, params=best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target-Dep+</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Our results (Acc)</th>\n",
       "      <td>70.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paper results (Acc)</th>\n",
       "      <td>71.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Our results (Macro F1)</th>\n",
       "      <td>68.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paper results (Macro F1)</th>\n",
       "      <td>69.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Target-Dep+\n",
       "Our results (Acc)                70.7\n",
       "Paper results (Acc)              71.1\n",
       "Our results (Macro F1)           68.2\n",
       "Paper results (Macro F1)         69.9"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dep_plus_res = target_dep_plus.predict(test_data)\n",
    "norm_score = lambda score: round(score * 100, 1)\n",
    "score_acc = norm_score(accuracy_score(test_y, target_dep_plus_res))\n",
    "score_f1 = norm_score(f1_score(test_y, target_dep_plus_res, average='macro') )\n",
    "\n",
    "index = ['Our results (Acc)', 'Paper results (Acc)', 'Our results (Macro F1)', 'Paper results (Macro F1)']\n",
    "fine_tune_results_df = pd.DataFrame({'Target-Dep+' : [score_acc, 71.1, score_f1, 69.9]}, index=index)\n",
    "fine_tune_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the results above we have no improvement we actually got slightly worse results this could be due to the train data not perfectly representing the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems encountered when reproducing results\n",
    "When reproducing these methods the main errors we came across where the following:\n",
    "1. Not explicitly stating if the data has to be **scaled/normalised**\n",
    "2. Not stating that all text should be **lower cased**\n",
    "\n",
    "Both of these were not stated in the paper. We show the affects of not doing these to the results below using what we found to be the best performing model (**Target-Dep+ (Hu Liu)**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = target_dep_plus.get_params(word_vector=[w2v, sswe], senti_lexicon=hu_liu_low,\n",
    "                                         random_state=42, scale=False)\n",
    "target_dep_plus.fit(train_data, train_y, params=best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target-Dep+</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Not scaled (Acc)</th>\n",
       "      <td>44.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scaled (Acc)</th>\n",
       "      <td>70.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paper results (Acc)</th>\n",
       "      <td>71.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not scaled (Macro F1)</th>\n",
       "      <td>40.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scaled (Macro F1)</th>\n",
       "      <td>68.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paper results (Macro F1)</th>\n",
       "      <td>69.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Target-Dep+\n",
       "Not scaled (Acc)                 44.4\n",
       "Scaled (Acc)                     70.8\n",
       "Paper results (Acc)              71.1\n",
       "Not scaled (Macro F1)            40.6\n",
       "Scaled (Macro F1)                68.7\n",
       "Paper results (Macro F1)         69.9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dep_plus_res = target_dep_plus.predict(test_data)\n",
    "norm_score = lambda score: round(score * 100, 1)\n",
    "score_not_scale_acc = norm_score(accuracy_score(test_y, target_dep_plus_res))\n",
    "score_not_scale_f1 = norm_score(f1_score(test_y, target_dep_plus_res, average='macro') )\n",
    "score_scaled_acc = round(final_results_df['Our results (Acc)']['Target-Dep+ (Hu Liu)'], 1)\n",
    "score_scaled_f1 = round(final_results_df['Our results (Macro F1)']['Target-Dep+ (Hu Liu)'], 1)\n",
    "\n",
    "index = ['Not scaled (Acc)', 'Scaled (Acc)', 'Paper results (Acc)', \n",
    "         'Not scaled (Macro F1)', 'Scaled (Macro F1)', 'Paper results (Macro F1)']\n",
    "scaled_results_df = pd.DataFrame({'Target-Dep+' : [score_not_scale_acc, score_scaled_acc, 71.1, \n",
    "                                                   score_not_scale_f1, score_scaled_f1, 69.9]}, index=index)\n",
    "scaled_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "str(score_scaled_acc - score_not_scale_acc)": "26.4"
    }
   },
   "source": [
    "As you can see not scaling the data (in this can we used [MinMax](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) scaling) affects the results by around ~{{str(score_scaled_acc - score_not_scale_acc)}}%. This was not stated in the paper. They did use a different Support Vector Machine Library [LibLinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/) however we actually use this library just using the [Scikit-learn interface](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html). Even though in the [Practical guide to LibLinear](https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf) it states that you should scale and shows the importance like we have done above but this is not stated or reiterated in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = target_dep_plus.get_params(word_vector=[w2v, sswe], senti_lexicon=hu_liu,\n",
    "                                         random_state=42, lower=False)\n",
    "target_dep_plus.fit(train_data, train_y, params=best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target-Dep+</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Not lower cased (Acc)</th>\n",
       "      <td>68.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lower cased (Acc)</th>\n",
       "      <td>70.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paper results (Acc)</th>\n",
       "      <td>71.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not lower cased (Macro F1)</th>\n",
       "      <td>65.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lower cased (Macro F1)</th>\n",
       "      <td>68.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paper results (Macro F1)</th>\n",
       "      <td>69.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Target-Dep+\n",
       "Not lower cased (Acc)              68.6\n",
       "Lower cased (Acc)                  70.8\n",
       "Paper results (Acc)                71.1\n",
       "Not lower cased (Macro F1)         65.2\n",
       "Lower cased (Macro F1)             68.7\n",
       "Paper results (Macro F1)           69.9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dep_plus_res = target_dep_plus.predict(test_data)\n",
    "norm_score = lambda score: round(score * 100, 1)\n",
    "score_not_lower_acc = norm_score(accuracy_score(test_y, target_dep_plus_res))\n",
    "score_not_lower_f1 = norm_score(f1_score(test_y, target_dep_plus_res, average='macro'))\n",
    "score_lower_acc = score_scaled_acc\n",
    "score_lower_f1 = score_scaled_f1\n",
    "\n",
    "index = ['Not lower cased (Acc)', 'Lower cased (Acc)', 'Paper results (Acc)', \n",
    "         'Not lower cased (Macro F1)', 'Lower cased (Macro F1)', 'Paper results (Macro F1)']\n",
    "pd.DataFrame({'Target-Dep+' : [score_not_lower_acc, score_lower_acc, 71.1, \n",
    "                               score_not_lower_f1, score_lower_f1, 69.9]}, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "str(score_lower_acc - score_not_lower_acc)": "2.2"
    }
   },
   "source": [
    "As you can see above the results change slightly (~{{str(score_lower_acc - score_not_lower_acc)}}%). This is because the word embeddings used have all been pre-processed and lowered thus causing this affect. Lower casing words in Sentiment Analysis loses some information, as you would expect `GREAT` to be more positive than `great` when lower casing all the words you lose this information. This process is normally done to remove sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
