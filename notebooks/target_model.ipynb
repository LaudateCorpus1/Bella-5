{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Model\n",
    "from tdparse.models.target import TargetInd\n",
    "from tdparse.models.target import TargetDepC\n",
    "from tdparse.models.target import TargetDep\n",
    "from tdparse.models.target import TargetDepSent\n",
    "# Word Vector methods\n",
    "from tdparse.word_vectors import GensimVectors\n",
    "from tdparse.word_vectors import PreTrained\n",
    "from tdparse.helper import read_config, full_path\n",
    "# Sentiment lexicons\n",
    "from tdparse import lexicons\n",
    "# Get the data\n",
    "from tdparse.parsers import dong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target dependent models\n",
    "This notebook shows how to use the target dependent models and comparing the results of our implementation to the one in the original [paper](https://www.ijcai.org/Proceedings/15/Papers/194.pdf)\n",
    "\n",
    "The paper had four different models:\n",
    "1. **Target-Ind** -- Uses only the full Tweet as context.\n",
    "2. **Target-Dep-** -- Uses the left and right context of the target word as well as the target word as context.\n",
    "3. **Target-Dep** -- Uses all of the above contexts.\n",
    "4. **Target-Dep+** -- Uses all of the above as well as including two additional left and right contexts which ignores all words in the contexts unless they are part of the given sentiment lexicon (or any lexicon).\n",
    "\n",
    "The above models correspond to the following classes in our implementation:\n",
    "1. [TargetInd](../tdparse/models/target.py), 2. [TargetDepC](../tdparse/models/target.py), 3. [TargetDep](../tdparse/models/target.py), 4. [TargetDepSent](../tdparse/models/target.py)\n",
    "\n",
    "All of the results shown below are 5 fold cross validation over the training data of [Dong et al.](https://aclanthology.coli.uni-saarland.de/papers/P14-2009/p14-2009) as reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = full_path(read_config('dong_twit_train_data'))\n",
    "train_data = dong(train_data)\n",
    "train_y = [target_dict['sentiment'] for target_dict in train_data]\n",
    "\n",
    "# Get word vectors\n",
    "w2v_path = full_path(read_config('word2vec_files')['vo_zhang'])\n",
    "w2v = GensimVectors(w2v_path, None, model='word2vec', name='w2v')\n",
    "sswe_path = full_path(read_config('sswe_files')['vo_zhang'])\n",
    "sswe = PreTrained(sswe_path, name='sswe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the three base models\n",
    "\n",
    "In the paper the base models (target-ind, target-dep- and target-dep) using the the word2vec word vectors were compared after they found the best C-values therefore we are going to use the C-Values stated in the paper to compare our results to theres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances of the models\n",
    "target_ind = TargetInd()\n",
    "target_depc = TargetDepC()\n",
    "target_dep = TargetDep()\n",
    "# Getting the grid parameters for each model\n",
    "grid_params_ind = target_ind.get_cv_params(word_vectors=[[w2v]], random_state=42)\n",
    "grid_params_depc = target_depc.get_cv_params(word_vectors=[[w2v]], random_state=42)\n",
    "grid_params_dep = target_dep.get_cv_params(word_vectors=[[w2v]], random_state=42)\n",
    "# Running the grid search over 5 folds.\n",
    "results_ind = target_ind.grid_search(train_data, train_y, params=grid_params_ind, cv=5, n_jobs=1)\n",
    "results_depc = target_depc.grid_search(train_data, train_y, params=grid_params_depc, cv=5, n_jobs=1)\n",
    "results_dep = target_dep.grid_search(train_data, train_y, params=grid_params_dep, cv=5, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Our results</th>\n",
       "      <th>Paper results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Target-Ind</th>\n",
       "      <td>61.01</td>\n",
       "      <td>59.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target-Dep-</th>\n",
       "      <td>65.70</td>\n",
       "      <td>65.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target-Dep</th>\n",
       "      <td>66.87</td>\n",
       "      <td>65.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Our results  Paper results\n",
       "Target-Ind         61.01          59.22\n",
       "Target-Dep-        65.70          65.38\n",
       "Target-Dep         66.87          65.72"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [results_ind['mean_test_score'], results_depc['mean_test_score'], results_dep['mean_test_score']]\n",
    "all_results = {'Our results' : [result.round(4)[0] * 100 for result in results]}\n",
    "all_results['Paper results'] = [59.22, 65.38, 65.72]\n",
    "index = ['Target-Ind', 'Target-Dep-', 'Target-Dep']\n",
    "base_model_df = pd.DataFrame(all_results, index=index)\n",
    "base_model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the results above that we get similar results and the order of the models stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target-Dep+ and sentiment lexicons\n",
    "The **Target-Dep+** model uses sentiment lexicons to remove words therefore in this section we compare:\n",
    "1. The statistics on the sentiment lexicons\n",
    "2. The results of the model using different lexicons\n",
    "\n",
    "All the experiments below again use the Word2Vec word embeddings.\n",
    "## Sentiment lexicon statistics\n",
    "\n",
    "Below we present the size of the sentiment lexicon once it has been processed and the size of that lexicon stated in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentiment lexicons and remove all words that are not associated\n",
    "# to the Positive or Negative class.\n",
    "subset_cats = {'positive', 'negative'}\n",
    "mpqa = lexicons.Mpqa(subset_cats=subset_cats)\n",
    "nrc = lexicons.NRC(subset_cats=subset_cats)\n",
    "hu_liu = lexicons.HuLiu(subset_cats=subset_cats)\n",
    "# Combine sentiment lexicons - Removes words that contradict each other.\n",
    "mpqa_huliu = lexicons.Lexicon.combine_lexicons(mpqa, hu_liu)\n",
    "all_three = lexicons.Lexicon.combine_lexicons(mpqa_huliu, nrc)\n",
    "\n",
    "# Load the sentiment lexicons but lower case all the words\n",
    "mpqa_low = lexicons.Mpqa(subset_cats=subset_cats, lower=True)\n",
    "nrc_low = lexicons.NRC(subset_cats=subset_cats, lower=True)\n",
    "hu_liu_low = lexicons.HuLiu(subset_cats=subset_cats, lower=True)\n",
    "mpqa_huliu_low = lexicons.Lexicon.combine_lexicons(mpqa_low, hu_liu_low)\n",
    "all_three_low = lexicons.Lexicon.combine_lexicons(mpqa_huliu_low, nrc_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper No. Positive</th>\n",
       "      <th>Ours No. Positive</th>\n",
       "      <th>Ours low No. Positive</th>\n",
       "      <th>Paper No. Negative</th>\n",
       "      <th>Ours No. Negative</th>\n",
       "      <th>Ours low No. Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MPQA</th>\n",
       "      <td>2289</td>\n",
       "      <td>2304</td>\n",
       "      <td>2304</td>\n",
       "      <td>4114</td>\n",
       "      <td>4154</td>\n",
       "      <td>4154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hu Liu</th>\n",
       "      <td>2003</td>\n",
       "      <td>2006</td>\n",
       "      <td>2006</td>\n",
       "      <td>4780</td>\n",
       "      <td>4783</td>\n",
       "      <td>4783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NRC</th>\n",
       "      <td>2231</td>\n",
       "      <td>2312</td>\n",
       "      <td>2312</td>\n",
       "      <td>3243</td>\n",
       "      <td>3324</td>\n",
       "      <td>3324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MPQA &amp; Hu Liu</th>\n",
       "      <td>2706</td>\n",
       "      <td>2725</td>\n",
       "      <td>2725</td>\n",
       "      <td>5069</td>\n",
       "      <td>5077</td>\n",
       "      <td>5073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All Three</th>\n",
       "      <td>3940</td>\n",
       "      <td>4043</td>\n",
       "      <td>4043</td>\n",
       "      <td>6490</td>\n",
       "      <td>6548</td>\n",
       "      <td>6544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Paper No. Positive  Ours No. Positive  Ours low No. Positive  \\\n",
       "MPQA                         2289               2304                   2304   \n",
       "Hu Liu                       2003               2006                   2006   \n",
       "NRC                          2231               2312                   2312   \n",
       "MPQA & Hu Liu                2706               2725                   2725   \n",
       "All Three                    3940               4043                   4043   \n",
       "\n",
       "               Paper No. Negative  Ours No. Negative  Ours low No. Negative  \n",
       "MPQA                         4114               4154                   4154  \n",
       "Hu Liu                       4780               4783                   4783  \n",
       "NRC                          3243               3324                   3324  \n",
       "MPQA & Hu Liu                5069               5077                   5073  \n",
       "All Three                    6490               6548                   6544  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_cat(lexicon, filter_cat):\n",
    "    return [word for word, cat in lexicon.lexicon if cat == filter_cat]\n",
    "\n",
    "all_lexicons = [mpqa, hu_liu, nrc, mpqa_huliu, all_three]\n",
    "num_positive = [len(filter_cat(lexicon, 'positive')) for lexicon in all_lexicons]\n",
    "num_negative = [len(filter_cat(lexicon, 'negative')) for lexicon in all_lexicons]\n",
    "\n",
    "all_lexicons_low = [mpqa_low, hu_liu_low, nrc_low, mpqa_huliu_low, all_three_low]\n",
    "num_positive_low = [len(filter_cat(lexicon, 'positive')) for lexicon in all_lexicons_low]\n",
    "num_negative_low = [len(filter_cat(lexicon, 'negative')) for lexicon in all_lexicons_low]\n",
    "\n",
    "columns = ['Paper No. Positive', 'Ours No. Positive', 'Ours low No. Positive', \n",
    "           'Paper No. Negative', 'Ours No. Negative', 'Ours low No. Negative']\n",
    "index = ['MPQA', 'Hu Liu', 'NRC', 'MPQA & Hu Liu', 'All Three']\n",
    "data = [[2289, 2003, 2231, 2706, 3940], num_positive, num_positive_low, \n",
    "        [4114, 4780, 3243, 5069, 6490], num_negative, num_negative_low]\n",
    "senti_info = dict(list(zip(columns, data)))\n",
    "pd.DataFrame(senti_info, columns=columns, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anti-US', 'anti-Semites', 'anti-American', 'anti-Israeli']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words that are shared between the MPQA and Hu Liu sentiment lexicons\n",
    "[word for word, cat in list(set(mpqa_huliu.lexicon).difference(set(mpqa_huliu_low.lexicon))) if cat == 'negative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we never agree on the number of words within the lexicons. We get the lexicons from the sources described in the paper. Intrestingly if we do not lower case the words in the lexicons we won't see the same similarities between the MPQA and Hu Liu sentiment lexicon as they both share the words above just the Hu Liu lexicon has the words lower cased already where as MPQA has not.\n",
    "\n",
    "## Showing the affect of using different sentiment lexicons in the Target-Dep+ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances of the model\n",
    "target_dep_plus = TargetDepSent()\n",
    "# Getting the grid parameters for each model\n",
    "grid_params_sent = target_dep_plus.get_cv_params(word_vectors=[[w2v]], senti_lexicons=all_lexicons_low,\n",
    "                                                 random_state=42)\n",
    "# Running the grid search over 5 folds.\n",
    "results_sent = target_dep_plus.grid_search(train_data, train_y, params=grid_params_sent, cv=5, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sent_results = {'Paper results' : [65.72, 66.05, 67.24, 65.56, 67.40, 67.30],\n",
    "                    'Our results' : np.zeros(6)}\n",
    "index = ['Target-Dep', 'Target-Dep+: NRC', 'Target-Dep+: Hu Liu', 'Target-Dep+: MPQA',\n",
    "         'Target-Dep+: MPQA + Hu Liu', 'Target-Dep+: All Three']\n",
    "sent_results_df = pd.DataFrame(all_sent_results, index=index)\n",
    "sent_results_df['Our results']['Target-Dep'] = base_model_df['Our results']['Target-Dep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Our results</th>\n",
       "      <th>Paper results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Target-Dep</th>\n",
       "      <td>66.87</td>\n",
       "      <td>65.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target-Dep+: NRC</th>\n",
       "      <td>66.90</td>\n",
       "      <td>66.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target-Dep+: Hu Liu</th>\n",
       "      <td>68.31</td>\n",
       "      <td>67.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target-Dep+: MPQA</th>\n",
       "      <td>67.05</td>\n",
       "      <td>65.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target-Dep+: MPQA + Hu Liu</th>\n",
       "      <td>68.13</td>\n",
       "      <td>67.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target-Dep+: All Three</th>\n",
       "      <td>67.93</td>\n",
       "      <td>67.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Our results  Paper results\n",
       "Target-Dep                        66.87          65.72\n",
       "Target-Dep+: NRC                  66.90          66.05\n",
       "Target-Dep+: Hu Liu               68.31          67.24\n",
       "Target-Dep+: MPQA                 67.05          65.56\n",
       "Target-Dep+: MPQA + Hu Liu        68.13          67.40\n",
       "Target-Dep+: All Three            67.93          67.30"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_map = {'Mpqa' : 'Target-Dep+: MPQA', 'HuLiu' : 'Target-Dep+: Hu Liu', 'NRC' : 'Target-Dep+: NRC',\n",
    "            'Mpqa HuLiu' : 'Target-Dep+: MPQA + Hu Liu', 'Mpqa HuLiu NRC' : 'Target-Dep+: All Three'}\n",
    "results_sent['lexicon'] = results_sent['param_union__left_s__filter__lexicon'].apply(lambda lex: lex.name)\n",
    "for lex_name, model_name in name_map.items():\n",
    "    score = results_sent.loc[results_sent['lexicon'] == lex_name]['mean_test_score']\n",
    "    score = score.round(4) * 100\n",
    "    sent_results_df['Our results'][model_name] = score\n",
    "sent_results_df['Our results']['Target-Dep'] = base_model_df['Our results']['Target-Dep']\n",
    "sent_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results shown above we get different results but the results also have a different rank between the lexicons as in the best lexicon was **Hu and Liu** where as the papers original results show the combination of **MPQA and Hu & Liu** was the best. However in general we can see that it is better to use a sentiment lexicon than not. Also that both our implmentation and the original paper show that the best single sentiment lexicon is **Hu & Liu** and that using **all three** sentiment lexicons is worse than using **MPQA and Hu & Liu**.\n",
    "\n",
    "# Showing the affect of the different word vectors\n",
    "As presented in the paper they show the affect of using different word vectors accross the four models using the best sentiment lexicon for the sentiment dependent model. As we had different result for the sentiment lexicons compared to the original paper we will show the results of using Hu & Liu lexicon and using the combination of Hu & Liu and MPQA. The word vectors used are the following:\n",
    "1. Word2Vec - Which has been used throughout the previous experiments (100 dimensions)\n",
    "2. SSWE - Sentiment Specific Word Embeddings (50 dimensions)\n",
    "3. Concatenation of Word2vec and SSWE (150 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the results\n",
    "grid_params_ind = target_ind.get_cv_params(word_vectors=[[w2v], [sswe], [w2v, sswe]], random_state=42)\n",
    "grid_params_depc = target_depc.get_cv_params(word_vectors=[[w2v], [sswe], [w2v, sswe]], random_state=42)\n",
    "grid_params_dep = target_dep.get_cv_params(word_vectors=[[w2v], [sswe], [w2v, sswe]], random_state=42)\n",
    "grid_params_dep_sent = target_dep_plus.get_cv_params(word_vectors=[[w2v], [sswe], [w2v, sswe]], \n",
    "                                                     senti_lexicons=[hu_liu_low, mpqa_huliu_low], random_state=42)\n",
    "\n",
    "results_ind = target_ind.grid_search(train_data, train_y, params=grid_params_ind, cv=5, n_jobs=4)\n",
    "results_depc = target_depc.grid_search(train_data, train_y, params=grid_params_depc, cv=5, n_jobs=4)\n",
    "results_dep = target_dep.grid_search(train_data, train_y, params=grid_params_dep, cv=5, n_jobs=4)\n",
    "results_dep_sent = target_dep_plus.grid_search(train_data, train_y, params=grid_params_dep_sent, cv=5, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target-Ind</th>\n",
       "      <th>Target-Dep-</th>\n",
       "      <th>Target-Dep</th>\n",
       "      <th>Target-Dep+: Hu Liu</th>\n",
       "      <th>Target-Dep+: MPQA + Hu Liu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word2vec</th>\n",
       "      <td>61.01</td>\n",
       "      <td>65.70</td>\n",
       "      <td>66.87</td>\n",
       "      <td>68.31</td>\n",
       "      <td>68.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sswe</th>\n",
       "      <td>60.37</td>\n",
       "      <td>66.68</td>\n",
       "      <td>66.68</td>\n",
       "      <td>67.97</td>\n",
       "      <td>67.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec + sswe</th>\n",
       "      <td>63.59</td>\n",
       "      <td>67.01</td>\n",
       "      <td>67.73</td>\n",
       "      <td>69.22</td>\n",
       "      <td>68.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Target-Ind  Target-Dep-  Target-Dep  Target-Dep+: Hu Liu  \\\n",
       "word2vec              61.01        65.70       66.87                68.31   \n",
       "sswe                  60.37        66.68       66.68                67.97   \n",
       "word2vec + sswe       63.59        67.01       67.73                69.22   \n",
       "\n",
       "                 Target-Dep+: MPQA + Hu Liu  \n",
       "word2vec                              68.13  \n",
       "sswe                                  67.57  \n",
       "word2vec + sswe                       68.34  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrangling the results\n",
    "results_dep_sent['lexicon'] = results_dep_sent['param_union__left_s__filter__lexicon'].apply(lambda lex: lex.name)\n",
    "results_dep_sent_hu = results_dep_sent[results_dep_sent['lexicon'] == 'HuLiu']\n",
    "results_dep_sent_hu_mpqa = results_dep_sent[results_dep_sent['lexicon'] == 'Mpqa HuLiu']\n",
    "grid_results = {'Target-Ind' : results_ind, 'Target-Dep-' : results_depc, 'Target-Dep' : results_dep, \n",
    "                'Target-Dep+: Hu Liu' : results_dep_sent_hu, \n",
    "                'Target-Dep+: MPQA + Hu Liu' : results_dep_sent_hu_mpqa}\n",
    "index = ['word2vec', 'sswe', 'word2vec + sswe']\n",
    "columns = list(grid_results.keys())\n",
    "name_map = {'w2v' : 'word2vec', 'sswe' : 'sswe', 'w2vsswe' : 'word2vec + sswe'}\n",
    "vector_results_df = pd.DataFrame(np.zeros((len(index), len(columns))), columns=columns, index=index)\n",
    "for model_name, result in grid_results.items():\n",
    "    vec_col = result.columns[result.columns.map(lambda x: 'vector' in x)==True][0]\n",
    "    get_vec_name = lambda vec_list: ''.join(map(lambda vec: vec.name, vec_list))\n",
    "    result['vector'] = result[vec_col].apply(get_vec_name)\n",
    "    for vec_name, index_name in name_map.items():\n",
    "        score = result.loc[result['vector'] == vec_name]['mean_test_score']\n",
    "        score = score.round(4) * 100\n",
    "        vector_results_df[model_name][index_name] = score\n",
    "vector_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results above using the combination of the two word vectors is best accross all models which is the finding in the original paper. Also that **Target-Dep+** > **Target-Dep** > **Target-Dep-** > **Target-Ind** which is also what the original paper found. However un-like the original paper we found that using the *SSWE* word vectors to be worse than using the *Word2Vec* vectors showing that using just semantic information is more important than using a vector model that was created by reducing the semantic and sentiment loss. Also we found that using **Hu & Liu** lexicon to be better than any other and any other combination of lexicons compared to the original paper which found using the combination of **MPQA and Hu & Liu** to be the best. Finally we can see that we got similar results to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
