{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import time\n",
    "import tempfile\n",
    "import pickle\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Notebook helper methods\n",
    "from tdparse import notebook_helper\n",
    "# Models\n",
    "from tdparse.models.tdlstm import LSTM, TDLSTM, TCLSTM\n",
    "# Tokenisers\n",
    "from tdparse.tokenisers import ark_twokenize\n",
    "# Word Vectors\n",
    "from tdparse.word_vectors import PreTrained, GloveCommonCrawl\n",
    "# Get the data\n",
    "from tdparse.parsers import semeval_14, dong, election\n",
    "from tdparse.data_types import TargetCollection\n",
    "from tdparse.helper import read_config, full_path\n",
    "from tdparse.evaluation import evaluation_results\n",
    "from tdparse.notebook_helper import get_json_data, write_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of the datasets\n",
    "youtubean_train = semeval_14(full_path(read_config('youtubean_train')))\n",
    "youtubean_test = semeval_14(full_path(read_config('youtubean_test')))\n",
    "semeval_14_rest_train = semeval_14(full_path(read_config('semeval_2014_rest_train')))\n",
    "semeval_14_lap_train = semeval_14(full_path(read_config('semeval_2014_lap_train')))\n",
    "semeval_14_rest_test = semeval_14(full_path(read_config('semeval_2014_rest_test')))\n",
    "semeval_14_lap_test = semeval_14(full_path(read_config('semeval_2014_lap_test')))\n",
    "dong_train = dong(full_path(read_config('dong_twit_train_data')))\n",
    "dong_test = dong(full_path(read_config('dong_twit_test_data')))\n",
    "election_train, election_test = election(full_path(read_config('election_folder_dir')))\n",
    "mitchel_train = semeval_14(full_path(read_config('mitchel_train')))\n",
    "mitchel_test = semeval_14(full_path(read_config('mitchel_test')))\n",
    "\n",
    "\n",
    "dataset_train_test = {#'SemEval 14 Laptop' : (semeval_14_lap_train, semeval_14_lap_test),\n",
    "                      #'SemEval 14 Restaurant' : (semeval_14_rest_train, semeval_14_rest_test),\n",
    "                      #'Dong Twitter' : (dong_train, dong_test),\n",
    "                      'Election Twitter' : (election_train, election_test),\n",
    "                      #'YouTuBean' : (youtubean_train, youtubean_test),\n",
    "                      #'Mitchel' : (mitchel_train, mitchel_test)\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the word vectors\n",
    "sswe_path = full_path(read_config('sswe_files')['vo_zhang'])\n",
    "sswe = PreTrained(sswe_path, name='sswe')\n",
    "glove_300 = GloveCommonCrawl(version=42)\n",
    "# Word vectors that we are searching over\n",
    "word_vectors = [sswe, glove_300]\n",
    "\n",
    "\n",
    "# This is required as we have 3 classes and one of them is -1 and when one hot encoded\n",
    "# the index of -1 is 2 and that is what it thinks the label is when it should be \n",
    "# -1 hence the sentiment mapper\n",
    "sentiment_mapper = {0 : 0, 1 : 1, 2 : -1}\n",
    "\n",
    "# Folder to store all the sub folder for each model\n",
    "result_folder = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results', 'TDLstm'))\n",
    "# Folder to store all of the saved models (model zoo folder)\n",
    "model_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'model zoo'))\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_predictions(train, test, dataset_name, model_class, \n",
    "                        word_vector_file_path, result_file_path,\n",
    "                        model_folder_path, model_params):\n",
    "    \n",
    "    print('{} {}'.format(dataset_name, model_params))\n",
    "\n",
    "    data_train = train.data_dict()\n",
    "    y_train = train.sentiment_data()\n",
    "    data_test = test.data_dict()\n",
    "    y_test = test.sentiment_data()\n",
    "    \n",
    "    # Fits the model\n",
    "    word_vector_data = get_json_data(word_vector_file_path, dataset_name)\n",
    "    best_score = 0\n",
    "    best_word_vector = None\n",
    "    best_model = None\n",
    "    for word_vector in word_vectors:\n",
    "        print(word_vector)\n",
    "        word_vector_name = '{}'.format(word_vector)\n",
    "        if word_vector_name in word_vector_data:\n",
    "            word_vec_val_score = word_vector_data[word_vector_name]\n",
    "            if word_vec_val_score > best_score:\n",
    "                best_score = word_vec_val_score\n",
    "                best_word_vector = word_vector\n",
    "            continue\n",
    "        model_params['embeddings'] = word_vector\n",
    "        model = model_class(**model_params)\n",
    "        print('{} {}'.format(model_params, word_vector))\n",
    "        history = model.fit(data_train, y_train, validation_size=0.3, verbose=1,\n",
    "                            reproducible=True, patience=10, epochs=300, org_initialisers=True)\n",
    "        word_vec_val_score = max(history.history['val_acc'])\n",
    "        word_vector_data[word_vector_name] = word_vec_val_score\n",
    "        if word_vec_val_score > best_score:\n",
    "                best_score = word_vec_val_score\n",
    "                best_word_vector = word_vector\n",
    "                best_model = model\n",
    "                \n",
    "        # Save word vector validation score result\n",
    "        write_json_data(word_vector_file_path, dataset_name, word_vector_data)\n",
    "    if best_word_vector is None:\n",
    "        raise ValueError('best word vector should not be None')\n",
    "    if best_model is None:\n",
    "        model_params['embeddings'] = best_word_vector\n",
    "        model = model_class(**model_params)\n",
    "        print('{} {}'.format(model_params, best_word_vector))\n",
    "        model.fit(data_train, y_train, validation_size=0.3, verbose=1,\n",
    "                  reproducible=True, patience=10, epochs=300, org_initialisers=True)\n",
    "    # Saves the model to the model zoo\n",
    "    model_folder_join = lambda file_name: os.path.join(model_folder_path, file_name)\n",
    "    model_arch_fp = model_folder_join('{} {} architecture'.format(model, dataset_name))\n",
    "    model_weights_fp = model_folder_join('{} {} weights'.format(model, dataset_name))\n",
    "    model.save_model(model_arch_fp, model_weights_fp, verbose=1)\n",
    "    \n",
    "    # Predicts on the test data\n",
    "    predicted_values = model.predict(data_test)\n",
    "    # Convert prediction from one hot encoded to category value e.g. -1, 0, 1\n",
    "    predicted_values_cats =  model.prediction_to_cats(y_test, predicted_values, \n",
    "                                                      mapper=sentiment_mapper)\n",
    "    # Evaluates the predictions and save the results\n",
    "    return evaluation_results(predicted_values_cats, test, dataset_name, \n",
    "                              file_name=result_file_path, \n",
    "                              save_raw_data=True, re_write=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass Evaluation of the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Election Twitter {'tokeniser': <function ark_twokenize at 0x7fd7b4194950>, 'lower': True, 'pad_size': -1}\n",
      "sswe\n",
      "glove 300d 42b common crawl\n",
      "{'tokeniser': <function ark_twokenize at 0x7fd7b4194950>, 'lower': True, 'pad_size': -1, 'embeddings': glove 300d 42b common crawl} glove 300d 42b common crawl\n",
      "Train on 6550 samples, validate on 2808 samples\n",
      "Epoch 1/300\n",
      "6550/6550 [==============================] - 84s 13ms/step - loss: 1.0603 - acc: 0.4655 - val_loss: 1.0343 - val_acc: 0.4676\n",
      "Epoch 2/300\n",
      "6550/6550 [==============================] - 87s 13ms/step - loss: 1.0225 - acc: 0.4678 - val_loss: 1.0141 - val_acc: 0.4676\n",
      "Epoch 3/300\n",
      "6550/6550 [==============================] - 85s 13ms/step - loss: 1.0100 - acc: 0.4678 - val_loss: 1.0070 - val_acc: 0.4676\n",
      "Epoch 4/300\n",
      "6550/6550 [==============================] - 88s 13ms/step - loss: 1.0056 - acc: 0.4678 - val_loss: 1.0045 - val_acc: 0.4676\n",
      "Epoch 5/300\n",
      "6550/6550 [==============================] - 90s 14ms/step - loss: 1.0041 - acc: 0.4678 - val_loss: 1.0037 - val_acc: 0.4676\n",
      "Epoch 6/300\n",
      "6550/6550 [==============================] - 95s 15ms/step - loss: 1.0036 - acc: 0.4678 - val_loss: 1.0034 - val_acc: 0.4676\n",
      "Epoch 7/300\n",
      "6550/6550 [==============================] - 95s 15ms/step - loss: 1.0033 - acc: 0.4678 - val_loss: 1.0032 - val_acc: 0.4676\n",
      "Epoch 8/300\n",
      "6550/6550 [==============================] - 89s 14ms/step - loss: 1.0032 - acc: 0.4678 - val_loss: 1.0031 - val_acc: 0.4676\n",
      "Epoch 9/300\n",
      "6550/6550 [==============================] - 89s 14ms/step - loss: 1.0030 - acc: 0.4678 - val_loss: 1.0030 - val_acc: 0.4676\n",
      "Epoch 10/300\n",
      "6550/6550 [==============================] - 92s 14ms/step - loss: 1.0029 - acc: 0.4678 - val_loss: 1.0029 - val_acc: 0.4676\n",
      "Epoch 11/300\n",
      "6550/6550 [==============================] - 88s 13ms/step - loss: 1.0028 - acc: 0.4678 - val_loss: 1.0028 - val_acc: 0.4676\n",
      "Epoch 12/300\n",
      "6550/6550 [==============================] - 91s 14ms/step - loss: 1.0027 - acc: 0.4678 - val_loss: 1.0027 - val_acc: 0.4676\n",
      "Epoch 13/300\n",
      "6550/6550 [==============================] - 91s 14ms/step - loss: 1.0026 - acc: 0.4678 - val_loss: 1.0026 - val_acc: 0.4676\n",
      "Epoch 14/300\n",
      "6550/6550 [==============================] - 89s 14ms/step - loss: 1.0024 - acc: 0.4678 - val_loss: 1.0025 - val_acc: 0.4676\n",
      "Epoch 15/300\n",
      "6550/6550 [==============================] - 98s 15ms/step - loss: 1.0023 - acc: 0.4678 - val_loss: 1.0024 - val_acc: 0.4676\n",
      "Epoch 16/300\n",
      "6550/6550 [==============================] - 98s 15ms/step - loss: 1.0022 - acc: 0.4678 - val_loss: 1.0023 - val_acc: 0.4676\n",
      "Epoch 17/300\n",
      "6550/6550 [==============================] - 90s 14ms/step - loss: 1.0020 - acc: 0.4678 - val_loss: 1.0022 - val_acc: 0.4676\n",
      "Epoch 18/300\n",
      "6550/6550 [==============================] - 92s 14ms/step - loss: 1.0019 - acc: 0.4678 - val_loss: 1.0020 - val_acc: 0.4676\n",
      "Epoch 19/300\n",
      "6550/6550 [==============================] - 89s 14ms/step - loss: 1.0018 - acc: 0.4678 - val_loss: 1.0019 - val_acc: 0.4676\n",
      "Epoch 20/300\n",
      "6550/6550 [==============================] - 91s 14ms/step - loss: 1.0016 - acc: 0.4678 - val_loss: 1.0018 - val_acc: 0.4690\n",
      "Epoch 21/300\n",
      "6550/6550 [==============================] - 88s 13ms/step - loss: 1.0015 - acc: 0.4689 - val_loss: 1.0017 - val_acc: 0.4690\n",
      "Epoch 22/300\n",
      "6550/6550 [==============================] - 86s 13ms/step - loss: 1.0012 - acc: 0.4689 - val_loss: 1.0016 - val_acc: 0.4697\n",
      "Epoch 23/300\n",
      "6550/6550 [==============================] - 85s 13ms/step - loss: 1.0012 - acc: 0.4687 - val_loss: 1.0014 - val_acc: 0.4697\n",
      "Epoch 24/300\n",
      "6550/6550 [==============================] - 81s 12ms/step - loss: 1.0010 - acc: 0.4698 - val_loss: 1.0013 - val_acc: 0.4697\n",
      "Epoch 25/300\n",
      "6550/6550 [==============================] - 86s 13ms/step - loss: 1.0008 - acc: 0.4708 - val_loss: 1.0011 - val_acc: 0.4722\n",
      "Epoch 26/300\n",
      "6550/6550 [==============================] - 91s 14ms/step - loss: 1.0006 - acc: 0.4737 - val_loss: 1.0010 - val_acc: 0.4722\n",
      "Epoch 27/300\n",
      "6550/6550 [==============================] - 92s 14ms/step - loss: 1.0004 - acc: 0.4733 - val_loss: 1.0009 - val_acc: 0.4722\n",
      "Epoch 28/300\n",
      "6550/6550 [==============================] - 93s 14ms/step - loss: 1.0002 - acc: 0.4727 - val_loss: 1.0007 - val_acc: 0.4740\n",
      "Epoch 29/300\n",
      "6550/6550 [==============================] - 90s 14ms/step - loss: 1.0000 - acc: 0.4753 - val_loss: 1.0006 - val_acc: 0.4729\n",
      "Epoch 30/300\n",
      "6550/6550 [==============================] - 89s 14ms/step - loss: 0.9997 - acc: 0.4754 - val_loss: 1.0004 - val_acc: 0.4751\n",
      "Epoch 31/300\n",
      "6550/6550 [==============================] - 92s 14ms/step - loss: 0.9995 - acc: 0.4802 - val_loss: 1.0003 - val_acc: 0.4726\n",
      "Epoch 32/300\n",
      "6550/6550 [==============================] - 93s 14ms/step - loss: 0.9993 - acc: 0.4777 - val_loss: 1.0000 - val_acc: 0.4758\n",
      "Epoch 33/300\n",
      "6550/6550 [==============================] - 94s 14ms/step - loss: 0.9990 - acc: 0.4817 - val_loss: 0.9999 - val_acc: 0.4751\n",
      "Epoch 34/300\n",
      "6550/6550 [==============================] - 93s 14ms/step - loss: 0.9988 - acc: 0.4824 - val_loss: 0.9997 - val_acc: 0.4779\n",
      "Epoch 35/300\n",
      "6550/6550 [==============================] - 93s 14ms/step - loss: 0.9985 - acc: 0.4873 - val_loss: 0.9995 - val_acc: 0.4786\n",
      "Epoch 36/300\n",
      "6550/6550 [==============================] - 88s 13ms/step - loss: 0.9980 - acc: 0.4896 - val_loss: 0.9993 - val_acc: 0.4776\n",
      "Epoch 37/300\n",
      "6550/6550 [==============================] - 90s 14ms/step - loss: 0.9978 - acc: 0.4887 - val_loss: 0.9990 - val_acc: 0.4808\n",
      "Epoch 38/300\n",
      "6550/6550 [==============================] - 97s 15ms/step - loss: 0.9974 - acc: 0.4902 - val_loss: 0.9988 - val_acc: 0.4868\n",
      "Epoch 39/300\n",
      "6550/6550 [==============================] - 94s 14ms/step - loss: 0.9970 - acc: 0.4939 - val_loss: 0.9985 - val_acc: 0.4872\n",
      "Epoch 40/300\n",
      "6550/6550 [==============================] - 93s 14ms/step - loss: 0.9966 - acc: 0.4956 - val_loss: 0.9983 - val_acc: 0.4861\n",
      "Epoch 41/300\n",
      "6550/6550 [==============================] - 94s 14ms/step - loss: 0.9962 - acc: 0.4942 - val_loss: 0.9979 - val_acc: 0.4929\n",
      "Epoch 42/300\n",
      "6550/6550 [==============================] - 93s 14ms/step - loss: 0.9957 - acc: 0.4998 - val_loss: 0.9975 - val_acc: 0.4939\n",
      "Epoch 43/300\n",
      "6550/6550 [==============================] - 92s 14ms/step - loss: 0.9951 - acc: 0.5003 - val_loss: 0.9972 - val_acc: 0.4950\n",
      "Epoch 44/300\n",
      "6550/6550 [==============================] - 90s 14ms/step - loss: 0.9946 - acc: 0.4982 - val_loss: 0.9967 - val_acc: 0.4950\n",
      "Epoch 45/300\n",
      "6550/6550 [==============================] - 95s 14ms/step - loss: 0.9938 - acc: 0.4994 - val_loss: 0.9965 - val_acc: 0.4879\n",
      "Epoch 46/300\n",
      "6550/6550 [==============================] - 96s 15ms/step - loss: 0.9930 - acc: 0.5035 - val_loss: 0.9959 - val_acc: 0.4936\n",
      "Epoch 47/300\n",
      "6550/6550 [==============================] - 92s 14ms/step - loss: 0.9924 - acc: 0.5035 - val_loss: 0.9953 - val_acc: 0.4922\n",
      "Epoch 48/300\n",
      "6550/6550 [==============================] - 90s 14ms/step - loss: 0.9914 - acc: 0.5021 - val_loss: 0.9948 - val_acc: 0.4911\n",
      "Epoch 49/300\n",
      "6550/6550 [==============================] - 91s 14ms/step - loss: 0.9904 - acc: 0.5061 - val_loss: 0.9947 - val_acc: 0.4918\n",
      "Epoch 50/300\n",
      "6550/6550 [==============================] - 92s 14ms/step - loss: 0.9892 - acc: 0.5104 - val_loss: 0.9944 - val_acc: 0.4900\n",
      "Epoch 51/300\n",
      "6550/6550 [==============================] - 90s 14ms/step - loss: 0.9883 - acc: 0.5150 - val_loss: 0.9929 - val_acc: 0.4989\n",
      "Epoch 52/300\n",
      "6550/6550 [==============================] - 90s 14ms/step - loss: 0.9870 - acc: 0.5185 - val_loss: 0.9920 - val_acc: 0.5021\n",
      "Epoch 53/300\n",
      "6550/6550 [==============================] - 92s 14ms/step - loss: 0.9854 - acc: 0.5171 - val_loss: 0.9918 - val_acc: 0.5004\n",
      "Epoch 54/300\n",
      "6550/6550 [==============================] - 101s 15ms/step - loss: 0.9842 - acc: 0.5173 - val_loss: 0.9902 - val_acc: 0.4954\n",
      "Epoch 55/300\n",
      "6550/6550 [==============================] - 102s 16ms/step - loss: 0.9820 - acc: 0.5250 - val_loss: 0.9890 - val_acc: 0.5004\n",
      "Epoch 56/300\n",
      "6550/6550 [==============================] - 100s 15ms/step - loss: 0.9805 - acc: 0.5261 - val_loss: 0.9877 - val_acc: 0.5036\n",
      "Epoch 57/300\n",
      "6550/6550 [==============================] - 96s 15ms/step - loss: 0.9784 - acc: 0.5296 - val_loss: 0.9869 - val_acc: 0.5071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/300\n",
      "6550/6550 [==============================] - 85s 13ms/step - loss: 0.9758 - acc: 0.5301 - val_loss: 0.9884 - val_acc: 0.5046\n",
      "Epoch 59/300\n",
      "6550/6550 [==============================] - 88s 13ms/step - loss: 0.9737 - acc: 0.5311 - val_loss: 0.9834 - val_acc: 0.5014\n",
      "Epoch 60/300\n",
      "6550/6550 [==============================] - 86s 13ms/step - loss: 0.9715 - acc: 0.5307 - val_loss: 0.9818 - val_acc: 0.5000\n",
      "Epoch 61/300\n",
      "6550/6550 [==============================] - 81s 12ms/step - loss: 0.9698 - acc: 0.5321 - val_loss: 0.9803 - val_acc: 0.5039\n",
      "Epoch 62/300\n",
      "6550/6550 [==============================] - 78s 12ms/step - loss: 0.9669 - acc: 0.5337 - val_loss: 0.9845 - val_acc: 0.5157\n",
      "Epoch 63/300\n",
      "6550/6550 [==============================] - 80s 12ms/step - loss: 0.9644 - acc: 0.5394 - val_loss: 0.9783 - val_acc: 0.5139\n",
      "Epoch 64/300\n",
      "6550/6550 [==============================] - 87s 13ms/step - loss: 0.9639 - acc: 0.5316 - val_loss: 0.9774 - val_acc: 0.5207\n",
      "Epoch 65/300\n",
      "6550/6550 [==============================] - 77s 12ms/step - loss: 0.9626 - acc: 0.5328 - val_loss: 0.9917 - val_acc: 0.5135\n",
      "Epoch 66/300\n",
      "6550/6550 [==============================] - 76s 12ms/step - loss: 0.9620 - acc: 0.5299 - val_loss: 0.9936 - val_acc: 0.4865\n",
      "Epoch 67/300\n",
      "6550/6550 [==============================] - 89s 14ms/step - loss: 0.9621 - acc: 0.5420 - val_loss: 0.9771 - val_acc: 0.5256\n",
      "Epoch 68/300\n",
      "6550/6550 [==============================] - 86s 13ms/step - loss: 0.9594 - acc: 0.5438 - val_loss: 0.9725 - val_acc: 0.5189\n",
      "Epoch 69/300\n",
      "6550/6550 [==============================] - 76s 12ms/step - loss: 0.9587 - acc: 0.5362 - val_loss: 0.9771 - val_acc: 0.5256\n",
      "Epoch 70/300\n",
      "6550/6550 [==============================] - 77s 12ms/step - loss: 0.9587 - acc: 0.5427 - val_loss: 1.0403 - val_acc: 0.4416\n",
      "Epoch 71/300\n",
      "6550/6550 [==============================] - 82s 13ms/step - loss: 0.9562 - acc: 0.5443 - val_loss: 0.9719 - val_acc: 0.5328\n",
      "Epoch 72/300\n",
      "6550/6550 [==============================] - 87s 13ms/step - loss: 0.9554 - acc: 0.5417 - val_loss: 0.9702 - val_acc: 0.5264\n",
      "Epoch 73/300\n",
      "6550/6550 [==============================] - 77s 12ms/step - loss: 0.9527 - acc: 0.5447 - val_loss: 0.9704 - val_acc: 0.5214\n",
      "Epoch 74/300\n",
      "6550/6550 [==============================] - 90s 14ms/step - loss: 0.9542 - acc: 0.5458 - val_loss: 0.9689 - val_acc: 0.5338\n",
      "Epoch 75/300\n",
      "6550/6550 [==============================] - 77s 12ms/step - loss: 0.9530 - acc: 0.5490 - val_loss: 1.0003 - val_acc: 0.5146\n",
      "Epoch 76/300\n",
      "6550/6550 [==============================] - 74s 11ms/step - loss: 0.9497 - acc: 0.5537 - val_loss: 0.9843 - val_acc: 0.5271\n",
      "Epoch 77/300\n",
      "6550/6550 [==============================] - 80s 12ms/step - loss: 0.9515 - acc: 0.5481 - val_loss: 0.9679 - val_acc: 0.5310\n",
      "Epoch 78/300\n",
      "6550/6550 [==============================] - 80s 12ms/step - loss: 0.9502 - acc: 0.5489 - val_loss: 0.9671 - val_acc: 0.5424\n",
      "Epoch 79/300\n",
      "6550/6550 [==============================] - 76s 12ms/step - loss: 0.9525 - acc: 0.5438 - val_loss: 0.9691 - val_acc: 0.5385\n",
      "Epoch 80/300\n",
      "6550/6550 [==============================] - 77s 12ms/step - loss: 0.9491 - acc: 0.5534 - val_loss: 0.9935 - val_acc: 0.5224\n",
      "Epoch 81/300\n",
      "6550/6550 [==============================] - 77s 12ms/step - loss: 0.9480 - acc: 0.5502 - val_loss: 0.9736 - val_acc: 0.5071\n",
      "Epoch 82/300\n",
      "6550/6550 [==============================] - 81s 12ms/step - loss: 0.9472 - acc: 0.5508 - val_loss: 0.9667 - val_acc: 0.5231\n",
      "Epoch 83/300\n",
      "6550/6550 [==============================] - 74s 11ms/step - loss: 0.9481 - acc: 0.5492 - val_loss: 1.0357 - val_acc: 0.4533\n",
      "Epoch 84/300\n",
      "6550/6550 [==============================] - 74s 11ms/step - loss: 0.9484 - acc: 0.5493 - val_loss: 1.0350 - val_acc: 0.4979\n",
      "Epoch 85/300\n",
      "6550/6550 [==============================] - 74s 11ms/step - loss: 0.9475 - acc: 0.5527 - val_loss: 0.9677 - val_acc: 0.5399\n",
      "Epoch 86/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.9472 - acc: 0.5557 - val_loss: 0.9714 - val_acc: 0.5075\n",
      "Epoch 87/300\n",
      "6550/6550 [==============================] - 77s 12ms/step - loss: 0.9465 - acc: 0.5540 - val_loss: 0.9631 - val_acc: 0.5459\n",
      "Epoch 88/300\n",
      "6550/6550 [==============================] - 74s 11ms/step - loss: 0.9460 - acc: 0.5518 - val_loss: 1.0018 - val_acc: 0.5096\n",
      "Epoch 89/300\n",
      "6550/6550 [==============================] - 74s 11ms/step - loss: 0.9447 - acc: 0.5501 - val_loss: 0.9764 - val_acc: 0.5004\n",
      "Epoch 90/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.9383 - acc: 0.5627 - val_loss: 0.9822 - val_acc: 0.5004\n",
      "Epoch 91/300\n",
      "6550/6550 [==============================] - 74s 11ms/step - loss: 0.9449 - acc: 0.5548 - val_loss: 0.9642 - val_acc: 0.5231\n",
      "Epoch 92/300\n",
      "6550/6550 [==============================] - 74s 11ms/step - loss: 0.9430 - acc: 0.5508 - val_loss: 0.9661 - val_acc: 0.5121\n",
      "Epoch 93/300\n",
      "6550/6550 [==============================] - 77s 12ms/step - loss: 0.9429 - acc: 0.5556 - val_loss: 0.9624 - val_acc: 0.5288\n",
      "Epoch 94/300\n",
      "6550/6550 [==============================] - 74s 11ms/step - loss: 0.9400 - acc: 0.5521 - val_loss: 0.9749 - val_acc: 0.5331\n",
      "Epoch 95/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.9369 - acc: 0.5565 - val_loss: 0.9643 - val_acc: 0.5167\n",
      "Epoch 96/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.9399 - acc: 0.5554 - val_loss: 0.9673 - val_acc: 0.5085\n",
      "Epoch 97/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.9373 - acc: 0.5647 - val_loss: 0.9889 - val_acc: 0.4858\n",
      "Epoch 98/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.9387 - acc: 0.5545 - val_loss: 0.9717 - val_acc: 0.5061\n",
      "Epoch 99/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.9382 - acc: 0.5573 - val_loss: 0.9738 - val_acc: 0.5068\n",
      "Epoch 100/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.9381 - acc: 0.5507 - val_loss: 1.0031 - val_acc: 0.4769\n",
      "Epoch 101/300\n",
      "6550/6550 [==============================] - 77s 12ms/step - loss: 0.9342 - acc: 0.5623 - val_loss: 0.9563 - val_acc: 0.5431\n",
      "Epoch 102/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.9347 - acc: 0.5606 - val_loss: 0.9851 - val_acc: 0.4925\n",
      "Epoch 103/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.9349 - acc: 0.5580 - val_loss: 0.9571 - val_acc: 0.5467\n",
      "Epoch 104/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.9344 - acc: 0.5617 - val_loss: 0.9581 - val_acc: 0.5285\n",
      "Epoch 105/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.9340 - acc: 0.5629 - val_loss: 0.9714 - val_acc: 0.5071\n",
      "Epoch 106/300\n",
      "6550/6550 [==============================] - 76s 12ms/step - loss: 0.9309 - acc: 0.5595 - val_loss: 0.9552 - val_acc: 0.5438\n",
      "Epoch 107/300\n",
      "6550/6550 [==============================] - 77s 12ms/step - loss: 0.9303 - acc: 0.5618 - val_loss: 0.9534 - val_acc: 0.5459\n",
      "Epoch 108/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.9278 - acc: 0.5666 - val_loss: 1.0164 - val_acc: 0.5071\n",
      "Epoch 109/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.9265 - acc: 0.5653 - val_loss: 0.9986 - val_acc: 0.5167\n",
      "Epoch 110/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.9278 - acc: 0.5627 - val_loss: 0.9613 - val_acc: 0.5139\n",
      "Epoch 111/300\n",
      "6550/6550 [==============================] - 74s 11ms/step - loss: 0.9278 - acc: 0.5635 - val_loss: 0.9975 - val_acc: 0.5160\n",
      "Epoch 112/300\n",
      "6550/6550 [==============================] - 77s 12ms/step - loss: 0.9274 - acc: 0.5690 - val_loss: 0.9530 - val_acc: 0.5321\n",
      "Epoch 113/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.9266 - acc: 0.5689 - val_loss: 0.9576 - val_acc: 0.5235\n",
      "Epoch 114/300\n",
      "6550/6550 [==============================] - 75s 11ms/step - loss: 0.9242 - acc: 0.5595 - val_loss: 0.9757 - val_acc: 0.5043\n",
      "Epoch 115/300\n",
      "6550/6550 [==============================] - 80s 12ms/step - loss: 0.9240 - acc: 0.5667 - val_loss: 0.9599 - val_acc: 0.5160\n",
      "Epoch 116/300\n",
      "6550/6550 [==============================] - 78s 12ms/step - loss: 0.9234 - acc: 0.5652 - val_loss: 0.9560 - val_acc: 0.5381\n",
      "Epoch 117/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6550/6550 [==============================] - 83s 13ms/step - loss: 0.9178 - acc: 0.5710 - val_loss: 0.9487 - val_acc: 0.5402\n",
      "Epoch 118/300\n",
      "6550/6550 [==============================] - 79s 12ms/step - loss: 0.9197 - acc: 0.5705 - val_loss: 0.9854 - val_acc: 0.4900\n",
      "Epoch 119/300\n",
      "6550/6550 [==============================] - 78s 12ms/step - loss: 0.9219 - acc: 0.5632 - val_loss: 0.9589 - val_acc: 0.5199\n",
      "Epoch 120/300\n",
      "6550/6550 [==============================] - 79s 12ms/step - loss: 0.9215 - acc: 0.5647 - val_loss: 0.9508 - val_acc: 0.5449\n",
      "Epoch 121/300\n",
      "6550/6550 [==============================] - 83s 13ms/step - loss: 0.9178 - acc: 0.5704 - val_loss: 0.9472 - val_acc: 0.5442\n",
      "Epoch 122/300\n",
      "6550/6550 [==============================] - 79s 12ms/step - loss: 0.9186 - acc: 0.5676 - val_loss: 0.9496 - val_acc: 0.5324\n",
      "Epoch 123/300\n",
      "6550/6550 [==============================] - 79s 12ms/step - loss: 0.9158 - acc: 0.5693 - val_loss: 0.9551 - val_acc: 0.5274\n",
      "Epoch 124/300\n",
      "6550/6550 [==============================] - 78s 12ms/step - loss: 0.9137 - acc: 0.5750 - val_loss: 0.9675 - val_acc: 0.5068\n",
      "Epoch 125/300\n",
      "6550/6550 [==============================] - 79s 12ms/step - loss: 0.9121 - acc: 0.5696 - val_loss: 0.9718 - val_acc: 0.5331\n",
      "Epoch 126/300\n",
      "6550/6550 [==============================] - 78s 12ms/step - loss: 0.9111 - acc: 0.5734 - val_loss: 0.9566 - val_acc: 0.5417\n",
      "Epoch 127/300\n",
      "6550/6550 [==============================] - 86s 13ms/step - loss: 0.9154 - acc: 0.5711 - val_loss: 0.9449 - val_acc: 0.5509\n",
      "Epoch 128/300\n",
      "6550/6550 [==============================] - 87s 13ms/step - loss: 0.9096 - acc: 0.5740 - val_loss: 0.9435 - val_acc: 0.5438\n",
      "Epoch 129/300\n",
      "6550/6550 [==============================] - 79s 12ms/step - loss: 0.9074 - acc: 0.5785 - val_loss: 1.0167 - val_acc: 0.4694\n",
      "Epoch 130/300\n",
      "6550/6550 [==============================] - 79s 12ms/step - loss: 0.9079 - acc: 0.5791 - val_loss: 0.9440 - val_acc: 0.5552\n",
      "Epoch 131/300\n",
      "6550/6550 [==============================] - 79s 12ms/step - loss: 0.9053 - acc: 0.5783 - val_loss: 0.9449 - val_acc: 0.5488\n",
      "Epoch 132/300\n",
      "6550/6550 [==============================] - 79s 12ms/step - loss: 0.9027 - acc: 0.5783 - val_loss: 0.9544 - val_acc: 0.5424\n",
      "Epoch 133/300\n",
      "6550/6550 [==============================] - 78s 12ms/step - loss: 0.9007 - acc: 0.5788 - val_loss: 0.9446 - val_acc: 0.5541\n",
      "Epoch 134/300\n",
      "6550/6550 [==============================] - 86s 13ms/step - loss: 0.9010 - acc: 0.5818 - val_loss: 0.9418 - val_acc: 0.5385\n",
      "Epoch 135/300\n",
      "6550/6550 [==============================] - 77s 12ms/step - loss: 0.8985 - acc: 0.5866 - val_loss: 0.9427 - val_acc: 0.5345\n",
      "Epoch 136/300\n",
      "6550/6550 [==============================] - 78s 12ms/step - loss: 0.9003 - acc: 0.5824 - val_loss: 0.9432 - val_acc: 0.5506\n",
      "Epoch 137/300\n",
      "6550/6550 [==============================] - 75s 11ms/step - loss: 0.8970 - acc: 0.5824 - val_loss: 1.0264 - val_acc: 0.4793\n",
      "Epoch 138/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.8928 - acc: 0.5849 - val_loss: 1.0221 - val_acc: 0.4744\n",
      "Epoch 139/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.8955 - acc: 0.5826 - val_loss: 0.9450 - val_acc: 0.5474\n",
      "Epoch 140/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.8935 - acc: 0.5901 - val_loss: 0.9447 - val_acc: 0.5299\n",
      "Epoch 141/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.8925 - acc: 0.5849 - val_loss: 1.0760 - val_acc: 0.4576\n",
      "Epoch 142/300\n",
      "6550/6550 [==============================] - 84s 13ms/step - loss: 0.8920 - acc: 0.5869 - val_loss: 0.9394 - val_acc: 0.5413\n",
      "Epoch 143/300\n",
      "6550/6550 [==============================] - 74s 11ms/step - loss: 0.8890 - acc: 0.5907 - val_loss: 0.9548 - val_acc: 0.5477\n",
      "Epoch 144/300\n",
      "6550/6550 [==============================] - 84s 13ms/step - loss: 0.8882 - acc: 0.5875 - val_loss: 0.9345 - val_acc: 0.5573\n",
      "Epoch 145/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.8868 - acc: 0.5885 - val_loss: 0.9405 - val_acc: 0.5367\n",
      "Epoch 146/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.8851 - acc: 0.5921 - val_loss: 0.9527 - val_acc: 0.5467\n",
      "Epoch 147/300\n",
      "6550/6550 [==============================] - 74s 11ms/step - loss: 0.8830 - acc: 0.5942 - val_loss: 0.9513 - val_acc: 0.5484\n",
      "Epoch 148/300\n",
      "6550/6550 [==============================] - 74s 11ms/step - loss: 0.8823 - acc: 0.5966 - val_loss: 0.9656 - val_acc: 0.5217\n",
      "Epoch 149/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.8804 - acc: 0.5950 - val_loss: 0.9351 - val_acc: 0.5538\n",
      "Epoch 150/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.8763 - acc: 0.5980 - val_loss: 0.9463 - val_acc: 0.5477\n",
      "Epoch 151/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.8771 - acc: 0.5985 - val_loss: 0.9350 - val_acc: 0.5623\n",
      "Epoch 152/300\n",
      "6550/6550 [==============================] - 74s 11ms/step - loss: 0.8769 - acc: 0.6024 - val_loss: 0.9579 - val_acc: 0.5406\n",
      "Epoch 153/300\n",
      "6550/6550 [==============================] - 74s 11ms/step - loss: 0.8760 - acc: 0.6009 - val_loss: 0.9370 - val_acc: 0.5620\n",
      "Epoch 154/300\n",
      "6550/6550 [==============================] - 73s 11ms/step - loss: 0.8754 - acc: 0.6024 - val_loss: 0.9423 - val_acc: 0.5545\n",
      "Model architecture saved to: /mnt/silo/users/moorea/tdparse/model zoo/LSTM Election Twitter architecture.yaml\n",
      "Model weights saved to /mnt/silo/users/moorea/tdparse/model zoo/LSTM Election Twitter weights.h5\n",
      "Save time 11.47\n",
      "saving raw data\n"
     ]
    }
   ],
   "source": [
    "# Model folder results\n",
    "lstm_folder = os.path.join(result_folder, 'lstm')\n",
    "os.makedirs(lstm_folder, exist_ok=True)\n",
    "\n",
    "# Result files\n",
    "word_vector_file = os.path.join(lstm_folder, 'word vector results.json')\n",
    "result_file = os.path.join(lstm_folder, 'results file.tsv')\n",
    "\n",
    "for dataset_name, train_test in dataset_train_test.items():\n",
    "    train, test = train_test\n",
    "    model_params = {'tokeniser' : ark_twokenize,\n",
    "                    'lower' : True, 'pad_size' : -1}\n",
    "    dataset_predictions(train, test, dataset_name, LSTM, \n",
    "                        word_vector_file, result_file, model_dir, model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass Evaluation of the TDLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mitchel {'tokeniser': <function ark_twokenize at 0x7f2c83bdd950>, 'lower': True, 'pad_size': -1}\n",
      "sswe\n",
      "glove 300d 42b common crawl\n",
      "{'tokeniser': <function ark_twokenize at 0x7f2c83bdd950>, 'lower': True, 'pad_size': -1, 'embeddings': glove 300d 42b common crawl} glove 300d 42b common crawl\n",
      "Train on 1610 samples, validate on 691 samples\n",
      "Epoch 1/5\n",
      "1610/1610 [==============================] - 46s 29ms/step - loss: 0.9869 - acc: 0.6957 - val_loss: 0.9009 - val_acc: 0.7019\n",
      "Epoch 2/5\n",
      "1610/1610 [==============================] - 48s 30ms/step - loss: 0.8628 - acc: 0.7012 - val_loss: 0.8373 - val_acc: 0.7019\n",
      "Epoch 3/5\n",
      "1610/1610 [==============================] - 48s 30ms/step - loss: 0.8251 - acc: 0.7012 - val_loss: 0.8170 - val_acc: 0.7019\n",
      "Epoch 4/5\n",
      "1610/1610 [==============================] - 49s 30ms/step - loss: 0.8125 - acc: 0.7012 - val_loss: 0.8097 - val_acc: 0.7019\n",
      "Epoch 5/5\n",
      "1610/1610 [==============================] - 47s 29ms/step - loss: 0.8064 - acc: 0.7012 - val_loss: 0.8056 - val_acc: 0.7019\n",
      "{'tokeniser': <function ark_twokenize at 0x7f2c83bdd950>, 'lower': True, 'pad_size': -1, 'embeddings': sswe} sswe\n",
      "Train on 1610 samples, validate on 691 samples\n",
      "Epoch 1/5\n",
      "1610/1610 [==============================] - 5s 3ms/step - loss: 0.8850 - acc: 0.6913 - val_loss: 0.8079 - val_acc: 0.7019\n",
      "Epoch 2/5\n",
      "1610/1610 [==============================] - 3s 2ms/step - loss: 0.7949 - acc: 0.7012 - val_loss: 0.7916 - val_acc: 0.7019\n",
      "Epoch 3/5\n",
      "1610/1610 [==============================] - 3s 2ms/step - loss: 0.7883 - acc: 0.7012 - val_loss: 0.7894 - val_acc: 0.7019\n",
      "Epoch 4/5\n",
      "1610/1610 [==============================] - 3s 2ms/step - loss: 0.7867 - acc: 0.7012 - val_loss: 0.7873 - val_acc: 0.7019\n",
      "Epoch 5/5\n",
      "1600/1610 [============================>.] - ETA: 0s - loss: 0.7864 - acc: 0.7006"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-abb461fd71dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                     'lower' : True, 'pad_size' : -1}\n\u001b[1;32m     13\u001b[0m     dataset_predictions(train, test, dataset_name, TDLSTM,\n\u001b[0;32m---> 14\u001b[0;31m                         word_vector_file, result_file, model_dir, model_params)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-5ef4ae043916>\u001b[0m in \u001b[0;36mdataset_predictions\u001b[0;34m(train, test, dataset_name, model_class, word_vector_file_path, result_file_path, model_folder_path, model_params)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_word_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         model.fit(data_train, y_train, validation_size=0.3, verbose=1,\n\u001b[0;32m---> 47\u001b[0;31m                   reproducible=True, patience=10, epochs=5, org_initialisers=True)\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Saves the model to the model zoo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mmodel_folder_join\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_folder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/silo/users/moorea/tdparse/tdparse/models/tdlstm.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, train_y, validation_size, verbose, reproducible, embedding_layer_trainable, lstm_dimension, optimiser, patience, batch_size, epochs, org_initialisers)\u001b[0m\n\u001b[1;32m    854\u001b[0m                                 \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                                 verbose=verbose, batch_size=batch_size)\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0;31m# Load the best model from the saved weight file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1218\u001b[0m                             val_outs = self._test_loop(val_f, val_ins,\n\u001b[1;32m   1219\u001b[0m                                                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m                                                        verbose=0)\n\u001b[0m\u001b[1;32m   1221\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m                                 \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1380\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model folder results\n",
    "tdlstm_folder = os.path.join(result_folder, 'tdlstm')\n",
    "os.makedirs(tdlstm_folder, exist_ok=True)\n",
    "\n",
    "# Result files\n",
    "word_vector_file = os.path.join(tdlstm_folder, 'word vector results.json')\n",
    "result_file = os.path.join(tdlstm_folder, 'results file.tsv')\n",
    "\n",
    "for dataset_name, train_test in dataset_train_test.items():\n",
    "    train, test = train_test\n",
    "    model_params = {'tokeniser' : ark_twokenize,\n",
    "                    'lower' : True, 'pad_size' : -1}\n",
    "    dataset_predictions(train, test, dataset_name, TDLSTM,\n",
    "                        word_vector_file, result_file, model_dir, model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass Evaluation of the TCLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mitchel {'tokeniser': <function ark_twokenize at 0x7fbd07a14950>, 'lower': True, 'pad_size': -1}\n",
      "sswe\n",
      "{'tokeniser': <function ark_twokenize at 0x7fbd07a14950>, 'lower': True, 'pad_size': -1, 'embeddings': sswe} sswe\n",
      "Train on 1610 samples, validate on 691 samples\n",
      "Epoch 1/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.8468 - acc: 0.7006 - val_loss: 0.7888 - val_acc: 0.7019\n",
      "Epoch 2/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7847 - acc: 0.7012 - val_loss: 0.7809 - val_acc: 0.7019\n",
      "Epoch 3/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7818 - acc: 0.7012 - val_loss: 0.7805 - val_acc: 0.7019\n",
      "Epoch 4/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7814 - acc: 0.7012 - val_loss: 0.7798 - val_acc: 0.7019\n",
      "Epoch 5/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7800 - acc: 0.7012 - val_loss: 0.7791 - val_acc: 0.7019\n",
      "Epoch 6/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7792 - acc: 0.7012 - val_loss: 0.7787 - val_acc: 0.7019\n",
      "Epoch 7/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7787 - acc: 0.7012 - val_loss: 0.7780 - val_acc: 0.7019\n",
      "Epoch 8/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7777 - acc: 0.7012 - val_loss: 0.7787 - val_acc: 0.7019\n",
      "Epoch 9/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7771 - acc: 0.7012 - val_loss: 0.7767 - val_acc: 0.7019\n",
      "Epoch 10/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7761 - acc: 0.7012 - val_loss: 0.7765 - val_acc: 0.7019\n",
      "Epoch 11/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7754 - acc: 0.7012 - val_loss: 0.7757 - val_acc: 0.7019\n",
      "Epoch 12/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7743 - acc: 0.7012 - val_loss: 0.7746 - val_acc: 0.7019\n",
      "Epoch 13/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7733 - acc: 0.7012 - val_loss: 0.7740 - val_acc: 0.7019\n",
      "Epoch 14/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7729 - acc: 0.7012 - val_loss: 0.7731 - val_acc: 0.7019\n",
      "Epoch 15/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7719 - acc: 0.7012 - val_loss: 0.7716 - val_acc: 0.7033\n",
      "Epoch 16/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7708 - acc: 0.7006 - val_loss: 0.7709 - val_acc: 0.7033\n",
      "Epoch 17/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7688 - acc: 0.7006 - val_loss: 0.7700 - val_acc: 0.7033\n",
      "Epoch 18/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7695 - acc: 0.7006 - val_loss: 0.7701 - val_acc: 0.7033\n",
      "Epoch 19/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7683 - acc: 0.7006 - val_loss: 0.7708 - val_acc: 0.7033\n",
      "Epoch 20/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7670 - acc: 0.7006 - val_loss: 0.7672 - val_acc: 0.7033\n",
      "Epoch 21/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7642 - acc: 0.7012 - val_loss: 0.7683 - val_acc: 0.7033\n",
      "Epoch 22/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7640 - acc: 0.7012 - val_loss: 0.7654 - val_acc: 0.7033\n",
      "Epoch 23/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7621 - acc: 0.7019 - val_loss: 0.7686 - val_acc: 0.7033\n",
      "Epoch 24/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7600 - acc: 0.7012 - val_loss: 0.7700 - val_acc: 0.7033\n",
      "Epoch 25/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7620 - acc: 0.7012 - val_loss: 0.7701 - val_acc: 0.7033\n",
      "Epoch 26/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7585 - acc: 0.7019 - val_loss: 0.7606 - val_acc: 0.7033\n",
      "Epoch 27/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7573 - acc: 0.7025 - val_loss: 0.7614 - val_acc: 0.7033\n",
      "Epoch 28/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7566 - acc: 0.7025 - val_loss: 0.7583 - val_acc: 0.7048\n",
      "Epoch 29/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7518 - acc: 0.7031 - val_loss: 0.7957 - val_acc: 0.7033\n",
      "Epoch 30/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7568 - acc: 0.7043 - val_loss: 0.7555 - val_acc: 0.7033\n",
      "Epoch 31/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7506 - acc: 0.7043 - val_loss: 0.7634 - val_acc: 0.7048\n",
      "Epoch 32/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7515 - acc: 0.7043 - val_loss: 0.7536 - val_acc: 0.7048\n",
      "Epoch 33/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7518 - acc: 0.7043 - val_loss: 0.7580 - val_acc: 0.7033\n",
      "Epoch 34/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7492 - acc: 0.7043 - val_loss: 0.7503 - val_acc: 0.7033\n",
      "Epoch 35/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7449 - acc: 0.7062 - val_loss: 0.7591 - val_acc: 0.7033\n",
      "Epoch 36/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7436 - acc: 0.7056 - val_loss: 0.7637 - val_acc: 0.7033\n",
      "Epoch 37/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7404 - acc: 0.7050 - val_loss: 0.7494 - val_acc: 0.7033\n",
      "Epoch 38/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7422 - acc: 0.7050 - val_loss: 0.7847 - val_acc: 0.7033\n",
      "Epoch 39/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7419 - acc: 0.7050 - val_loss: 0.7593 - val_acc: 0.7048\n",
      "Epoch 40/300\n",
      "1610/1610 [==============================] - 7s 4ms/step - loss: 0.7396 - acc: 0.7062 - val_loss: 0.7464 - val_acc: 0.7033\n",
      "Epoch 41/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7362 - acc: 0.7075 - val_loss: 0.7365 - val_acc: 0.7033\n",
      "Epoch 42/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7352 - acc: 0.7075 - val_loss: 0.7639 - val_acc: 0.7033\n",
      "Epoch 43/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7364 - acc: 0.7062 - val_loss: 0.7443 - val_acc: 0.7019\n",
      "Epoch 44/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7335 - acc: 0.7062 - val_loss: 0.7378 - val_acc: 0.7048\n",
      "Epoch 45/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7295 - acc: 0.7068 - val_loss: 0.7376 - val_acc: 0.7019\n",
      "Epoch 46/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7317 - acc: 0.7093 - val_loss: 0.7368 - val_acc: 0.7062\n",
      "Epoch 47/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7309 - acc: 0.7093 - val_loss: 0.7546 - val_acc: 0.7048\n",
      "Epoch 48/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7277 - acc: 0.7112 - val_loss: 0.7408 - val_acc: 0.7004\n",
      "Epoch 49/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7188 - acc: 0.7075 - val_loss: 0.9180 - val_acc: 0.4616\n",
      "Epoch 50/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7346 - acc: 0.7037 - val_loss: 0.7440 - val_acc: 0.6990\n",
      "Epoch 51/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7219 - acc: 0.7093 - val_loss: 0.7350 - val_acc: 0.7004\n",
      "Epoch 52/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7173 - acc: 0.7112 - val_loss: 0.7730 - val_acc: 0.6975\n",
      "Epoch 53/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7180 - acc: 0.7112 - val_loss: 0.7417 - val_acc: 0.7048\n",
      "Epoch 54/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7209 - acc: 0.7168 - val_loss: 0.7267 - val_acc: 0.7048\n",
      "Epoch 55/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7185 - acc: 0.7137 - val_loss: 0.8136 - val_acc: 0.7033\n",
      "Epoch 56/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7157 - acc: 0.7112 - val_loss: 0.7342 - val_acc: 0.7019\n",
      "Epoch 57/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7099 - acc: 0.7168 - val_loss: 0.7958 - val_acc: 0.6744\n",
      "Epoch 58/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7156 - acc: 0.7130 - val_loss: 0.7300 - val_acc: 0.7033\n",
      "Epoch 59/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7087 - acc: 0.7168 - val_loss: 0.8884 - val_acc: 0.5007\n",
      "Epoch 60/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7105 - acc: 0.7161 - val_loss: 0.7431 - val_acc: 0.6990\n",
      "Epoch 61/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7057 - acc: 0.7155 - val_loss: 0.7199 - val_acc: 0.7062\n",
      "Epoch 62/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7044 - acc: 0.7137 - val_loss: 0.7238 - val_acc: 0.7019\n",
      "Epoch 63/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7023 - acc: 0.7193 - val_loss: 0.7397 - val_acc: 0.7033\n",
      "Epoch 64/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7055 - acc: 0.7161 - val_loss: 0.7722 - val_acc: 0.6990\n",
      "Epoch 65/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7002 - acc: 0.7205 - val_loss: 0.8458 - val_acc: 0.6397\n",
      "Epoch 66/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7042 - acc: 0.7224 - val_loss: 0.7202 - val_acc: 0.7048\n",
      "Epoch 67/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6996 - acc: 0.7174 - val_loss: 0.7190 - val_acc: 0.7091\n",
      "Epoch 68/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6987 - acc: 0.7180 - val_loss: 0.7203 - val_acc: 0.7019\n",
      "Epoch 69/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7027 - acc: 0.7174 - val_loss: 0.9020 - val_acc: 0.5137\n",
      "Epoch 70/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6963 - acc: 0.7161 - val_loss: 0.7358 - val_acc: 0.7004\n",
      "Epoch 71/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6869 - acc: 0.7174 - val_loss: 1.0217 - val_acc: 0.3140\n",
      "Epoch 72/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6933 - acc: 0.7211 - val_loss: 0.7866 - val_acc: 0.6918\n",
      "Epoch 73/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6972 - acc: 0.7174 - val_loss: 0.7369 - val_acc: 0.7062\n",
      "Epoch 74/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6925 - acc: 0.7248 - val_loss: 0.7210 - val_acc: 0.7019\n",
      "Epoch 75/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6893 - acc: 0.7211 - val_loss: 0.7048 - val_acc: 0.7033\n",
      "Epoch 76/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6912 - acc: 0.7267 - val_loss: 1.2419 - val_acc: 0.2605\n",
      "Epoch 77/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7012 - acc: 0.7193 - val_loss: 0.7304 - val_acc: 0.7033\n",
      "Epoch 78/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6820 - acc: 0.7273 - val_loss: 0.7171 - val_acc: 0.6975\n",
      "Epoch 79/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6862 - acc: 0.7292 - val_loss: 0.7849 - val_acc: 0.6585\n",
      "Epoch 80/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6744 - acc: 0.7280 - val_loss: 0.7444 - val_acc: 0.7091\n",
      "Epoch 81/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6869 - acc: 0.7217 - val_loss: 0.7124 - val_acc: 0.7004\n",
      "Epoch 82/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6707 - acc: 0.7298 - val_loss: 0.7182 - val_acc: 0.7033\n",
      "Epoch 83/300\n",
      "1610/1610 [==============================] - 7s 4ms/step - loss: 0.6794 - acc: 0.7230 - val_loss: 0.6990 - val_acc: 0.7033\n",
      "Epoch 84/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6735 - acc: 0.7273 - val_loss: 0.7330 - val_acc: 0.7077\n",
      "Epoch 85/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6668 - acc: 0.7267 - val_loss: 1.0812 - val_acc: 0.3242\n",
      "Epoch 86/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6716 - acc: 0.7236 - val_loss: 0.8109 - val_acc: 0.6729\n",
      "Epoch 87/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6655 - acc: 0.7354 - val_loss: 0.7050 - val_acc: 0.7077\n",
      "Epoch 88/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6647 - acc: 0.7335 - val_loss: 0.7500 - val_acc: 0.6918\n",
      "Epoch 89/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6565 - acc: 0.7304 - val_loss: 0.8964 - val_acc: 0.5311\n",
      "Epoch 90/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6608 - acc: 0.7242 - val_loss: 0.7181 - val_acc: 0.7033\n",
      "Epoch 91/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6665 - acc: 0.7261 - val_loss: 0.8363 - val_acc: 0.5962\n",
      "Epoch 92/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6618 - acc: 0.7348 - val_loss: 0.7173 - val_acc: 0.7019\n",
      "Epoch 93/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6641 - acc: 0.7280 - val_loss: 0.7287 - val_acc: 0.7077\n",
      "glove 300d 42b common crawl\n",
      "{'tokeniser': <function ark_twokenize at 0x7fbd07a14950>, 'lower': True, 'pad_size': -1, 'embeddings': glove 300d 42b common crawl} glove 300d 42b common crawl\n",
      "Train on 1610 samples, validate on 691 samples\n",
      "Epoch 1/300\n",
      "1610/1610 [==============================] - 132s 82ms/step - loss: 0.9759 - acc: 0.6925 - val_loss: 0.8845 - val_acc: 0.7019\n",
      "Epoch 2/300\n",
      "1610/1610 [==============================] - 136s 85ms/step - loss: 0.8516 - acc: 0.7012 - val_loss: 0.8278 - val_acc: 0.7019\n",
      "Epoch 3/300\n",
      "1610/1610 [==============================] - 135s 84ms/step - loss: 0.8201 - acc: 0.7012 - val_loss: 0.8119 - val_acc: 0.7019\n",
      "Epoch 4/300\n",
      "1610/1610 [==============================] - 135s 84ms/step - loss: 0.8094 - acc: 0.7012 - val_loss: 0.8062 - val_acc: 0.7019\n",
      "Epoch 5/300\n",
      "1610/1610 [==============================] - 137s 85ms/step - loss: 0.8032 - acc: 0.7012 - val_loss: 0.8017 - val_acc: 0.7019\n",
      "Epoch 6/300\n",
      "1610/1610 [==============================] - 136s 84ms/step - loss: 0.7983 - acc: 0.7012 - val_loss: 0.7984 - val_acc: 0.7019\n",
      "Epoch 7/300\n",
      "1610/1610 [==============================] - 152s 95ms/step - loss: 0.7941 - acc: 0.7012 - val_loss: 0.7953 - val_acc: 0.7019\n",
      "Epoch 8/300\n",
      "1610/1610 [==============================] - 151s 94ms/step - loss: 0.7903 - acc: 0.7012 - val_loss: 0.7924 - val_acc: 0.7019\n",
      "Epoch 9/300\n",
      "1610/1610 [==============================] - 153s 95ms/step - loss: 0.7864 - acc: 0.7012 - val_loss: 0.7897 - val_acc: 0.7019\n",
      "Epoch 10/300\n",
      "1610/1610 [==============================] - 156s 97ms/step - loss: 0.7827 - acc: 0.7012 - val_loss: 0.7870 - val_acc: 0.7019\n",
      "Epoch 11/300\n",
      "1610/1610 [==============================] - 154s 96ms/step - loss: 0.7788 - acc: 0.7012 - val_loss: 0.7845 - val_acc: 0.7019\n",
      "Epoch 12/300\n",
      "1610/1610 [==============================] - 146s 90ms/step - loss: 0.7753 - acc: 0.7012 - val_loss: 0.7820 - val_acc: 0.7019\n",
      "Epoch 13/300\n",
      "1610/1610 [==============================] - 146s 91ms/step - loss: 0.7717 - acc: 0.7012 - val_loss: 0.7796 - val_acc: 0.7019\n",
      "Epoch 14/300\n",
      "1610/1610 [==============================] - 147s 91ms/step - loss: 0.7682 - acc: 0.7012 - val_loss: 0.7771 - val_acc: 0.7019\n",
      "Epoch 15/300\n",
      "1610/1610 [==============================] - 145s 90ms/step - loss: 0.7648 - acc: 0.7012 - val_loss: 0.7747 - val_acc: 0.7019\n",
      "Epoch 16/300\n",
      "1610/1610 [==============================] - 146s 91ms/step - loss: 0.7610 - acc: 0.7012 - val_loss: 0.7721 - val_acc: 0.7019\n",
      "Epoch 17/300\n",
      "1610/1610 [==============================] - 146s 91ms/step - loss: 0.7575 - acc: 0.7012 - val_loss: 0.7696 - val_acc: 0.7019\n",
      "Epoch 18/300\n",
      "1610/1610 [==============================] - 146s 91ms/step - loss: 0.7538 - acc: 0.7012 - val_loss: 0.7672 - val_acc: 0.7019\n",
      "Epoch 19/300\n",
      "1610/1610 [==============================] - 143s 89ms/step - loss: 0.7500 - acc: 0.7019 - val_loss: 0.7646 - val_acc: 0.7019\n",
      "Epoch 20/300\n",
      "1610/1610 [==============================] - 150s 93ms/step - loss: 0.7462 - acc: 0.7031 - val_loss: 0.7621 - val_acc: 0.7019\n",
      "Epoch 21/300\n",
      "1610/1610 [==============================] - 142s 88ms/step - loss: 0.7422 - acc: 0.7037 - val_loss: 0.7602 - val_acc: 0.7019\n",
      "Epoch 22/300\n",
      "1610/1610 [==============================] - 151s 94ms/step - loss: 0.7386 - acc: 0.7037 - val_loss: 0.7579 - val_acc: 0.7019\n",
      "Epoch 23/300\n",
      "1610/1610 [==============================] - 145s 90ms/step - loss: 0.7352 - acc: 0.7062 - val_loss: 0.7559 - val_acc: 0.7019\n",
      "Epoch 24/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1610/1610 [==============================] - 147s 91ms/step - loss: 0.7314 - acc: 0.7062 - val_loss: 0.7529 - val_acc: 0.7019\n",
      "Epoch 25/300\n",
      "1610/1610 [==============================] - 151s 94ms/step - loss: 0.7279 - acc: 0.7087 - val_loss: 0.7509 - val_acc: 0.7019\n",
      "Epoch 26/300\n",
      "1408/1610 [=========================>....] - ETA: 13s - loss: 0.7263 - acc: 0.7067"
     ]
    }
   ],
   "source": [
    "# Model folder results\n",
    "tclstm_folder = os.path.join(result_folder, 'tclstm')\n",
    "os.makedirs(tclstm_folder, exist_ok=True)\n",
    "\n",
    "# Result files\n",
    "word_vector_file = os.path.join(tclstm_folder, 'word vector results.json')\n",
    "result_file = os.path.join(tclstm_folder, 'results file.tsv')\n",
    "\n",
    "for dataset_name, train_test in dataset_train_test.items():\n",
    "    train, test = train_test\n",
    "    model_params = {'tokeniser' : ark_twokenize,\n",
    "                    'lower' : True, 'pad_size' : -1}\n",
    "    dataset_predictions(train, test, dataset_name, TCLSTM, \n",
    "                        word_vector_file, result_file, model_dir, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
