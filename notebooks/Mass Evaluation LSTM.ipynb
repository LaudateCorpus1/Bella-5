{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from multiprocessing.pool import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras import initializers, optimizers\n",
    "\n",
    "from bella.data_types import TargetCollection, Target\n",
    "# Models\n",
    "from bella.models.tdlstm import LSTM, TDLSTM, TCLSTM\n",
    "# Tokenisers\n",
    "from bella.tokenisers import ark_twokenize\n",
    "# Word Vectors\n",
    "from bella.word_vectors import SSWE, GloveCommonCrawl\n",
    "# Get the data\n",
    "from bella.parsers import semeval_14, dong, election\n",
    "from bella.helper import read_config\n",
    "from bella import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(train, split_size=0.2, seed=42):\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=split_size, random_state=seed)\n",
    "    data = np.asarray(train.data_dict())\n",
    "    sentiment = np.asarray(train.sentiment_data())\n",
    "    for train_indexs, test_indexs in splitter.split(data, sentiment):\n",
    "        train_data = data[train_indexs]\n",
    "        test_data = data[test_indexs]\n",
    "\n",
    "    convert_to_targets = lambda data: [Target(**target) for target in data]\n",
    "    train = TargetCollection(convert_to_targets(train_data))\n",
    "    val = TargetCollection(convert_to_targets(test_data))\n",
    "\n",
    "    X_train = np.array(train.data_dict())\n",
    "    y_train = np.array(train.sentiment_data())\n",
    "    X_val = np.array(val.data_dict())\n",
    "    y_val = np.array(val.sentiment_data())\n",
    "    return (X_train, y_train), (X_val, y_val)\n",
    "\n",
    "def class_mapper(y, inverse=False):\n",
    "    class_map = {-1: 0, 0: 1, 1: 2}\n",
    "    inv_class_map = {0: -1, 1: 0, 2: 1}\n",
    "    if inverse:\n",
    "        return np.array([inv_class_map[val] for val in y])\n",
    "    return np.array([class_map[val] for val in y])\n",
    "\n",
    "uniform_init = initializers.RandomUniform(minval=-0.003, maxval=0.003)\n",
    "lstm_layer_kwargs = {'kernel_initializer' : uniform_init,\n",
    "                     'recurrent_initializer' : uniform_init,\n",
    "                     'bias_initializer' : uniform_init}\n",
    "dense_layer_kwargs = {'kernel_initializer' : uniform_init,\n",
    "                      'bias_initializer' : uniform_init}\n",
    "embedding_layer_kwargs = {'embeddings_initializer' : uniform_init}\n",
    "model_kwargs = {'lstm_layer_kwargs': lstm_layer_kwargs,\n",
    "                'dense_layer_kwargs': dense_layer_kwargs,\n",
    "                'embedding_layer_kwargs': embedding_layer_kwargs,\n",
    "                'optimiser': optimizers.SGD,\n",
    "                'optimiser_params': {'lr': 0.01},\n",
    "                'reproducible': 42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_word_vector(word_vectors, dataset_train_test, model, model_result_folder):\n",
    "    dataset_name, train_data, _ = dataset_train_test\n",
    "    (X_train, y_train), (X_val, y_val) = train_val_split(train_data)\n",
    "    y_train, y_val = class_mapper(y_train), class_mapper(y_val)\n",
    "    word_vector_scores = {}\n",
    "    for word_vector in word_vectors:\n",
    "        model.embeddings = word_vector\n",
    "        model_history = model.fit(X_train, y_train, (X_val, y_val))\n",
    "        best_score = max(model_history.history['val_acc'])\n",
    "        word_vector_scores[word_vector] = best_score\n",
    "    \n",
    "    # Saving results\n",
    "    model_word_vector_fp = model_result_folder.joinpath('word vectors.json')\n",
    "    \n",
    "    dataset_word_vector_scores = {}\n",
    "    if model_word_vector_fp.is_file():\n",
    "        with model_word_vector_fp.open('r') as model_word_vector_file:\n",
    "            dataset_word_vector_scores = json.load(model_word_vector_file)\n",
    "    dataset_word_vector_scores[dataset_name] = {str(vector): score \n",
    "                                                for vector, score in word_vector_scores.items()}\n",
    "    with model_word_vector_fp.open('w') as model_word_vector_file:\n",
    "            json.dump(dataset_word_vector_scores, model_word_vector_file)\n",
    "        \n",
    "    return word_vector_scores\n",
    "\n",
    "def dataset_predictions(dataset_train_test, model, model_result_folder):\n",
    "    # Preparing the data\n",
    "    dataset_name, train_data, test_data = dataset_train_test\n",
    "    (X_train, y_train), (X_val, y_val) = train_val_split(train_data)\n",
    "    y_train, y_val = class_mapper(y_train), class_mapper(y_val)\n",
    "    X_test = test_data.data_dict()\n",
    "    \n",
    "    model_history = model.fit(X_train, y_train, (X_val, y_val))\n",
    "    predictions = model.predict(X_test)\n",
    "    # Change back to the original class labels from the mapping version\n",
    "    predictions = class_mapper(predictions, inverse=True)\n",
    "    predictions = predictions.tolist()\n",
    "    \n",
    "    # Saving results\n",
    "    model_dataset_fp = model_result_folder.joinpath('dataset predictions.json')\n",
    "    \n",
    "    dataset_predictions = {}\n",
    "    if model_dataset_fp.is_file():\n",
    "        with model_dataset_fp.open('r') as model_dataset_file:\n",
    "            dataset_predictions = json.load(model_dataset_file)\n",
    "    dataset_predictions[dataset_name] = predictions\n",
    "    with model_dataset_fp.open('w') as model_dataset_file:\n",
    "            json.dump(dataset_predictions, model_dataset_file)\n",
    "    return predictions\n",
    "\n",
    "def model_evaluation(model_class, word_vectors, dataset_train_test, \n",
    "                     result_folder, model_zoo_folder, model_kwargs=None):\n",
    "    if model_kwargs is None:\n",
    "        model_kwargs = {}\n",
    "    model = model_class(ark_twokenize, word_vectors[0], **model_kwargs)\n",
    "    model_result_folder = result_folder.joinpath(f'{str(model)}')\n",
    "    model_result_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    vector_scores = best_word_vector(word_vectors, dataset_train_test, \n",
    "                                     model, model_result_folder)\n",
    "    best_vector = max(vector_scores.items(), key=lambda x: x[1])[0]\n",
    "    model.embeddings = best_vector\n",
    "    best_predictions = dataset_predictions(dataset_train_test, model, model_result_folder)\n",
    "    dataset_name = dataset_train_test[0]\n",
    "    # Save model\n",
    "    model_zoo_file = model_zoo_folder.joinpath(f'{str(model)} {dataset_name}')\n",
    "    model.save(model, model_zoo_file)\n",
    "    return dataset_name, best_predictions\n",
    "\n",
    "def load_and_run(model_class, dataset_train_test, \n",
    "                 results_folder, model_zoo_folder, model_kwargs=None):\n",
    "    model_dataset_fp = results_folder.joinpath(f'{model_class.name()}')\\\n",
    "                                     .joinpath('dataset predictions.json')\n",
    "    if model_dataset_fp.is_file():\n",
    "        with model_dataset_fp.open('r') as dataset_preds:\n",
    "            dataset_preds = json.load(dataset_preds)\n",
    "            dataset_name = dataset_train_test[0]\n",
    "            if dataset_name in dataset_preds:\n",
    "                return dataset_name, dataset_preds[dataset_name]\n",
    "    if model_kwargs is None:\n",
    "        model_kwargs = {}\n",
    "    # Setting the word vectors up for each dataset\n",
    "    train_list = dataset_train_test[1].word_list(ark_twokenize)\n",
    "    test_list = dataset_train_test[2].word_list(ark_twokenize)\n",
    "    all_words = list(set(train_list + test_list))\n",
    "    sswe = SSWE(filter_words=all_words)\n",
    "    glove_300 = GloveCommonCrawl(version=42, filter_words=all_words)\n",
    "    word_vectors = [sswe, glove_300]\n",
    "    \n",
    "    return model_evaluation(model_class, word_vectors, dataset_train_test, \n",
    "                            results_folder, model_zoo_folder, model_kwargs)\n",
    "\n",
    "def model_evaluation_args(model_classes, dataset_train_test, \n",
    "                          result_folder, model_zoo_folder, model_kwargs=None):\n",
    "    for model_class in model_classes:\n",
    "        for dataset_name_train_test in dataset_train_test:\n",
    "            yield (model_class, dataset_name_train_test, \n",
    "                   results_folder, model_zoo_folder, model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#  ADD YOUR CONFIG FILE PATH HERE \n",
    "##\n",
    "CONFIG_FP = Path('..', 'config.yaml')\n",
    "\n",
    "# Load all of the datasets\n",
    "youtubean_train = semeval_14(read_config('youtubean_train', CONFIG_FP))\n",
    "youtubean_test = semeval_14(read_config('youtubean_test', CONFIG_FP))\n",
    "semeval_14_rest_train = semeval_14(read_config('semeval_2014_rest_train', CONFIG_FP))\n",
    "semeval_14_lap_train = semeval_14(read_config('semeval_2014_lap_train', CONFIG_FP))\n",
    "semeval_14_rest_test = semeval_14(read_config('semeval_2014_rest_test', CONFIG_FP))\n",
    "semeval_14_lap_test = semeval_14(read_config('semeval_2014_lap_test', CONFIG_FP))\n",
    "dong_train = dong(read_config('dong_twit_train_data', CONFIG_FP))\n",
    "dong_test = dong(read_config('dong_twit_test_data', CONFIG_FP))\n",
    "election_train, election_test = election(read_config('election_folder_dir', CONFIG_FP))\n",
    "mitchel_train = semeval_14(read_config('mitchel_train', CONFIG_FP))\n",
    "mitchel_test = semeval_14(read_config('mitchel_test', CONFIG_FP))\n",
    "\n",
    "\n",
    "dataset_train_test = [('SemEval 14 Laptop', semeval_14_lap_train, semeval_14_lap_test),\n",
    "                      ('SemEval 14 Restaurant', semeval_14_rest_train, semeval_14_rest_test),\n",
    "                      ('Dong Twitter', dong_train, dong_test),\n",
    "                      ('Election Twitter', election_train, election_test),\n",
    "                      ('YouTuBean', youtubean_train, youtubean_test),\n",
    "                      ('Mitchel', mitchel_train, mitchel_test)]\n",
    "\n",
    "\n",
    "results_folder = Path(read_config('results_folder', CONFIG_FP))\n",
    "results_folder = results_folder.joinpath('TDLstm')\n",
    "model_zoo_folder = Path(read_config('model_zoo_folder', CONFIG_FP))\n",
    "model_zoo_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass evaluation of the LSTM, TDLSTM, and TCLSTM models\n",
    "\n",
    "This code will run the methods across all datasets and save the results as well as the methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Number of cpus to use\n",
    "#\n",
    "n_cpus = 15\n",
    "\n",
    "model_eval_args = model_evaluation_args([LSTM, TDLSTM, TCLSTM], dataset_train_test, results_folder,\n",
    "                                        model_zoo_folder, model_kwargs)\n",
    "dataset_name_predictions = []\n",
    "with Pool(n_cpus) as pool:\n",
    "    dataset_name_predictions = pool.starmap(load_and_run, model_eval_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mass Evaluation Results\n",
    "\n",
    "Using the results that have been saved we can evaluate the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval_args = model_evaluation_args([LSTM, TDLSTM, TCLSTM], dataset_train_test, results_folder,\n",
    "                                        model_zoo_folder, model_kwargs)\n",
    "model_names = []\n",
    "for model_args in model_eval_args:\n",
    "    model_names.append(model_args[0].name())\n",
    "\n",
    "\n",
    "model_dataset_predictions = defaultdict(lambda: dict())\n",
    "for i, name_preds in enumerate(dataset_name_predictions):\n",
    "    dataset_name, predictions = name_preds\n",
    "    model_name = model_names[i]\n",
    "    model_dataset_predictions[model_name][dataset_name] = predictions\n",
    "    \n",
    "dataset_test = {name: test for name, train, test in dataset_train_test}\n",
    "f1_results = evaluation.evaluate_models(f1_score, dataset_test, \n",
    "                                        model_dataset_predictions, \n",
    "                                        dataframe=True, average='macro')\n",
    "acc_results = evaluation.evaluate_models(accuracy_score, dataset_test, \n",
    "                                         model_dataset_predictions, \n",
    "                                         dataframe=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSTM</th>\n",
       "      <th>TCLSTM</th>\n",
       "      <th>TDLSTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dong Twitter</th>\n",
       "      <td>66.47</td>\n",
       "      <td>69.51</td>\n",
       "      <td>68.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Election Twitter</th>\n",
       "      <td>53.80</td>\n",
       "      <td>57.22</td>\n",
       "      <td>57.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mitchel</th>\n",
       "      <td>72.14</td>\n",
       "      <td>69.81</td>\n",
       "      <td>73.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SemEval 14 Laptop</th>\n",
       "      <td>58.93</td>\n",
       "      <td>57.05</td>\n",
       "      <td>65.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SemEval 14 Restaurant</th>\n",
       "      <td>71.07</td>\n",
       "      <td>72.41</td>\n",
       "      <td>74.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YouTuBean</th>\n",
       "      <td>72.50</td>\n",
       "      <td>66.67</td>\n",
       "      <td>72.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>65.82</td>\n",
       "      <td>65.44</td>\n",
       "      <td>68.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        LSTM  TCLSTM  TDLSTM\n",
       "Dong Twitter           66.47   69.51   68.79\n",
       "Election Twitter       53.80   57.22   57.30\n",
       "Mitchel                72.14   69.81   73.45\n",
       "SemEval 14 Laptop      58.93   57.05   65.52\n",
       "SemEval 14 Restaurant  71.07   72.41   74.38\n",
       "YouTuBean              72.50   66.67   72.50\n",
       "Mean                   65.82   65.44   68.66"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(acc_results * 100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSTM</th>\n",
       "      <th>TCLSTM</th>\n",
       "      <th>TDLSTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dong Twitter</th>\n",
       "      <td>63.60</td>\n",
       "      <td>67.14</td>\n",
       "      <td>66.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Election Twitter</th>\n",
       "      <td>38.70</td>\n",
       "      <td>42.08</td>\n",
       "      <td>43.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mitchel</th>\n",
       "      <td>47.17</td>\n",
       "      <td>41.03</td>\n",
       "      <td>51.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SemEval 14 Laptop</th>\n",
       "      <td>47.84</td>\n",
       "      <td>46.80</td>\n",
       "      <td>57.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SemEval 14 Restaurant</th>\n",
       "      <td>46.36</td>\n",
       "      <td>55.38</td>\n",
       "      <td>57.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YouTuBean</th>\n",
       "      <td>45.93</td>\n",
       "      <td>38.07</td>\n",
       "      <td>45.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>48.27</td>\n",
       "      <td>48.42</td>\n",
       "      <td>53.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        LSTM  TCLSTM  TDLSTM\n",
       "Dong Twitter           63.60   67.14   66.09\n",
       "Election Twitter       38.70   42.08   43.60\n",
       "Mitchel                47.17   41.03   51.16\n",
       "SemEval 14 Laptop      47.84   46.80   57.91\n",
       "SemEval 14 Restaurant  46.36   55.38   57.68\n",
       "YouTuBean              45.93   38.07   45.47\n",
       "Mean                   48.27   48.42   53.65"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f1_results * 100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
