{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import time\n",
    "import tempfile\n",
    "import pickle\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Notebook helper methods\n",
    "from tdparse import notebook_helper\n",
    "# Models\n",
    "from tdparse.models.tdlstm import LSTM, TDLSTM, TCLSTM\n",
    "# Tokenisers\n",
    "from tdparse.tokenisers import ark_twokenize\n",
    "# Word Vectors\n",
    "from tdparse.word_vectors import PreTrained, GloveCommonCrawl\n",
    "# Get the data\n",
    "from tdparse.parsers import semeval_14, dong, election\n",
    "from tdparse.data_types import TargetCollection\n",
    "from tdparse.helper import read_config, full_path\n",
    "from tdparse.evaluation import evaluation_results\n",
    "from tdparse.notebook_helper import get_json_data, write_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of the datasets\n",
    "youtubean_train = semeval_14(full_path(read_config('youtubean_train')))\n",
    "youtubean_test = semeval_14(full_path(read_config('youtubean_test')))\n",
    "semeval_14_rest_train = semeval_14(full_path(read_config('semeval_2014_rest_train')))\n",
    "semeval_14_lap_train = semeval_14(full_path(read_config('semeval_2014_lap_train')))\n",
    "semeval_14_rest_test = semeval_14(full_path(read_config('semeval_2014_rest_test')))\n",
    "semeval_14_lap_test = semeval_14(full_path(read_config('semeval_2014_lap_test')))\n",
    "dong_train = dong(full_path(read_config('dong_twit_train_data')))\n",
    "dong_test = dong(full_path(read_config('dong_twit_test_data')))\n",
    "election_train, election_test = election(full_path(read_config('election_folder_dir')))\n",
    "mitchel_train = semeval_14(full_path(read_config('mitchel_train')))\n",
    "mitchel_test = semeval_14(full_path(read_config('mitchel_test')))\n",
    "\n",
    "\n",
    "dataset_train_test = {#'SemEval 14 Laptop' : (semeval_14_lap_train, semeval_14_lap_test),\n",
    "                      'SemEval 14 Restaurant' : (semeval_14_rest_train, semeval_14_rest_test),\n",
    "                      #'Dong Twitter' : (dong_train, dong_test),\n",
    "                      #'Election Twitter' : (election_train, election_test),\n",
    "                      #'YouTuBean' : (youtubean_train, youtubean_test),\n",
    "                      #'Mitchel' : (mitchel_train, mitchel_test)\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the word vectors\n",
    "sswe_path = full_path(read_config('sswe_files')['vo_zhang'])\n",
    "sswe = PreTrained(sswe_path, name='sswe')\n",
    "glove_300 = GloveCommonCrawl(version=42)\n",
    "# Word vectors that we are searching over\n",
    "word_vectors = [sswe, glove_300]\n",
    "\n",
    "\n",
    "# This is required as we have 3 classes and one of them is -1 and when one hot encoded\n",
    "# the index of -1 is 2 and that is what it thinks the label is when it should be \n",
    "# -1 hence the sentiment mapper\n",
    "sentiment_mapper = {0 : 0, 1 : 1, 2 : -1}\n",
    "\n",
    "# Folder to store all the sub folder for each model\n",
    "result_folder = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results', 'TDLstm'))\n",
    "# Folder to store all of the saved models (model zoo folder)\n",
    "model_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'model zoo'))\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_predictions(train, test, dataset_name, model_class, \n",
    "                        word_vector_file_path, result_file_path,\n",
    "                        model_folder_path, model_params):\n",
    "    \n",
    "    print('{} {}'.format(dataset_name, model_params))\n",
    "\n",
    "    data_train = train.data_dict()\n",
    "    y_train = train.sentiment_data()\n",
    "    data_test = test.data_dict()\n",
    "    y_test = test.sentiment_data()\n",
    "    \n",
    "    # Fits the model\n",
    "    word_vector_data = get_json_data(word_vector_file_path, dataset_name)\n",
    "    best_score = 0\n",
    "    best_word_vector = None\n",
    "    best_model = None\n",
    "    for word_vector in word_vectors:\n",
    "        print(word_vector)\n",
    "        word_vector_name = '{}'.format(word_vector)\n",
    "        if word_vector_name in word_vector_data:\n",
    "            word_vec_val_score = word_vector_data[word_vector_name]\n",
    "            if word_vec_val_score > best_score:\n",
    "                best_score = word_vec_val_score\n",
    "                best_word_vector = word_vector\n",
    "                best_model = None\n",
    "            continue\n",
    "        model_params['embeddings'] = word_vector\n",
    "        model = model_class(**model_params)\n",
    "        print('{} {}'.format(model_params, word_vector))\n",
    "        history = model.fit(data_train, y_train, validation_size=0.3, verbose=1,\n",
    "                            reproducible=True, patience=10, epochs=300, org_initialisers=True)\n",
    "        word_vec_val_score = max(history.history['val_acc'])\n",
    "        word_vector_data[word_vector_name] = word_vec_val_score\n",
    "        if word_vec_val_score > best_score:\n",
    "                best_score = word_vec_val_score\n",
    "                best_word_vector = word_vector\n",
    "                best_model = model\n",
    "                \n",
    "        # Save word vector validation score result\n",
    "        write_json_data(word_vector_file_path, dataset_name, word_vector_data)\n",
    "    if best_word_vector is None:\n",
    "        raise ValueError('best word vector should not be None')\n",
    "    if best_model is None:\n",
    "        model_params['embeddings'] = best_word_vector\n",
    "        model = model_class(**model_params)\n",
    "        print('{} {}'.format(model_params, best_word_vector))\n",
    "        model.fit(data_train, y_train, validation_size=0.3, verbose=1,\n",
    "                  reproducible=True, patience=10, epochs=300, org_initialisers=True)\n",
    "    # Saves the model to the model zoo\n",
    "    model_folder_join = lambda file_name: os.path.join(model_folder_path, file_name)\n",
    "    model_arch_fp = model_folder_join('{} {} architecture'.format(model, dataset_name))\n",
    "    model_weights_fp = model_folder_join('{} {} weights'.format(model, dataset_name))\n",
    "    model.save_model(model_arch_fp, model_weights_fp, verbose=1)\n",
    "    \n",
    "    # Predicts on the test data\n",
    "    predicted_values = model.predict(data_test)\n",
    "    # Convert prediction from one hot encoded to category value e.g. -1, 0, 1\n",
    "    predicted_values_cats =  model.prediction_to_cats(y_test, predicted_values, \n",
    "                                                      mapper=sentiment_mapper)\n",
    "    # Evaluates the predictions and save the results\n",
    "    return evaluation_results(predicted_values_cats, test, dataset_name, \n",
    "                              file_name=result_file_path, \n",
    "                              save_raw_data=True, re_write=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass Evaluation of the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SemEval 14 Restaurant {'tokeniser': <function ark_twokenize at 0x7fce47a0b0d0>, 'lower': True, 'pad_size': -1}\n",
      "sswe\n",
      "glove 300d 42b common crawl\n",
      "{'tokeniser': <function ark_twokenize at 0x7fce47a0b0d0>, 'lower': True, 'pad_size': -1, 'embeddings': sswe} sswe\n",
      "Train on 2521 samples, validate on 1081 samples\n",
      "Epoch 1/300\n",
      "2521/2521 [==============================] - 16s 6ms/step - loss: 1.0627 - acc: 0.5970 - val_loss: 1.0316 - val_acc: 0.6004\n",
      "Epoch 2/300\n",
      "2521/2521 [==============================] - 9s 4ms/step - loss: 1.0108 - acc: 0.6010 - val_loss: 0.9929 - val_acc: 0.6004\n",
      "Epoch 3/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.9802 - acc: 0.6010 - val_loss: 0.9695 - val_acc: 0.6004\n",
      "Epoch 4/300\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: 0.9619 - acc: 0.6010 - val_loss: 0.9558 - val_acc: 0.6004\n",
      "Epoch 5/300\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: 0.9516 - acc: 0.6010 - val_loss: 0.9486 - val_acc: 0.6004\n",
      "Epoch 6/300\n",
      "2521/2521 [==============================] - 11s 5ms/step - loss: 0.9467 - acc: 0.6010 - val_loss: 0.9457 - val_acc: 0.6004\n",
      "Epoch 7/300\n",
      "2521/2521 [==============================] - 12s 5ms/step - loss: 0.9446 - acc: 0.6010 - val_loss: 0.9441 - val_acc: 0.6004\n",
      "Epoch 8/300\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: 0.9434 - acc: 0.6010 - val_loss: 0.9431 - val_acc: 0.6004\n",
      "Epoch 9/300\n",
      "2521/2521 [==============================] - 7s 3ms/step - loss: 0.9424 - acc: 0.6010 - val_loss: 0.9422 - val_acc: 0.6004\n",
      "Epoch 10/300\n",
      "2521/2521 [==============================] - 10s 4ms/step - loss: 0.9415 - acc: 0.6010 - val_loss: 0.9411 - val_acc: 0.6004\n",
      "Epoch 11/300\n",
      "2521/2521 [==============================] - 7s 3ms/step - loss: 0.9404 - acc: 0.6010 - val_loss: 0.9398 - val_acc: 0.6004\n",
      "Epoch 12/300\n",
      "2521/2521 [==============================] - 7s 3ms/step - loss: 0.9391 - acc: 0.6010 - val_loss: 0.9383 - val_acc: 0.6004\n",
      "Epoch 13/300\n",
      "2521/2521 [==============================] - 7s 3ms/step - loss: 0.9374 - acc: 0.6010 - val_loss: 0.9364 - val_acc: 0.6004\n",
      "Epoch 14/300\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: 0.9356 - acc: 0.6010 - val_loss: 0.9342 - val_acc: 0.6004\n",
      "Epoch 15/300\n",
      "2521/2521 [==============================] - 10s 4ms/step - loss: 0.9330 - acc: 0.6010 - val_loss: 0.9314 - val_acc: 0.6004\n",
      "Epoch 16/300\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: 0.9300 - acc: 0.6010 - val_loss: 0.9280 - val_acc: 0.6004\n",
      "Epoch 17/300\n",
      "2521/2521 [==============================] - 10s 4ms/step - loss: 0.9262 - acc: 0.6010 - val_loss: 0.9238 - val_acc: 0.6004\n",
      "Epoch 18/300\n",
      "2521/2521 [==============================] - 6s 3ms/step - loss: 0.9216 - acc: 0.6010 - val_loss: 0.9187 - val_acc: 0.6004\n",
      "Epoch 19/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.9159 - acc: 0.6010 - val_loss: 0.9125 - val_acc: 0.6004\n",
      "Epoch 20/300\n",
      "2521/2521 [==============================] - 10s 4ms/step - loss: 0.9089 - acc: 0.6010 - val_loss: 0.9048 - val_acc: 0.6004\n",
      "Epoch 21/300\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: 0.9002 - acc: 0.6013 - val_loss: 0.8952 - val_acc: 0.6031\n",
      "Epoch 22/300\n",
      "2521/2521 [==============================] - 7s 3ms/step - loss: 0.8891 - acc: 0.6033 - val_loss: 0.8832 - val_acc: 0.6124\n",
      "Epoch 23/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.8751 - acc: 0.6101 - val_loss: 0.8688 - val_acc: 0.6216\n",
      "Epoch 24/300\n",
      "2521/2521 [==============================] - 9s 4ms/step - loss: 0.8571 - acc: 0.6271 - val_loss: 0.8527 - val_acc: 0.6226\n",
      "Epoch 25/300\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: 0.8380 - acc: 0.6355 - val_loss: 0.8333 - val_acc: 0.6216\n",
      "Epoch 26/300\n",
      "2521/2521 [==============================] - 9s 4ms/step - loss: 0.8242 - acc: 0.6414 - val_loss: 0.8280 - val_acc: 0.6420\n",
      "Epoch 27/300\n",
      "2521/2521 [==============================] - 10s 4ms/step - loss: 0.8186 - acc: 0.6430 - val_loss: 0.8521 - val_acc: 0.6309\n",
      "Epoch 28/300\n",
      "2521/2521 [==============================] - 9s 3ms/step - loss: 0.8143 - acc: 0.6446 - val_loss: 0.8170 - val_acc: 0.6411\n",
      "Epoch 29/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.8158 - acc: 0.6430 - val_loss: 0.8206 - val_acc: 0.6420\n",
      "Epoch 30/300\n",
      "2521/2521 [==============================] - 10s 4ms/step - loss: 0.8095 - acc: 0.6466 - val_loss: 0.8094 - val_acc: 0.6448\n",
      "Epoch 31/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.8085 - acc: 0.6462 - val_loss: 0.8303 - val_acc: 0.6364\n",
      "Epoch 32/300\n",
      "2521/2521 [==============================] - 7s 3ms/step - loss: 0.8052 - acc: 0.6478 - val_loss: 0.8155 - val_acc: 0.6438\n",
      "Epoch 33/300\n",
      "2521/2521 [==============================] - 9s 4ms/step - loss: 0.8037 - acc: 0.6529 - val_loss: 0.8029 - val_acc: 0.6540\n",
      "Epoch 34/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.8026 - acc: 0.6497 - val_loss: 0.8091 - val_acc: 0.6438\n",
      "Epoch 35/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7958 - acc: 0.6521 - val_loss: 0.7963 - val_acc: 0.6531\n",
      "Epoch 36/300\n",
      "2521/2521 [==============================] - 7s 3ms/step - loss: 0.7972 - acc: 0.6545 - val_loss: 0.7924 - val_acc: 0.6559\n",
      "Epoch 37/300\n",
      "2521/2521 [==============================] - 7s 3ms/step - loss: 0.8007 - acc: 0.6486 - val_loss: 0.7932 - val_acc: 0.6559\n",
      "Epoch 38/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7913 - acc: 0.6569 - val_loss: 0.7884 - val_acc: 0.6549\n",
      "Epoch 39/300\n",
      "2521/2521 [==============================] - 7s 3ms/step - loss: 0.7892 - acc: 0.6577 - val_loss: 0.7859 - val_acc: 0.6605\n",
      "Epoch 40/300\n",
      "2521/2521 [==============================] - 14s 5ms/step - loss: 0.7860 - acc: 0.6533 - val_loss: 0.7906 - val_acc: 0.6503\n",
      "Epoch 41/300\n",
      "2521/2521 [==============================] - 7s 3ms/step - loss: 0.7871 - acc: 0.6557 - val_loss: 0.7906 - val_acc: 0.6586\n",
      "Epoch 42/300\n",
      "2521/2521 [==============================] - 11s 4ms/step - loss: 0.7829 - acc: 0.6565 - val_loss: 0.7837 - val_acc: 0.6614\n",
      "Epoch 43/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7847 - acc: 0.6589 - val_loss: 0.7963 - val_acc: 0.6540\n",
      "Epoch 44/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7748 - acc: 0.6660 - val_loss: 0.7900 - val_acc: 0.6475\n",
      "Epoch 45/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7741 - acc: 0.6581 - val_loss: 0.7768 - val_acc: 0.6735\n",
      "Epoch 46/300\n",
      "2521/2521 [==============================] - 7s 3ms/step - loss: 0.7760 - acc: 0.6632 - val_loss: 0.7795 - val_acc: 0.6549\n",
      "Epoch 47/300\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: 0.7751 - acc: 0.6644 - val_loss: 0.7771 - val_acc: 0.6623\n",
      "Epoch 48/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7695 - acc: 0.6652 - val_loss: 0.8446 - val_acc: 0.6457\n",
      "Epoch 49/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7732 - acc: 0.6640 - val_loss: 0.7731 - val_acc: 0.6707\n",
      "Epoch 50/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7759 - acc: 0.6628 - val_loss: 0.7915 - val_acc: 0.6466\n",
      "Epoch 51/300\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: 0.7700 - acc: 0.6612 - val_loss: 0.7845 - val_acc: 0.6559\n",
      "Epoch 52/300\n",
      "2521/2521 [==============================] - 7s 3ms/step - loss: 0.7684 - acc: 0.6668 - val_loss: 0.7707 - val_acc: 0.6633\n",
      "Epoch 53/300\n",
      "2521/2521 [==============================] - 7s 3ms/step - loss: 0.7690 - acc: 0.6708 - val_loss: 0.7799 - val_acc: 0.6568\n",
      "Epoch 54/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7678 - acc: 0.6605 - val_loss: 0.7850 - val_acc: 0.6605\n",
      "Epoch 55/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7680 - acc: 0.6676 - val_loss: 0.7686 - val_acc: 0.6679\n",
      "Epoch 56/300\n",
      "2521/2521 [==============================] - 9s 3ms/step - loss: 0.7690 - acc: 0.6664 - val_loss: 0.7812 - val_acc: 0.6559\n",
      "Epoch 57/300\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: 0.7629 - acc: 0.6676 - val_loss: 0.7659 - val_acc: 0.6688\n",
      "Epoch 58/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7647 - acc: 0.6684 - val_loss: 0.7849 - val_acc: 0.6698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7708 - acc: 0.6644 - val_loss: 0.7719 - val_acc: 0.6679\n",
      "Epoch 60/300\n",
      "2521/2521 [==============================] - 9s 3ms/step - loss: 0.7621 - acc: 0.6700 - val_loss: 0.7800 - val_acc: 0.6605\n",
      "Epoch 61/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7610 - acc: 0.6735 - val_loss: 0.7629 - val_acc: 0.6707\n",
      "Epoch 62/300\n",
      "2521/2521 [==============================] - 11s 5ms/step - loss: 0.7561 - acc: 0.6735 - val_loss: 0.7612 - val_acc: 0.6725\n",
      "Epoch 63/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7557 - acc: 0.6747 - val_loss: 0.7626 - val_acc: 0.6735\n",
      "Epoch 64/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7525 - acc: 0.6799 - val_loss: 0.7787 - val_acc: 0.6586\n",
      "Epoch 65/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7514 - acc: 0.6763 - val_loss: 0.7700 - val_acc: 0.6642\n",
      "Epoch 66/300\n",
      "2521/2521 [==============================] - 9s 4ms/step - loss: 0.7541 - acc: 0.6771 - val_loss: 0.8095 - val_acc: 0.6531\n",
      "Epoch 67/300\n",
      "2521/2521 [==============================] - 12s 5ms/step - loss: 0.7505 - acc: 0.6771 - val_loss: 0.7863 - val_acc: 0.6744\n",
      "Epoch 68/300\n",
      "2521/2521 [==============================] - 11s 4ms/step - loss: 0.7497 - acc: 0.6787 - val_loss: 0.7601 - val_acc: 0.6762\n",
      "Epoch 69/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7521 - acc: 0.6783 - val_loss: 0.7599 - val_acc: 0.6716\n",
      "Epoch 70/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7454 - acc: 0.6823 - val_loss: 0.7732 - val_acc: 0.6605\n",
      "Epoch 71/300\n",
      "2521/2521 [==============================] - 9s 3ms/step - loss: 0.7440 - acc: 0.6807 - val_loss: 0.8008 - val_acc: 0.6503\n",
      "Epoch 72/300\n",
      "2521/2521 [==============================] - 6s 3ms/step - loss: 0.7455 - acc: 0.6755 - val_loss: 0.7573 - val_acc: 0.6790\n",
      "Epoch 73/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7482 - acc: 0.6815 - val_loss: 0.7595 - val_acc: 0.6790\n",
      "Epoch 74/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7444 - acc: 0.6823 - val_loss: 0.7559 - val_acc: 0.6781\n",
      "Epoch 75/300\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: 0.7484 - acc: 0.6795 - val_loss: 0.7574 - val_acc: 0.6790\n",
      "Epoch 76/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7415 - acc: 0.6787 - val_loss: 0.7979 - val_acc: 0.6429\n",
      "Epoch 77/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7426 - acc: 0.6771 - val_loss: 0.7927 - val_acc: 0.6660\n",
      "Epoch 78/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7446 - acc: 0.6779 - val_loss: 0.7690 - val_acc: 0.6799\n",
      "Epoch 79/300\n",
      "2521/2521 [==============================] - 9s 4ms/step - loss: 0.7433 - acc: 0.6807 - val_loss: 0.7547 - val_acc: 0.6772\n",
      "Epoch 80/300\n",
      "2521/2521 [==============================] - 9s 4ms/step - loss: 0.7370 - acc: 0.6882 - val_loss: 0.7603 - val_acc: 0.6698\n",
      "Epoch 81/300\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: 0.7379 - acc: 0.6858 - val_loss: 0.7644 - val_acc: 0.6670\n",
      "Epoch 82/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7327 - acc: 0.6850 - val_loss: 0.7549 - val_acc: 0.6836\n",
      "Epoch 83/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7381 - acc: 0.6882 - val_loss: 0.7617 - val_acc: 0.6772\n",
      "Epoch 84/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7361 - acc: 0.6866 - val_loss: 0.7599 - val_acc: 0.6772\n",
      "Epoch 85/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7352 - acc: 0.6862 - val_loss: 0.7527 - val_acc: 0.6772\n",
      "Epoch 86/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7355 - acc: 0.6902 - val_loss: 0.7515 - val_acc: 0.6883\n",
      "Epoch 87/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7315 - acc: 0.6843 - val_loss: 0.7826 - val_acc: 0.6707\n",
      "Epoch 88/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7320 - acc: 0.6854 - val_loss: 0.7563 - val_acc: 0.6753\n",
      "Epoch 89/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7282 - acc: 0.6866 - val_loss: 0.7608 - val_acc: 0.6670\n",
      "Epoch 90/300\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: 0.7332 - acc: 0.6894 - val_loss: 0.7522 - val_acc: 0.6855\n",
      "Epoch 91/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7344 - acc: 0.6886 - val_loss: 0.7691 - val_acc: 0.6633\n",
      "Epoch 92/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7284 - acc: 0.6934 - val_loss: 0.7700 - val_acc: 0.6605\n",
      "Epoch 93/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7289 - acc: 0.6906 - val_loss: 0.7526 - val_acc: 0.6781\n",
      "Epoch 94/300\n",
      "2521/2521 [==============================] - 12s 5ms/step - loss: 0.7256 - acc: 0.6886 - val_loss: 0.7554 - val_acc: 0.6735\n",
      "Epoch 95/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7210 - acc: 0.6902 - val_loss: 0.7631 - val_acc: 0.6670\n",
      "Epoch 96/300\n",
      "2521/2521 [==============================] - 6s 2ms/step - loss: 0.7256 - acc: 0.6878 - val_loss: 0.7575 - val_acc: 0.6744\n",
      "Model architecture saved to: /mnt/silo/andrew/Sentiment_Models/tdparse/model zoo/LSTM SemEval 14 Restaurant architecture.yaml\n",
      "Model weights saved to /mnt/silo/andrew/Sentiment_Models/tdparse/model zoo/LSTM SemEval 14 Restaurant weights.h5\n",
      "Save time 0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/Envs/tdparse/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving raw data\n",
      "Re-writing over previous RAW results\n"
     ]
    }
   ],
   "source": [
    "# Model folder results\n",
    "lstm_folder = os.path.join(result_folder, 'lstm')\n",
    "os.makedirs(lstm_folder, exist_ok=True)\n",
    "\n",
    "# Result files\n",
    "word_vector_file = os.path.join(lstm_folder, 'word vector results.json')\n",
    "result_file = os.path.join(lstm_folder, 'results file.tsv')\n",
    "\n",
    "for dataset_name, train_test in dataset_train_test.items():\n",
    "    train, test = train_test\n",
    "    model_params = {'tokeniser' : ark_twokenize,\n",
    "                    'lower' : True, 'pad_size' : -1}\n",
    "    dataset_predictions(train, test, dataset_name, LSTM, \n",
    "                        word_vector_file, result_file, model_dir, model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass Evaluation of the TDLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mitchel {'tokeniser': <function ark_twokenize at 0x7f2c83bdd950>, 'lower': True, 'pad_size': -1}\n",
      "sswe\n",
      "glove 300d 42b common crawl\n",
      "{'tokeniser': <function ark_twokenize at 0x7f2c83bdd950>, 'lower': True, 'pad_size': -1, 'embeddings': glove 300d 42b common crawl} glove 300d 42b common crawl\n",
      "Train on 1610 samples, validate on 691 samples\n",
      "Epoch 1/5\n",
      "1610/1610 [==============================] - 46s 29ms/step - loss: 0.9869 - acc: 0.6957 - val_loss: 0.9009 - val_acc: 0.7019\n",
      "Epoch 2/5\n",
      "1610/1610 [==============================] - 48s 30ms/step - loss: 0.8628 - acc: 0.7012 - val_loss: 0.8373 - val_acc: 0.7019\n",
      "Epoch 3/5\n",
      "1610/1610 [==============================] - 48s 30ms/step - loss: 0.8251 - acc: 0.7012 - val_loss: 0.8170 - val_acc: 0.7019\n",
      "Epoch 4/5\n",
      "1610/1610 [==============================] - 49s 30ms/step - loss: 0.8125 - acc: 0.7012 - val_loss: 0.8097 - val_acc: 0.7019\n",
      "Epoch 5/5\n",
      "1610/1610 [==============================] - 47s 29ms/step - loss: 0.8064 - acc: 0.7012 - val_loss: 0.8056 - val_acc: 0.7019\n",
      "{'tokeniser': <function ark_twokenize at 0x7f2c83bdd950>, 'lower': True, 'pad_size': -1, 'embeddings': sswe} sswe\n",
      "Train on 1610 samples, validate on 691 samples\n",
      "Epoch 1/5\n",
      "1610/1610 [==============================] - 5s 3ms/step - loss: 0.8850 - acc: 0.6913 - val_loss: 0.8079 - val_acc: 0.7019\n",
      "Epoch 2/5\n",
      "1610/1610 [==============================] - 3s 2ms/step - loss: 0.7949 - acc: 0.7012 - val_loss: 0.7916 - val_acc: 0.7019\n",
      "Epoch 3/5\n",
      "1610/1610 [==============================] - 3s 2ms/step - loss: 0.7883 - acc: 0.7012 - val_loss: 0.7894 - val_acc: 0.7019\n",
      "Epoch 4/5\n",
      "1610/1610 [==============================] - 3s 2ms/step - loss: 0.7867 - acc: 0.7012 - val_loss: 0.7873 - val_acc: 0.7019\n",
      "Epoch 5/5\n",
      "1600/1610 [============================>.] - ETA: 0s - loss: 0.7864 - acc: 0.7006"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-abb461fd71dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                     'lower' : True, 'pad_size' : -1}\n\u001b[1;32m     13\u001b[0m     dataset_predictions(train, test, dataset_name, TDLSTM,\n\u001b[0;32m---> 14\u001b[0;31m                         word_vector_file, result_file, model_dir, model_params)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-5ef4ae043916>\u001b[0m in \u001b[0;36mdataset_predictions\u001b[0;34m(train, test, dataset_name, model_class, word_vector_file_path, result_file_path, model_folder_path, model_params)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_word_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         model.fit(data_train, y_train, validation_size=0.3, verbose=1,\n\u001b[0;32m---> 47\u001b[0;31m                   reproducible=True, patience=10, epochs=5, org_initialisers=True)\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Saves the model to the model zoo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mmodel_folder_join\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_folder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/silo/users/moorea/tdparse/tdparse/models/tdlstm.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, train_y, validation_size, verbose, reproducible, embedding_layer_trainable, lstm_dimension, optimiser, patience, batch_size, epochs, org_initialisers)\u001b[0m\n\u001b[1;32m    854\u001b[0m                                 \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                                 verbose=verbose, batch_size=batch_size)\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0;31m# Load the best model from the saved weight file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1218\u001b[0m                             val_outs = self._test_loop(val_f, val_ins,\n\u001b[1;32m   1219\u001b[0m                                                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m                                                        verbose=0)\n\u001b[0m\u001b[1;32m   1221\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m                                 \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1380\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/tdparse/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model folder results\n",
    "tdlstm_folder = os.path.join(result_folder, 'tdlstm')\n",
    "os.makedirs(tdlstm_folder, exist_ok=True)\n",
    "\n",
    "# Result files\n",
    "word_vector_file = os.path.join(tdlstm_folder, 'word vector results.json')\n",
    "result_file = os.path.join(tdlstm_folder, 'results file.tsv')\n",
    "\n",
    "for dataset_name, train_test in dataset_train_test.items():\n",
    "    train, test = train_test\n",
    "    model_params = {'tokeniser' : ark_twokenize,\n",
    "                    'lower' : True, 'pad_size' : -1}\n",
    "    dataset_predictions(train, test, dataset_name, TDLSTM,\n",
    "                        word_vector_file, result_file, model_dir, model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass Evaluation of the TCLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mitchel {'tokeniser': <function ark_twokenize at 0x7fbd07a14950>, 'lower': True, 'pad_size': -1}\n",
      "sswe\n",
      "{'tokeniser': <function ark_twokenize at 0x7fbd07a14950>, 'lower': True, 'pad_size': -1, 'embeddings': sswe} sswe\n",
      "Train on 1610 samples, validate on 691 samples\n",
      "Epoch 1/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.8468 - acc: 0.7006 - val_loss: 0.7888 - val_acc: 0.7019\n",
      "Epoch 2/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7847 - acc: 0.7012 - val_loss: 0.7809 - val_acc: 0.7019\n",
      "Epoch 3/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7818 - acc: 0.7012 - val_loss: 0.7805 - val_acc: 0.7019\n",
      "Epoch 4/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7814 - acc: 0.7012 - val_loss: 0.7798 - val_acc: 0.7019\n",
      "Epoch 5/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7800 - acc: 0.7012 - val_loss: 0.7791 - val_acc: 0.7019\n",
      "Epoch 6/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7792 - acc: 0.7012 - val_loss: 0.7787 - val_acc: 0.7019\n",
      "Epoch 7/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7787 - acc: 0.7012 - val_loss: 0.7780 - val_acc: 0.7019\n",
      "Epoch 8/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7777 - acc: 0.7012 - val_loss: 0.7787 - val_acc: 0.7019\n",
      "Epoch 9/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7771 - acc: 0.7012 - val_loss: 0.7767 - val_acc: 0.7019\n",
      "Epoch 10/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7761 - acc: 0.7012 - val_loss: 0.7765 - val_acc: 0.7019\n",
      "Epoch 11/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7754 - acc: 0.7012 - val_loss: 0.7757 - val_acc: 0.7019\n",
      "Epoch 12/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7743 - acc: 0.7012 - val_loss: 0.7746 - val_acc: 0.7019\n",
      "Epoch 13/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7733 - acc: 0.7012 - val_loss: 0.7740 - val_acc: 0.7019\n",
      "Epoch 14/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7729 - acc: 0.7012 - val_loss: 0.7731 - val_acc: 0.7019\n",
      "Epoch 15/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7719 - acc: 0.7012 - val_loss: 0.7716 - val_acc: 0.7033\n",
      "Epoch 16/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7708 - acc: 0.7006 - val_loss: 0.7709 - val_acc: 0.7033\n",
      "Epoch 17/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7688 - acc: 0.7006 - val_loss: 0.7700 - val_acc: 0.7033\n",
      "Epoch 18/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7695 - acc: 0.7006 - val_loss: 0.7701 - val_acc: 0.7033\n",
      "Epoch 19/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7683 - acc: 0.7006 - val_loss: 0.7708 - val_acc: 0.7033\n",
      "Epoch 20/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7670 - acc: 0.7006 - val_loss: 0.7672 - val_acc: 0.7033\n",
      "Epoch 21/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7642 - acc: 0.7012 - val_loss: 0.7683 - val_acc: 0.7033\n",
      "Epoch 22/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7640 - acc: 0.7012 - val_loss: 0.7654 - val_acc: 0.7033\n",
      "Epoch 23/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7621 - acc: 0.7019 - val_loss: 0.7686 - val_acc: 0.7033\n",
      "Epoch 24/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7600 - acc: 0.7012 - val_loss: 0.7700 - val_acc: 0.7033\n",
      "Epoch 25/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7620 - acc: 0.7012 - val_loss: 0.7701 - val_acc: 0.7033\n",
      "Epoch 26/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7585 - acc: 0.7019 - val_loss: 0.7606 - val_acc: 0.7033\n",
      "Epoch 27/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7573 - acc: 0.7025 - val_loss: 0.7614 - val_acc: 0.7033\n",
      "Epoch 28/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7566 - acc: 0.7025 - val_loss: 0.7583 - val_acc: 0.7048\n",
      "Epoch 29/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7518 - acc: 0.7031 - val_loss: 0.7957 - val_acc: 0.7033\n",
      "Epoch 30/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7568 - acc: 0.7043 - val_loss: 0.7555 - val_acc: 0.7033\n",
      "Epoch 31/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7506 - acc: 0.7043 - val_loss: 0.7634 - val_acc: 0.7048\n",
      "Epoch 32/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7515 - acc: 0.7043 - val_loss: 0.7536 - val_acc: 0.7048\n",
      "Epoch 33/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7518 - acc: 0.7043 - val_loss: 0.7580 - val_acc: 0.7033\n",
      "Epoch 34/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7492 - acc: 0.7043 - val_loss: 0.7503 - val_acc: 0.7033\n",
      "Epoch 35/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7449 - acc: 0.7062 - val_loss: 0.7591 - val_acc: 0.7033\n",
      "Epoch 36/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7436 - acc: 0.7056 - val_loss: 0.7637 - val_acc: 0.7033\n",
      "Epoch 37/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7404 - acc: 0.7050 - val_loss: 0.7494 - val_acc: 0.7033\n",
      "Epoch 38/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7422 - acc: 0.7050 - val_loss: 0.7847 - val_acc: 0.7033\n",
      "Epoch 39/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7419 - acc: 0.7050 - val_loss: 0.7593 - val_acc: 0.7048\n",
      "Epoch 40/300\n",
      "1610/1610 [==============================] - 7s 4ms/step - loss: 0.7396 - acc: 0.7062 - val_loss: 0.7464 - val_acc: 0.7033\n",
      "Epoch 41/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7362 - acc: 0.7075 - val_loss: 0.7365 - val_acc: 0.7033\n",
      "Epoch 42/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7352 - acc: 0.7075 - val_loss: 0.7639 - val_acc: 0.7033\n",
      "Epoch 43/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7364 - acc: 0.7062 - val_loss: 0.7443 - val_acc: 0.7019\n",
      "Epoch 44/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7335 - acc: 0.7062 - val_loss: 0.7378 - val_acc: 0.7048\n",
      "Epoch 45/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7295 - acc: 0.7068 - val_loss: 0.7376 - val_acc: 0.7019\n",
      "Epoch 46/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7317 - acc: 0.7093 - val_loss: 0.7368 - val_acc: 0.7062\n",
      "Epoch 47/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7309 - acc: 0.7093 - val_loss: 0.7546 - val_acc: 0.7048\n",
      "Epoch 48/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7277 - acc: 0.7112 - val_loss: 0.7408 - val_acc: 0.7004\n",
      "Epoch 49/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7188 - acc: 0.7075 - val_loss: 0.9180 - val_acc: 0.4616\n",
      "Epoch 50/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7346 - acc: 0.7037 - val_loss: 0.7440 - val_acc: 0.6990\n",
      "Epoch 51/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7219 - acc: 0.7093 - val_loss: 0.7350 - val_acc: 0.7004\n",
      "Epoch 52/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7173 - acc: 0.7112 - val_loss: 0.7730 - val_acc: 0.6975\n",
      "Epoch 53/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7180 - acc: 0.7112 - val_loss: 0.7417 - val_acc: 0.7048\n",
      "Epoch 54/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7209 - acc: 0.7168 - val_loss: 0.7267 - val_acc: 0.7048\n",
      "Epoch 55/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7185 - acc: 0.7137 - val_loss: 0.8136 - val_acc: 0.7033\n",
      "Epoch 56/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7157 - acc: 0.7112 - val_loss: 0.7342 - val_acc: 0.7019\n",
      "Epoch 57/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7099 - acc: 0.7168 - val_loss: 0.7958 - val_acc: 0.6744\n",
      "Epoch 58/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7156 - acc: 0.7130 - val_loss: 0.7300 - val_acc: 0.7033\n",
      "Epoch 59/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7087 - acc: 0.7168 - val_loss: 0.8884 - val_acc: 0.5007\n",
      "Epoch 60/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7105 - acc: 0.7161 - val_loss: 0.7431 - val_acc: 0.6990\n",
      "Epoch 61/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7057 - acc: 0.7155 - val_loss: 0.7199 - val_acc: 0.7062\n",
      "Epoch 62/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7044 - acc: 0.7137 - val_loss: 0.7238 - val_acc: 0.7019\n",
      "Epoch 63/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7023 - acc: 0.7193 - val_loss: 0.7397 - val_acc: 0.7033\n",
      "Epoch 64/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7055 - acc: 0.7161 - val_loss: 0.7722 - val_acc: 0.6990\n",
      "Epoch 65/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7002 - acc: 0.7205 - val_loss: 0.8458 - val_acc: 0.6397\n",
      "Epoch 66/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7042 - acc: 0.7224 - val_loss: 0.7202 - val_acc: 0.7048\n",
      "Epoch 67/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6996 - acc: 0.7174 - val_loss: 0.7190 - val_acc: 0.7091\n",
      "Epoch 68/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6987 - acc: 0.7180 - val_loss: 0.7203 - val_acc: 0.7019\n",
      "Epoch 69/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7027 - acc: 0.7174 - val_loss: 0.9020 - val_acc: 0.5137\n",
      "Epoch 70/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6963 - acc: 0.7161 - val_loss: 0.7358 - val_acc: 0.7004\n",
      "Epoch 71/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6869 - acc: 0.7174 - val_loss: 1.0217 - val_acc: 0.3140\n",
      "Epoch 72/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6933 - acc: 0.7211 - val_loss: 0.7866 - val_acc: 0.6918\n",
      "Epoch 73/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6972 - acc: 0.7174 - val_loss: 0.7369 - val_acc: 0.7062\n",
      "Epoch 74/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6925 - acc: 0.7248 - val_loss: 0.7210 - val_acc: 0.7019\n",
      "Epoch 75/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6893 - acc: 0.7211 - val_loss: 0.7048 - val_acc: 0.7033\n",
      "Epoch 76/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6912 - acc: 0.7267 - val_loss: 1.2419 - val_acc: 0.2605\n",
      "Epoch 77/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.7012 - acc: 0.7193 - val_loss: 0.7304 - val_acc: 0.7033\n",
      "Epoch 78/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6820 - acc: 0.7273 - val_loss: 0.7171 - val_acc: 0.6975\n",
      "Epoch 79/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6862 - acc: 0.7292 - val_loss: 0.7849 - val_acc: 0.6585\n",
      "Epoch 80/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6744 - acc: 0.7280 - val_loss: 0.7444 - val_acc: 0.7091\n",
      "Epoch 81/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6869 - acc: 0.7217 - val_loss: 0.7124 - val_acc: 0.7004\n",
      "Epoch 82/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6707 - acc: 0.7298 - val_loss: 0.7182 - val_acc: 0.7033\n",
      "Epoch 83/300\n",
      "1610/1610 [==============================] - 7s 4ms/step - loss: 0.6794 - acc: 0.7230 - val_loss: 0.6990 - val_acc: 0.7033\n",
      "Epoch 84/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6735 - acc: 0.7273 - val_loss: 0.7330 - val_acc: 0.7077\n",
      "Epoch 85/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6668 - acc: 0.7267 - val_loss: 1.0812 - val_acc: 0.3242\n",
      "Epoch 86/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6716 - acc: 0.7236 - val_loss: 0.8109 - val_acc: 0.6729\n",
      "Epoch 87/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6655 - acc: 0.7354 - val_loss: 0.7050 - val_acc: 0.7077\n",
      "Epoch 88/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6647 - acc: 0.7335 - val_loss: 0.7500 - val_acc: 0.6918\n",
      "Epoch 89/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6565 - acc: 0.7304 - val_loss: 0.8964 - val_acc: 0.5311\n",
      "Epoch 90/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6608 - acc: 0.7242 - val_loss: 0.7181 - val_acc: 0.7033\n",
      "Epoch 91/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6665 - acc: 0.7261 - val_loss: 0.8363 - val_acc: 0.5962\n",
      "Epoch 92/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6618 - acc: 0.7348 - val_loss: 0.7173 - val_acc: 0.7019\n",
      "Epoch 93/300\n",
      "1610/1610 [==============================] - 6s 4ms/step - loss: 0.6641 - acc: 0.7280 - val_loss: 0.7287 - val_acc: 0.7077\n",
      "glove 300d 42b common crawl\n",
      "{'tokeniser': <function ark_twokenize at 0x7fbd07a14950>, 'lower': True, 'pad_size': -1, 'embeddings': glove 300d 42b common crawl} glove 300d 42b common crawl\n",
      "Train on 1610 samples, validate on 691 samples\n",
      "Epoch 1/300\n",
      "1610/1610 [==============================] - 132s 82ms/step - loss: 0.9759 - acc: 0.6925 - val_loss: 0.8845 - val_acc: 0.7019\n",
      "Epoch 2/300\n",
      "1610/1610 [==============================] - 136s 85ms/step - loss: 0.8516 - acc: 0.7012 - val_loss: 0.8278 - val_acc: 0.7019\n",
      "Epoch 3/300\n",
      "1610/1610 [==============================] - 135s 84ms/step - loss: 0.8201 - acc: 0.7012 - val_loss: 0.8119 - val_acc: 0.7019\n",
      "Epoch 4/300\n",
      "1610/1610 [==============================] - 135s 84ms/step - loss: 0.8094 - acc: 0.7012 - val_loss: 0.8062 - val_acc: 0.7019\n",
      "Epoch 5/300\n",
      "1610/1610 [==============================] - 137s 85ms/step - loss: 0.8032 - acc: 0.7012 - val_loss: 0.8017 - val_acc: 0.7019\n",
      "Epoch 6/300\n",
      "1610/1610 [==============================] - 136s 84ms/step - loss: 0.7983 - acc: 0.7012 - val_loss: 0.7984 - val_acc: 0.7019\n",
      "Epoch 7/300\n",
      "1610/1610 [==============================] - 152s 95ms/step - loss: 0.7941 - acc: 0.7012 - val_loss: 0.7953 - val_acc: 0.7019\n",
      "Epoch 8/300\n",
      "1610/1610 [==============================] - 151s 94ms/step - loss: 0.7903 - acc: 0.7012 - val_loss: 0.7924 - val_acc: 0.7019\n",
      "Epoch 9/300\n",
      "1610/1610 [==============================] - 153s 95ms/step - loss: 0.7864 - acc: 0.7012 - val_loss: 0.7897 - val_acc: 0.7019\n",
      "Epoch 10/300\n",
      "1610/1610 [==============================] - 156s 97ms/step - loss: 0.7827 - acc: 0.7012 - val_loss: 0.7870 - val_acc: 0.7019\n",
      "Epoch 11/300\n",
      "1610/1610 [==============================] - 154s 96ms/step - loss: 0.7788 - acc: 0.7012 - val_loss: 0.7845 - val_acc: 0.7019\n",
      "Epoch 12/300\n",
      "1610/1610 [==============================] - 146s 90ms/step - loss: 0.7753 - acc: 0.7012 - val_loss: 0.7820 - val_acc: 0.7019\n",
      "Epoch 13/300\n",
      "1610/1610 [==============================] - 146s 91ms/step - loss: 0.7717 - acc: 0.7012 - val_loss: 0.7796 - val_acc: 0.7019\n",
      "Epoch 14/300\n",
      "1610/1610 [==============================] - 147s 91ms/step - loss: 0.7682 - acc: 0.7012 - val_loss: 0.7771 - val_acc: 0.7019\n",
      "Epoch 15/300\n",
      "1610/1610 [==============================] - 145s 90ms/step - loss: 0.7648 - acc: 0.7012 - val_loss: 0.7747 - val_acc: 0.7019\n",
      "Epoch 16/300\n",
      "1610/1610 [==============================] - 146s 91ms/step - loss: 0.7610 - acc: 0.7012 - val_loss: 0.7721 - val_acc: 0.7019\n",
      "Epoch 17/300\n",
      "1610/1610 [==============================] - 146s 91ms/step - loss: 0.7575 - acc: 0.7012 - val_loss: 0.7696 - val_acc: 0.7019\n",
      "Epoch 18/300\n",
      "1610/1610 [==============================] - 146s 91ms/step - loss: 0.7538 - acc: 0.7012 - val_loss: 0.7672 - val_acc: 0.7019\n",
      "Epoch 19/300\n",
      "1610/1610 [==============================] - 143s 89ms/step - loss: 0.7500 - acc: 0.7019 - val_loss: 0.7646 - val_acc: 0.7019\n",
      "Epoch 20/300\n",
      "1610/1610 [==============================] - 150s 93ms/step - loss: 0.7462 - acc: 0.7031 - val_loss: 0.7621 - val_acc: 0.7019\n",
      "Epoch 21/300\n",
      "1610/1610 [==============================] - 142s 88ms/step - loss: 0.7422 - acc: 0.7037 - val_loss: 0.7602 - val_acc: 0.7019\n",
      "Epoch 22/300\n",
      "1610/1610 [==============================] - 151s 94ms/step - loss: 0.7386 - acc: 0.7037 - val_loss: 0.7579 - val_acc: 0.7019\n",
      "Epoch 23/300\n",
      "1610/1610 [==============================] - 145s 90ms/step - loss: 0.7352 - acc: 0.7062 - val_loss: 0.7559 - val_acc: 0.7019\n",
      "Epoch 24/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1610/1610 [==============================] - 147s 91ms/step - loss: 0.7314 - acc: 0.7062 - val_loss: 0.7529 - val_acc: 0.7019\n",
      "Epoch 25/300\n",
      "1610/1610 [==============================] - 151s 94ms/step - loss: 0.7279 - acc: 0.7087 - val_loss: 0.7509 - val_acc: 0.7019\n",
      "Epoch 26/300\n",
      "1408/1610 [=========================>....] - ETA: 13s - loss: 0.7263 - acc: 0.7067"
     ]
    }
   ],
   "source": [
    "# Model folder results\n",
    "tclstm_folder = os.path.join(result_folder, 'tclstm')\n",
    "os.makedirs(tclstm_folder, exist_ok=True)\n",
    "\n",
    "# Result files\n",
    "word_vector_file = os.path.join(tclstm_folder, 'word vector results.json')\n",
    "result_file = os.path.join(tclstm_folder, 'results file.tsv')\n",
    "\n",
    "for dataset_name, train_test in dataset_train_test.items():\n",
    "    train, test = train_test\n",
    "    model_params = {'tokeniser' : ark_twokenize,\n",
    "                    'lower' : True, 'pad_size' : -1}\n",
    "    dataset_predictions(train, test, dataset_name, TCLSTM, \n",
    "                        word_vector_file, result_file, model_dir, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
