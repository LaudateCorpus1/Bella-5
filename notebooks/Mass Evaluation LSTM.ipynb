{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from multiprocessing.pool import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras import initializers, optimizers\n",
    "\n",
    "from bella.data_types import TargetCollection, Target\n",
    "# Models\n",
    "from bella.models.tdlstm import LSTM, TDLSTM, TCLSTM\n",
    "# Tokenisers\n",
    "from bella.tokenisers import ark_twokenize\n",
    "# Word Vectors\n",
    "from bella.word_vectors import SSWE, GloveCommonCrawl\n",
    "# Get the data\n",
    "from bella.parsers import semeval_14, dong, election\n",
    "from bella.helper import read_config\n",
    "from bella.evaluation import evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(train, split_size=0.2, seed=42):\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=split_size, random_state=seed)\n",
    "    data = np.asarray(train.data_dict())\n",
    "    sentiment = np.asarray(train.sentiment_data())\n",
    "    for train_indexs, test_indexs in splitter.split(data, sentiment):\n",
    "        train_data = data[train_indexs]\n",
    "        test_data = data[test_indexs]\n",
    "\n",
    "    convert_to_targets = lambda data: [Target(**target) for target in data]\n",
    "    train = TargetCollection(convert_to_targets(train_data))\n",
    "    val = TargetCollection(convert_to_targets(test_data))\n",
    "\n",
    "    X_train = np.array(train.data_dict())\n",
    "    y_train = np.array(train.sentiment_data())\n",
    "    X_val = np.array(val.data_dict())\n",
    "    y_val = np.array(val.sentiment_data())\n",
    "    return (X_train, y_train), (X_val, y_val)\n",
    "\n",
    "def class_mapper(y, inverse=False):\n",
    "    class_map = {-1: 0, 0: 1, 1: 2}\n",
    "    inv_class_map = {0: -1, 1: 0, 2: 1}\n",
    "    if inverse:\n",
    "        return np.array([inv_class_map[val] for val in y])\n",
    "    return np.array([class_map[val] for val in y])\n",
    "\n",
    "uniform_init = initializers.RandomUniform(minval=-0.003, maxval=0.003)\n",
    "lstm_layer_kwargs = {'kernel_initializer' : uniform_init,\n",
    "                     'recurrent_initializer' : uniform_init,\n",
    "                     'bias_initializer' : uniform_init}\n",
    "dense_layer_kwargs = {'kernel_initializer' : uniform_init,\n",
    "                      'bias_initializer' : uniform_init}\n",
    "embedding_layer_kwargs = {'embeddings_initializer' : uniform_init}\n",
    "model_kwargs = {'lstm_layer_kwargs': lstm_layer_kwargs,\n",
    "                'dense_layer_kwargs': dense_layer_kwargs,\n",
    "                'embedding_layer_kwargs': embedding_layer_kwargs,\n",
    "                'optimiser': optimizers.SGD,\n",
    "                'optimiser_params': {'lr': 0.01},\n",
    "                'reproducible': 42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_word_vector(word_vectors, dataset_train_test, model, model_result_folder):\n",
    "    dataset_name, train_data, _ = dataset_train_test\n",
    "    (X_train, y_train), (X_val, y_val) = train_val_split(train_data)\n",
    "    y_train, y_val = class_mapper(y_train), class_mapper(y_val)\n",
    "    word_vector_scores = {}\n",
    "    for word_vector in word_vectors:\n",
    "        model.embeddings = word_vector\n",
    "        model_history = model.fit(X_train, y_train, (X_val, y_val))\n",
    "        best_score = max(model_history.history['val_acc'])\n",
    "        word_vector_scores[word_vector] = best_score\n",
    "    \n",
    "    # Saving results\n",
    "    model_word_vector_fp = model_result_folder.joinpath('word vectors.json')\n",
    "    \n",
    "    dataset_word_vector_scores = {}\n",
    "    if model_word_vector_fp.is_file():\n",
    "        with model_word_vector_fp.open('r') as model_word_vector_file:\n",
    "            dataset_word_vector_scores = json.load(model_word_vector_file)\n",
    "    dataset_word_vector_scores[dataset_name] = {str(vector): score \n",
    "                                                for vector, score in word_vector_scores.items()}\n",
    "    with model_word_vector_fp.open('w') as model_word_vector_file:\n",
    "            json.dump(dataset_word_vector_scores, model_word_vector_file)\n",
    "        \n",
    "    return word_vector_scores\n",
    "\n",
    "def dataset_predictions(dataset_train_test, model, model_result_folder):\n",
    "    # Preparing the data\n",
    "    dataset_name, train_data, test_data = dataset_train_test\n",
    "    (X_train, y_train), (X_val, y_val) = train_val_split(train_data)\n",
    "    y_train, y_val = class_mapper(y_train), class_mapper(y_val)\n",
    "    X_test = test_data.data_dict()\n",
    "    \n",
    "    model_history = model.fit(X_train, y_train, (X_val, y_val))\n",
    "    predictions = model.predict(X_test)\n",
    "    # Change back to the original class labels from the mapping version\n",
    "    predictions = class_mapper(predictions, inverse=True)\n",
    "    predictions = predictions.tolist()\n",
    "    \n",
    "    # Saving results\n",
    "    model_dataset_fp = model_result_folder.joinpath('dataset predictions.json')\n",
    "    \n",
    "    dataset_predictions = {}\n",
    "    if model_dataset_fp.is_file():\n",
    "        with model_dataset_fp.open('r') as model_dataset_file:\n",
    "            dataset_predictions = json.load(model_dataset_file)\n",
    "    dataset_predictions[dataset_name] = predictions\n",
    "    with model_dataset_fp.open('w') as model_dataset_file:\n",
    "            json.dump(dataset_predictions, model_dataset_file)\n",
    "    return predictions\n",
    "\n",
    "def model_evaluation(model_class, word_vectors, dataset_train_test, \n",
    "                     result_folder, model_zoo_folder, model_kwargs=None):\n",
    "    if model_kwargs is None:\n",
    "        model_kwargs = {}\n",
    "    model = model_class(ark_twokenize, word_vectors[0], **model_kwargs)\n",
    "    model_result_folder = result_folder.joinpath(f'{str(model)}')\n",
    "    model_result_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    vector_scores = best_word_vector(word_vectors, dataset_train_test, \n",
    "                                     model, model_result_folder)\n",
    "    best_vector = max(vector_scores.items(), key=lambda x: x[1])[0]\n",
    "    model.embeddings = best_vector\n",
    "    best_predictions = dataset_predictions(dataset_train_test, model, model_result_folder)\n",
    "    dataset_name = dataset_train_test[0]\n",
    "    # Save model\n",
    "    model_zoo_file = model_zoo_folder.joinpath(f'{str(model)} {dataset_name}')\n",
    "    model.save(model, model_zoo_file)\n",
    "    return dataset_name, best_predictions\n",
    "\n",
    "def load_and_run(model_class, dataset_train_test, \n",
    "                 results_folder, model_zoo_folder, model_kwargs=None):\n",
    "    if model_kwargs is None:\n",
    "        model_kwargs = {}\n",
    "    # Setting the word vectors up for each dataset\n",
    "    train_list = dataset_train_test[1].word_list(ark_twokenize)\n",
    "    test_list = dataset_train_test[2].word_list(ark_twokenize)\n",
    "    all_words = list(set(train_list + test_list))\n",
    "    sswe = SSWE(filter_words=all_words)\n",
    "    glove_300 = GloveCommonCrawl(version=42, filter_words=all_words)\n",
    "    word_vectors = [sswe, glove_300]\n",
    "    \n",
    "    return model_evaluation(model_class, word_vectors, dataset_train_test, \n",
    "                            results_folder, model_zoo_folder, model_kwargs)\n",
    "\n",
    "def model_evaluation_args(model_classes, dataset_train_test, \n",
    "                          result_folder, model_zoo_folder, model_kwargs=None):\n",
    "    for model_class in model_classes:\n",
    "        for dataset_name_train_test in dataset_train_test:\n",
    "            yield (model_class, dataset_name_train_test, \n",
    "                   results_folder, model_zoo_folder, model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#  ADD YOUR CONFIG FILE PATH HERE \n",
    "##\n",
    "CONFIG_FP = Path('..', 'config.yaml')\n",
    "\n",
    "# Load all of the datasets\n",
    "youtubean_train = semeval_14(read_config('youtubean_train', CONFIG_FP))\n",
    "youtubean_test = semeval_14(read_config('youtubean_test', CONFIG_FP))\n",
    "semeval_14_rest_train = semeval_14(read_config('semeval_2014_rest_train', CONFIG_FP))\n",
    "semeval_14_lap_train = semeval_14(read_config('semeval_2014_lap_train', CONFIG_FP))\n",
    "semeval_14_rest_test = semeval_14(read_config('semeval_2014_rest_test', CONFIG_FP))\n",
    "semeval_14_lap_test = semeval_14(read_config('semeval_2014_lap_test', CONFIG_FP))\n",
    "dong_train = dong(read_config('dong_twit_train_data', CONFIG_FP))\n",
    "dong_test = dong(read_config('dong_twit_test_data', CONFIG_FP))\n",
    "election_train, election_test = election(read_config('election_folder_dir', CONFIG_FP))\n",
    "mitchel_train = semeval_14(read_config('mitchel_train', CONFIG_FP))\n",
    "mitchel_test = semeval_14(read_config('mitchel_test', CONFIG_FP))\n",
    "\n",
    "\n",
    "dataset_train_test = [('SemEval 14 Laptop', semeval_14_lap_train, semeval_14_lap_test),\n",
    "                      ('SemEval 14 Restaurant', semeval_14_rest_train, semeval_14_rest_test),\n",
    "                      ('Dong Twitter', dong_train, dong_test),\n",
    "                      ('Election Twitter', election_train, election_test),\n",
    "                      ('YouTuBean', youtubean_train, youtubean_test),\n",
    "                      ('Mitchel', mitchel_train, mitchel_test)]\n",
    "\n",
    "\n",
    "results_folder = Path(read_config('results_folder', CONFIG_FP))\n",
    "results_folder = results_folder.joinpath('TDLstm')\n",
    "model_zoo_folder = Path(read_config('model_zoo_folder', CONFIG_FP))\n",
    "model_zoo_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass evaluation of the LSTM, TDLSTM, and TCLSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading glove 300d 42b common crawl from file\n",
      "Loading glove 300d 42b common crawl from file\n",
      "Loading glove 300d 42b common crawl from file\n",
      "Loading glove 300d 42b common crawl from file\n",
      "Loading glove 300d 42b common crawl from file\n",
      "Loading glove 300d 42b common crawl from file\n",
      "Loading glove 300d 42b common crawl from file\n",
      "Loading glove 300d 42b common crawl from file\n",
      "Loading glove 300d 42b common crawl from file\n",
      "Loading glove 300d 42b common crawl from file\n",
      "Loading glove 300d 42b common crawl from file\n",
      "Loading glove 300d 42b common crawl from file\n",
      "Loading glove 300d 42b common crawl from file\n",
      "Loading glove 300d 42b common crawl from file\n",
      "Loading glove 300d 42b common crawl from file\n",
      "Loading glove 300d 42b common crawl from file\n",
      "Loading glove 300d 42b common crawl from file\n",
      "Loading glove 300d 42b common crawl from file\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Number of cpus to use\n",
    "#\n",
    "n_cpus = 15\n",
    "\n",
    "model_eval_args = args = model_evaluation_args([LSTM, TDLSTM, TCLSTM], dataset_train_test, results_folder,\n",
    "                                               model_zoo_folder, model_kwargs)\n",
    "dataset_name_predictions = []\n",
    "with Pool(n_cpus) as pool:\n",
    "    dataset_name_predictions = pool.starmap(load_and_run, model_eval_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
