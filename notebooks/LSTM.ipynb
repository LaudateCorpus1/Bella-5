{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import time\n",
    "import tempfile\n",
    "import pickle\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Notebook helper methods\n",
    "from tdparse import notebook_helper\n",
    "# Models\n",
    "from tdparse.models.tdlstm import LSTM, TDLSTM, TCLSTM\n",
    "# Tokenisers\n",
    "from tdparse.tokenisers import whitespace, ark_twokenize\n",
    "# Word Vectors\n",
    "from tdparse.word_vectors import PreTrained, GloveTwitterVectors, GloveCommonCrawl42\n",
    "# Get the data\n",
    "from tdparse.helper import read_config, full_path\n",
    "from tdparse.parsers import dong, semeval_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "dong_train = dong(full_path(read_config('dong_twit_train_data')))\n",
    "dong_test = dong(full_path(read_config('dong_twit_test_data')))\n",
    "# Load the word vectors\n",
    "sswe_path = full_path(read_config('sswe_files')['vo_zhang'])\n",
    "sswe = PreTrained(sswe_path, name='sswe')\n",
    "glove_50 = GloveTwitterVectors(50)\n",
    "glove_100 = GloveTwitterVectors(100)\n",
    "glove_200 = GloveTwitterVectors(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General results folder for the method\n",
    "result_folder = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results', 'TDLstm'))\n",
    "# Results folder for trying different pad sizes on different embeddings on different \n",
    "# datasets. Where the method is kept the same which is TCLSTM method\n",
    "pad_size_results_folder = os.path.join(result_folder, 'TCLstm Pad Sizes')\n",
    "os.makedirs(pad_size_results_folder, exist_ok=True)\n",
    "\n",
    "def half_average_dataset(dataset):\n",
    "    '''\n",
    "    :param dataset: A training dataset\n",
    "    :type dataset: TargetCollection\n",
    "    :returns: Half the average sentence length of the given dataset\n",
    "    :rtype: int\n",
    "    '''\n",
    "    sentence_lengths = [len(data['text'].split()) for data in dataset.data()]\n",
    "    return math.ceil(sum(sentence_lengths) / len(sentence_lengths) / 2)\n",
    "\n",
    "def pad_size_prediction(embedding_path, pad_size, reproducible, train_data, \n",
    "                        test_data, result_path, index_column, tokenizer=ark_twokenize):\n",
    "    '''\n",
    "    Works out the preiction of \n",
    "    '''\n",
    "\n",
    "    embedding = None\n",
    "    with open(embedding_path, 'rb') as embedding_file:\n",
    "        embedding = pickle.load(embedding_file, encoding='utf-8')\n",
    "    \n",
    "    embedding_name = '{}'.format(embedding)\n",
    "    saved_data = notebook_helper.get_pandas_data(result_path, pad_size, \n",
    "                                                 embedding_name, index_column)\n",
    "    if saved_data is not None:\n",
    "        return saved_data\n",
    "    model = TCLSTM(tokenizer, embedding, lower=True, pad_size=pad_size)\n",
    "    model.fit(train_data.data_dict(), train_data.sentiment_data(), reproducible=True,\n",
    "              validation_size=0.3, patience=10, epochs=300, verbose=1,\n",
    "              org_initialisers=True)\n",
    "    predictions = model.predict(test_data.data_dict())\n",
    "    score = TCLSTM.score(test_data.sentiment_data(), predictions, accuracy_score)\n",
    "    notebook_helper.save_pandas_data(result_path, pad_size, \n",
    "                                     embedding_name, index_column, score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding pad results done sswe\n",
      "Embedding pad results done glove twitter 50d\n",
      "Embedding pad results done glove twitter 100d\n",
      "Embedding pad results done glove twitter 200d\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <th>9</th>\n",
       "      <th>15</th>\n",
       "      <th>-1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embeddings</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sswe</th>\n",
       "      <td>0.606936</td>\n",
       "      <td>0.628613</td>\n",
       "      <td>0.614162</td>\n",
       "      <td>0.647399</td>\n",
       "      <td>0.660405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove twitter 50d</th>\n",
       "      <td>0.644509</td>\n",
       "      <td>0.645954</td>\n",
       "      <td>0.667630</td>\n",
       "      <td>0.654624</td>\n",
       "      <td>0.609827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove twitter 100d</th>\n",
       "      <td>0.663295</td>\n",
       "      <td>0.670520</td>\n",
       "      <td>0.661850</td>\n",
       "      <td>0.674855</td>\n",
       "      <td>0.624277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove twitter 200d</th>\n",
       "      <td>0.643064</td>\n",
       "      <td>0.673410</td>\n",
       "      <td>0.669075</td>\n",
       "      <td>0.669075</td>\n",
       "      <td>0.635838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           5        10         9        15        -1\n",
       "embeddings                                                          \n",
       "sswe                0.606936  0.628613  0.614162  0.647399  0.660405\n",
       "glove twitter 50d   0.644509  0.645954  0.667630  0.654624  0.609827\n",
       "glove twitter 100d  0.663295  0.670520  0.661850  0.674855  0.624277\n",
       "glove twitter 200d  0.643064  0.673410  0.669075  0.669075  0.635838"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = [sswe, glove_50, glove_100, glove_200]\n",
    "embedding_names = ['{}'.format(embedding) for embedding in embeddings]\n",
    "pad_sizes = [5, 10, half_average_dataset(dong_train), 15, -1]\n",
    "reproducible = [True]\n",
    "train_data = [dong_train]\n",
    "test_data = [dong_test]\n",
    "pad_results = []\n",
    "\n",
    "index_column = 'embeddings'\n",
    "dong_pad_size_res_file = os.path.join(pad_size_results_folder, 'Dong Twitter.csv')\n",
    "notebook_helper.create_pandas_file(dong_pad_size_res_file, pad_sizes, index_column, \n",
    "                                   embedding_names, re_write=False)\n",
    "for embedding in embeddings:\n",
    "    with tempfile.NamedTemporaryFile() as embedding_file:\n",
    "        embedding_path = embedding_file.name\n",
    "        pickle.dump(embedding, embedding_file)\n",
    "        pred_params = list(itertools.product([embedding_path], pad_sizes, reproducible, \n",
    "                                             train_data, test_data, \n",
    "                                             [dong_pad_size_res_file], \n",
    "                                             [index_column]))\n",
    "        for pred_param in pred_params: \n",
    "            pad_size_prediction(*pred_param)\n",
    "        #with Pool(3) as pool:\n",
    "        #    pool.starmap(pad_size_prediction, pred_params)\n",
    "        print('Embedding pad results done {}'.format(embedding))\n",
    "dong_pad_df_results = pd.read_csv(open(dong_pad_size_res_file, 'r'))\n",
    "dong_pad_df_results = dong_pad_df_results.set_index('embeddings')\n",
    "dong_pad_df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2521 samples, validate on 1081 samples\n",
      "Epoch 1/300\n",
      "2521/2521 [==============================] - 104s 41ms/step - loss: 0.9795 - acc: 0.5982 - val_loss: 0.9552 - val_acc: 0.6004\n",
      "Epoch 2/300\n",
      "2521/2521 [==============================] - 99s 39ms/step - loss: 0.9465 - acc: 0.6010 - val_loss: 0.9414 - val_acc: 0.6004\n",
      "Epoch 3/300\n",
      "2521/2521 [==============================] - 94s 37ms/step - loss: 0.9336 - acc: 0.6010 - val_loss: 0.9298 - val_acc: 0.6004\n",
      "Epoch 4/300\n",
      "2521/2521 [==============================] - 96s 38ms/step - loss: 0.9223 - acc: 0.6013 - val_loss: 0.9203 - val_acc: 0.6004\n",
      "Epoch 5/300\n",
      "2521/2521 [==============================] - 96s 38ms/step - loss: 0.9122 - acc: 0.6017 - val_loss: 0.9117 - val_acc: 0.6004\n",
      "Epoch 6/300\n",
      "2521/2521 [==============================] - 96s 38ms/step - loss: 0.9035 - acc: 0.6045 - val_loss: 0.9031 - val_acc: 0.6041\n",
      "Epoch 7/300\n",
      "2521/2521 [==============================] - 95s 37ms/step - loss: 0.8947 - acc: 0.6109 - val_loss: 0.8962 - val_acc: 0.6041\n",
      "Epoch 8/300\n",
      "2521/2521 [==============================] - 95s 38ms/step - loss: 0.8862 - acc: 0.6129 - val_loss: 0.8874 - val_acc: 0.6189\n",
      "Epoch 9/300\n",
      "2521/2521 [==============================] - 95s 38ms/step - loss: 0.8787 - acc: 0.6232 - val_loss: 0.8805 - val_acc: 0.6198\n",
      "Epoch 10/300\n",
      "2521/2521 [==============================] - 95s 38ms/step - loss: 0.8714 - acc: 0.6275 - val_loss: 0.8741 - val_acc: 0.6207\n",
      "Epoch 11/300\n",
      "2521/2521 [==============================] - 94s 37ms/step - loss: 0.8635 - acc: 0.6307 - val_loss: 0.8658 - val_acc: 0.6189\n",
      "Epoch 12/300\n",
      "2521/2521 [==============================] - 95s 38ms/step - loss: 0.8561 - acc: 0.6370 - val_loss: 0.8599 - val_acc: 0.6207\n",
      "Epoch 13/300\n",
      "2521/2521 [==============================] - 92s 36ms/step - loss: 0.8476 - acc: 0.6398 - val_loss: 0.8537 - val_acc: 0.6189\n",
      "Epoch 14/300\n",
      "2521/2521 [==============================] - 92s 36ms/step - loss: 0.8396 - acc: 0.6466 - val_loss: 0.8454 - val_acc: 0.6300\n",
      "Epoch 15/300\n",
      "2521/2521 [==============================] - 91s 36ms/step - loss: 0.8318 - acc: 0.6482 - val_loss: 0.8389 - val_acc: 0.6420\n",
      "Epoch 16/300\n",
      "2521/2521 [==============================] - 92s 36ms/step - loss: 0.8229 - acc: 0.6497 - val_loss: 0.8344 - val_acc: 0.6466\n",
      "Epoch 17/300\n",
      "2521/2521 [==============================] - 91s 36ms/step - loss: 0.8139 - acc: 0.6537 - val_loss: 0.8263 - val_acc: 0.6475\n",
      "Epoch 18/300\n",
      "2521/2521 [==============================] - 91s 36ms/step - loss: 0.8058 - acc: 0.6549 - val_loss: 0.8207 - val_acc: 0.6457\n",
      "Epoch 19/300\n",
      "2521/2521 [==============================] - 86s 34ms/step - loss: 0.7958 - acc: 0.6640 - val_loss: 0.8209 - val_acc: 0.6411\n",
      "Epoch 20/300\n",
      "2521/2521 [==============================] - 91s 36ms/step - loss: 0.7873 - acc: 0.6628 - val_loss: 0.8124 - val_acc: 0.6531\n",
      "Epoch 21/300\n",
      "2521/2521 [==============================] - 92s 36ms/step - loss: 0.7785 - acc: 0.6767 - val_loss: 0.8052 - val_acc: 0.6568\n",
      "Epoch 22/300\n",
      "2521/2521 [==============================] - 91s 36ms/step - loss: 0.7705 - acc: 0.6735 - val_loss: 0.8028 - val_acc: 0.6549\n",
      "Epoch 23/300\n",
      "2521/2521 [==============================] - 86s 34ms/step - loss: 0.7635 - acc: 0.6779 - val_loss: 0.8121 - val_acc: 0.6448\n",
      "Epoch 24/300\n",
      "2521/2521 [==============================] - 86s 34ms/step - loss: 0.7553 - acc: 0.6827 - val_loss: 0.8043 - val_acc: 0.6503\n",
      "Epoch 25/300\n",
      "2521/2521 [==============================] - 91s 36ms/step - loss: 0.7484 - acc: 0.6942 - val_loss: 0.7976 - val_acc: 0.6596\n",
      "Epoch 26/300\n",
      "2521/2521 [==============================] - 86s 34ms/step - loss: 0.7431 - acc: 0.6886 - val_loss: 0.7986 - val_acc: 0.6586\n",
      "Epoch 27/300\n",
      "2521/2521 [==============================] - 86s 34ms/step - loss: 0.7345 - acc: 0.6981 - val_loss: 0.8030 - val_acc: 0.6457\n",
      "Epoch 28/300\n",
      "2521/2521 [==============================] - 87s 34ms/step - loss: 0.7304 - acc: 0.6997 - val_loss: 0.7981 - val_acc: 0.6577\n",
      "Epoch 29/300\n",
      "2521/2521 [==============================] - 91s 36ms/step - loss: 0.7263 - acc: 0.6993 - val_loss: 0.7931 - val_acc: 0.6531\n",
      "Epoch 30/300\n",
      "2521/2521 [==============================] - 86s 34ms/step - loss: 0.7195 - acc: 0.7009 - val_loss: 0.8080 - val_acc: 0.6577\n",
      "Epoch 31/300\n",
      "2521/2521 [==============================] - 86s 34ms/step - loss: 0.7158 - acc: 0.6969 - val_loss: 0.8088 - val_acc: 0.6531\n",
      "Epoch 32/300\n",
      "2521/2521 [==============================] - 86s 34ms/step - loss: 0.7082 - acc: 0.6969 - val_loss: 0.7985 - val_acc: 0.6559\n",
      "Epoch 33/300\n",
      "2521/2521 [==============================] - 86s 34ms/step - loss: 0.7039 - acc: 0.7053 - val_loss: 0.7953 - val_acc: 0.6577\n",
      "Epoch 34/300\n",
      "2521/2521 [==============================] - 86s 34ms/step - loss: 0.6983 - acc: 0.7033 - val_loss: 0.7934 - val_acc: 0.6596\n",
      "Epoch 35/300\n",
      "2521/2521 [==============================] - 91s 36ms/step - loss: 0.6933 - acc: 0.7132 - val_loss: 0.7875 - val_acc: 0.6596\n",
      "Epoch 36/300\n",
      "2521/2521 [==============================] - 86s 34ms/step - loss: 0.6902 - acc: 0.7112 - val_loss: 0.7887 - val_acc: 0.6577\n",
      "Epoch 37/300\n",
      "2521/2521 [==============================] - 86s 34ms/step - loss: 0.6823 - acc: 0.7152 - val_loss: 0.7912 - val_acc: 0.6614\n",
      "Epoch 38/300\n",
      "2521/2521 [==============================] - 86s 34ms/step - loss: 0.6775 - acc: 0.7160 - val_loss: 0.8208 - val_acc: 0.6420\n",
      "Epoch 39/300\n",
      "2521/2521 [==============================] - 86s 34ms/step - loss: 0.6809 - acc: 0.7108 - val_loss: 0.7883 - val_acc: 0.6679\n",
      "Epoch 40/300\n",
      "2521/2521 [==============================] - 87s 34ms/step - loss: 0.6660 - acc: 0.7255 - val_loss: 0.7988 - val_acc: 0.6688\n",
      "Epoch 41/300\n",
      "2521/2521 [==============================] - 86s 34ms/step - loss: 0.6687 - acc: 0.7219 - val_loss: 0.7987 - val_acc: 0.6596\n",
      "Epoch 42/300\n",
      "2521/2521 [==============================] - 90s 36ms/step - loss: 0.6593 - acc: 0.7251 - val_loss: 0.7958 - val_acc: 0.6596\n",
      "Epoch 43/300\n",
      "2521/2521 [==============================] - 89s 35ms/step - loss: 0.6509 - acc: 0.7315 - val_loss: 0.8010 - val_acc: 0.6623\n",
      "Epoch 44/300\n",
      "2521/2521 [==============================] - 88s 35ms/step - loss: 0.6476 - acc: 0.7291 - val_loss: 0.9329 - val_acc: 0.5652\n",
      "Epoch 45/300\n",
      "2521/2521 [==============================] - 88s 35ms/step - loss: 0.6427 - acc: 0.7346 - val_loss: 0.8004 - val_acc: 0.6642\n",
      "Train on 2521 samples, validate on 1081 samples\n",
      "Epoch 1/300\n",
      "2521/2521 [==============================] - 320s 127ms/step - loss: 0.9802 - acc: 0.5974 - val_loss: 0.9561 - val_acc: 0.6004\n",
      "Epoch 2/300\n",
      "2521/2521 [==============================] - 318s 126ms/step - loss: 0.9478 - acc: 0.6010 - val_loss: 0.9425 - val_acc: 0.6004\n",
      "Epoch 3/300\n",
      "2521/2521 [==============================] - 307s 122ms/step - loss: 0.9357 - acc: 0.6010 - val_loss: 0.9310 - val_acc: 0.6004\n",
      "Epoch 4/300\n",
      "2521/2521 [==============================] - 305s 121ms/step - loss: 0.9250 - acc: 0.6010 - val_loss: 0.9216 - val_acc: 0.6004\n",
      "Epoch 5/300\n",
      "2521/2521 [==============================] - 303s 120ms/step - loss: 0.9153 - acc: 0.6010 - val_loss: 0.9131 - val_acc: 0.6004\n",
      "Epoch 6/300\n",
      "2521/2521 [==============================] - 303s 120ms/step - loss: 0.9070 - acc: 0.6010 - val_loss: 0.9044 - val_acc: 0.6013\n",
      "Epoch 7/300\n",
      "2521/2521 [==============================] - 303s 120ms/step - loss: 0.8985 - acc: 0.6081 - val_loss: 0.8976 - val_acc: 0.6022\n",
      "Epoch 8/300\n",
      "2521/2521 [==============================] - 303s 120ms/step - loss: 0.8901 - acc: 0.6077 - val_loss: 0.8889 - val_acc: 0.6161\n",
      "Epoch 9/300\n",
      "2521/2521 [==============================] - 302s 120ms/step - loss: 0.8827 - acc: 0.6208 - val_loss: 0.8821 - val_acc: 0.6161\n",
      "Epoch 10/300\n",
      "2521/2521 [==============================] - 302s 120ms/step - loss: 0.8757 - acc: 0.6240 - val_loss: 0.8758 - val_acc: 0.6179\n",
      "Epoch 11/300\n",
      "2521/2521 [==============================] - 302s 120ms/step - loss: 0.8680 - acc: 0.6228 - val_loss: 0.8675 - val_acc: 0.6179\n",
      "Epoch 12/300\n",
      "2521/2521 [==============================] - 302s 120ms/step - loss: 0.8606 - acc: 0.6244 - val_loss: 0.8616 - val_acc: 0.6189\n",
      "Epoch 13/300\n",
      "2521/2521 [==============================] - 302s 120ms/step - loss: 0.8522 - acc: 0.6315 - val_loss: 0.8555 - val_acc: 0.6207\n",
      "Epoch 14/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2521/2521 [==============================] - 301s 119ms/step - loss: 0.8443 - acc: 0.6374 - val_loss: 0.8475 - val_acc: 0.6272\n",
      "Epoch 15/300\n",
      "2521/2521 [==============================] - 301s 120ms/step - loss: 0.8366 - acc: 0.6414 - val_loss: 0.8412 - val_acc: 0.6383\n",
      "Epoch 16/300\n",
      "2521/2521 [==============================] - 302s 120ms/step - loss: 0.8275 - acc: 0.6438 - val_loss: 0.8378 - val_acc: 0.6420\n",
      "Epoch 17/300\n",
      "2521/2521 [==============================] - 304s 121ms/step - loss: 0.8184 - acc: 0.6541 - val_loss: 0.8295 - val_acc: 0.6420\n",
      "Epoch 18/300\n",
      "2521/2521 [==============================] - 302s 120ms/step - loss: 0.8105 - acc: 0.6553 - val_loss: 0.8249 - val_acc: 0.6392\n",
      "Epoch 19/300\n",
      "2521/2521 [==============================] - 298s 118ms/step - loss: 0.8004 - acc: 0.6601 - val_loss: 0.8263 - val_acc: 0.6355\n",
      "Epoch 20/300\n",
      "2521/2521 [==============================] - 303s 120ms/step - loss: 0.7923 - acc: 0.6557 - val_loss: 0.8196 - val_acc: 0.6512\n",
      "Epoch 21/300\n",
      "2521/2521 [==============================] - 302s 120ms/step - loss: 0.7841 - acc: 0.6708 - val_loss: 0.8124 - val_acc: 0.6531\n",
      "Epoch 22/300\n",
      "2521/2521 [==============================] - 304s 120ms/step - loss: 0.7771 - acc: 0.6720 - val_loss: 0.8108 - val_acc: 0.6466\n",
      "Epoch 23/300\n",
      "2521/2521 [==============================] - 297s 118ms/step - loss: 0.7707 - acc: 0.6692 - val_loss: 0.8332 - val_acc: 0.6485\n",
      "Epoch 24/300\n",
      "2521/2521 [==============================] - 298s 118ms/step - loss: 0.7633 - acc: 0.6767 - val_loss: 0.8151 - val_acc: 0.6494\n",
      "Epoch 25/300\n",
      "2521/2521 [==============================] - 303s 120ms/step - loss: 0.7574 - acc: 0.6767 - val_loss: 0.8130 - val_acc: 0.6485\n",
      "Epoch 26/300\n",
      "2521/2521 [==============================] - 305s 121ms/step - loss: 0.7528 - acc: 0.6811 - val_loss: 0.8137 - val_acc: 0.6494\n",
      "Epoch 27/300\n",
      "2521/2521 [==============================] - 305s 121ms/step - loss: 0.7425 - acc: 0.6882 - val_loss: 0.8211 - val_acc: 0.6337\n",
      "Epoch 28/300\n",
      "2521/2521 [==============================] - 312s 124ms/step - loss: 0.7405 - acc: 0.6894 - val_loss: 0.8120 - val_acc: 0.6494\n",
      "Epoch 29/300\n",
      "2521/2521 [==============================] - 320s 127ms/step - loss: 0.7359 - acc: 0.6894 - val_loss: 0.8096 - val_acc: 0.6559\n",
      "Epoch 30/300\n",
      "2521/2521 [==============================] - 303s 120ms/step - loss: 0.7288 - acc: 0.6910 - val_loss: 0.8277 - val_acc: 0.6457\n",
      "Epoch 31/300\n",
      "2521/2521 [==============================] - 297s 118ms/step - loss: 0.7248 - acc: 0.6894 - val_loss: 0.8346 - val_acc: 0.6318\n",
      "Epoch 32/300\n",
      "2521/2521 [==============================] - 303s 120ms/step - loss: 0.7183 - acc: 0.6969 - val_loss: 0.8168 - val_acc: 0.6494\n",
      "Epoch 33/300\n",
      "2521/2521 [==============================] - 297s 118ms/step - loss: 0.7131 - acc: 0.6946 - val_loss: 0.8133 - val_acc: 0.6531\n",
      "Epoch 34/300\n",
      "2521/2521 [==============================] - 297s 118ms/step - loss: 0.7081 - acc: 0.6977 - val_loss: 0.8160 - val_acc: 0.6540\n",
      "Epoch 35/300\n",
      "2521/2521 [==============================] - 303s 120ms/step - loss: 0.7036 - acc: 0.7065 - val_loss: 0.8085 - val_acc: 0.6531\n",
      "Epoch 36/300\n",
      "2521/2521 [==============================] - 298s 118ms/step - loss: 0.6994 - acc: 0.7045 - val_loss: 0.8128 - val_acc: 0.6531\n",
      "Epoch 37/300\n",
      "2521/2521 [==============================] - 297s 118ms/step - loss: 0.6916 - acc: 0.6985 - val_loss: 0.8134 - val_acc: 0.6559\n",
      "Epoch 38/300\n",
      "2521/2521 [==============================] - 298s 118ms/step - loss: 0.6854 - acc: 0.7120 - val_loss: 0.8671 - val_acc: 0.6281\n",
      "Epoch 39/300\n",
      "2521/2521 [==============================] - 298s 118ms/step - loss: 0.6907 - acc: 0.7073 - val_loss: 0.8129 - val_acc: 0.6512\n",
      "Epoch 40/300\n",
      "2521/2521 [==============================] - 298s 118ms/step - loss: 0.6756 - acc: 0.7160 - val_loss: 0.8561 - val_acc: 0.6512\n",
      "Epoch 41/300\n",
      "2521/2521 [==============================] - 298s 118ms/step - loss: 0.6787 - acc: 0.7108 - val_loss: 0.8375 - val_acc: 0.6420\n",
      "Epoch 42/300\n",
      "2521/2521 [==============================] - 298s 118ms/step - loss: 0.6678 - acc: 0.7207 - val_loss: 0.8318 - val_acc: 0.6577\n",
      "Epoch 43/300\n",
      "2521/2521 [==============================] - 298s 118ms/step - loss: 0.6578 - acc: 0.7307 - val_loss: 0.8343 - val_acc: 0.6586\n",
      "Epoch 44/300\n",
      "2521/2521 [==============================] - 297s 118ms/step - loss: 0.6581 - acc: 0.7251 - val_loss: 0.9558 - val_acc: 0.5495\n",
      "Epoch 45/300\n",
      "2521/2521 [==============================] - 297s 118ms/step - loss: 0.6514 - acc: 0.7243 - val_loss: 0.8486 - val_acc: 0.6494\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <th>9</th>\n",
       "      <th>15</th>\n",
       "      <th>-1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embeddings</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>glove 300d 42B common crawl</th>\n",
       "      <td>0.699107</td>\n",
       "      <td>0.683036</td>\n",
       "      <td>0.705357</td>\n",
       "      <td>0.683929</td>\n",
       "      <td>0.679464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    5        10         9        15        -1\n",
       "embeddings                                                                   \n",
       "glove 300d 42B common crawl  0.699107  0.683036  0.705357  0.683929  0.679464"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del embeddings\n",
    "del embedding_names\n",
    "\n",
    "semeval_14_rest_train = semeval_14(full_path(read_config('semeval_2014_rest_train')))\n",
    "semeval_14_rest_test = semeval_14(full_path(read_config('semeval_2014_rest_test')))\n",
    "train_data = [semeval_14_rest_train]\n",
    "test_data = [semeval_14_rest_test]\n",
    "# We also use the Glove 300d vectors trained on the common crawl and used in the following paper\n",
    "# http://www.anthology.aclweb.org/D/D16/D16-1021.pdf\n",
    "glove_300 = GloveCommonCrawl42()\n",
    "embedding = glove_300\n",
    "embedding_names = ['{}'.format(embedding)]\n",
    "pad_sizes = [5, 10, half_average_dataset(semeval_14_rest_train), 15, -1]\n",
    "\n",
    "index_column = 'embeddings'\n",
    "sem_rest_pad_size_res_file = os.path.join(pad_size_results_folder, 'Semeval Restaurant 14.csv')\n",
    "notebook_helper.create_pandas_file(sem_rest_pad_size_res_file, pad_sizes, index_column, \n",
    "                                   embedding_names, re_write=False)\n",
    "with tempfile.NamedTemporaryFile() as embedding_file:\n",
    "    embedding_path = embedding_file.name\n",
    "    pickle.dump(embedding, embedding_file)\n",
    "    pred_params = list(itertools.product([embedding_path], pad_sizes, reproducible, \n",
    "                                         train_data, test_data, \n",
    "                                         [sem_rest_pad_size_res_file], \n",
    "                                         [index_column], [whitespace]))\n",
    "    for pred_param in pred_params: \n",
    "        pad_size_prediction(*pred_param)\n",
    "sem_rest_pad_df_results = pd.read_csv(open(sem_rest_pad_size_res_file, 'r'))\n",
    "sem_rest_pad_df_results = sem_rest_pad_df_results.set_index('embeddings')\n",
    "sem_rest_pad_df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_14_lap_train = semeval_14(full_path(read_config('semeval_2014_lap_train')))\n",
    "semeval_14_lap_test = semeval_14(full_path(read_config('semeval_2014_lap_test')))\n",
    "train_data = [semeval_14_lap_train]\n",
    "test_data = [semeval_14_lap_test]\n",
    "\n",
    "pad_sizes = [5, half_average_dataset(semeval_14_lap_train), 15, -1]\n",
    "\n",
    "index_column = 'embeddings'\n",
    "sem_lap_pad_size_res_file = os.path.join(pad_size_results_folder, 'Semeval Laptop 14.csv')\n",
    "notebook_helper.create_pandas_file(sem_lap_pad_size_res_file, pad_sizes, index_column, \n",
    "                                   embedding_names, re_write=False)\n",
    "with tempfile.NamedTemporaryFile() as embedding_file:\n",
    "    embedding_path = embedding_file.name\n",
    "    pickle.dump(embedding, embedding_file)\n",
    "    pred_params = list(itertools.product([embedding_path], pad_sizes, reproducible, \n",
    "                                         train_data, test_data, \n",
    "                                         [sem_lap_pad_size_res_file], \n",
    "                                         [index_column], [whitespace]))\n",
    "    for pred_param in pred_params: \n",
    "        pad_size_prediction(*pred_param)\n",
    "sem_lap_pad_df_results = pd.read_csv(open(sem_lap_pad_size_res_file, 'r'))\n",
    "sem_lap_pad_df_results = sem_lap_pad_df_results.set_index('embeddings')\n",
    "sem_lap_pad_df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "This is an implementation of the LSTM model that is shown in [Tang et al. paper](https://aclanthology.info/papers/C16-1311/c16-1311).\n",
    "\n",
    "The LSTM is a single LSTM layer that outputs to a softmax function. The LSTM hidden layer dimension is the same as the embedding layer dimension. The optimiser is Stochastic Gradient Descent with a learning rate of 0.01.\n",
    "\n",
    "The number of epochs to run the model is unknown and Early Stopping is not mentioned in the paper. As we do not know the number of epochs used we are going to use Early Stopping with a patience of 10.\n",
    "\n",
    "This notebook is going to perform the following experiments:\n",
    "1. The affect of lower casing the words - This should cause some affect as all of the word embeddings only contain lower cased words.\n",
    "2. Tokeniser - the affect of using a simple Whitespace tokeniser and a data set specific tokeniser [ark tokenizer](https://www.cs.cmu.edu/~ark/TweetNLP/gimpel+etal.acl11.pdf) for Twitter data\n",
    "3. Different Validation set size affect - As we are using Early Stopping we require a validation set within the training data therefore we are going to see the affect of this on the Test set.\n",
    "\n",
    "\n",
    "All of the experiments above will be performed with the [Sentiment Specfic Word Embeddings (SSWE)](https://www.aclweb.org/anthology/P14-1146) which are 50 dimension word embeddings which we ctreated by jointly learning sentiment and semantics.\n",
    "\n",
    "After these experiments we will use the best settings to run over all the word embeddings used in the paper:\n",
    "1. SSWE 50 Dimensions In the paper these are called SSWE -u\n",
    "2. Twitter Glove 50 Dimensions\n",
    "3. Twitter Glove 100 Dimensions\n",
    "4. Twitter Glove 200 Dimensions\n",
    "\n",
    "All of the experiments above will be performed on [Dong et al. Twitter dataset](https://aclanthology.info/papers/P14-2009/p14-2009) as used in the original paper. The dataset we used can be found [here](https://github.com/bluemonk482/tdparse/tree/master/data/lidong), this was used as the original dataset had been pre-processed thus containg symbols such as: -LRB- and -RRB-\n",
    "\n",
    "The Twitter glove vectors can be found [here](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "### Affect of initialiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in here\n"
     ]
    }
   ],
   "source": [
    "tclstm_model = TCLSTM(ark_twokenize, sswe, lower=True)\n",
    "tclstm_model.fit(dong_train.data_dict(), dong_train.sentiment_data(), reproducible=True,\n",
    "                 validation_size=0.3, patience=10, epochs=300, verbose=0,\n",
    "                 org_initialisers=True)\n",
    "predictions_keras = tclstm_model.predict(dong_test.data_dict())\n",
    "score_keras = TCLSTM.score(dong_test.sentiment_data(), predictions_keras, accuracy_score)\n",
    "\n",
    "tclstm_model.fit(dong_train.data_dict(), dong_train.sentiment_data(), reproducible=True,\n",
    "               validation_size=0.3, patience=10, epochs=300, verbose=0,\n",
    "               org_initialisers=True)\n",
    "predictions_original = tclstm_model.predict(dong_test.data_dict())\n",
    "score_original = TCLSTM.score(dong_test.sentiment_data(), predictions_original, accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affect of lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_case_options = [True, False]\n",
    "lower_case_results = {}\n",
    "for lower_case_option in lower_case_options:\n",
    "    tclstm_model = TCLSTM(ark_twokenize, glove_200, lower=lower_case_option)\n",
    "    tclstm_model.fit(dong_train.data_dict(), dong_train.sentiment_data(), reproducible=True,\n",
    "                     validation_size=0.3, patience=10, epochs=300, verbose=0,\n",
    "                     org_initialisers=True)\n",
    "    predictions = tclstm_model.predict(dong_test.data_dict())\n",
    "    score = TCLSTM.score(dong_test.sentiment_data(), predictions, accuracy_score)\n",
    "    lower_case_results['Lower case option {}'.format(lower_case_option)] = score\n",
    "pd.DataFrame(lower_case_results, index=['LSTM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The affect of the tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenisers = [whitespace, ark_twokenize]\n",
    "tokeniser_results = {}\n",
    "for tokeniser in tokenisers:\n",
    "    tclstm_model = TCLSTM(tokeniser, glove_200, lower=True)\n",
    "    tclstm_model.fit(dong_train.data_dict(), dong_train.sentiment_data(), reproducible=True,\n",
    "                     validation_size=0.1, patience=10, epochs=300, verbose=0,\n",
    "                     org_initialisers=True)\n",
    "    predictions = tclstm_model.predict(dong_test.data_dict())\n",
    "    score = TCLSTM.score(dong_test.sentiment_data(), predictions, accuracy_score)\n",
    "    tokeniser_results['Tokeniser {}'.format(tokeniser.__name__)] = score\n",
    "pd.DataFrame(tokeniser_results, index=['LSTM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Validation set sizes\n",
    "\n",
    "Using 5 processors uses around ~22GB of RAM this is due to the size of the Glove Vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size 30 jobs 2\n",
      "start glove twitter 200d\n",
      "start glove twitter 200d\n",
      "end glove twitter 200d\n",
      "score 0.6791907514450867\n",
      "end glove twitter 200d\n",
      "score 0.6661849710982659\n",
      "done\n",
      "start glove twitter 200d\n",
      "start glove twitter 200d\n",
      "end glove twitter 200d\n",
      "score 0.6604046242774566\n",
      "end glove twitter 200d\n",
      "score 0.6647398843930635\n",
      "done\n",
      "start glove twitter 200d\n",
      "start glove twitter 200d\n"
     ]
    }
   ],
   "source": [
    "def validation_prediction(train_data, test_data, embedding_path, validation_size, reproducible):\n",
    "    embedding = None\n",
    "    with open(embedding_path, 'rb') as embedding_file:\n",
    "        embedding = pickle.load(embedding_file, encoding='utf-8')\n",
    "    print('start {}'.format(embedding))\n",
    "    pad_size = half_average_dataset(train_data)\n",
    "    tclstm_model = TCLSTM(ark_twokenize, embedding, lower=True, pad_size=pad_size)\n",
    "    tclstm_model.fit(train_data.data_dict(), train_data.sentiment_data(), reproducible=reproducible,\n",
    "                     validation_size=validation_size, patience=10, epochs=300, verbose=0,\n",
    "                     org_initialisers=True)\n",
    "    predictions = tclstm_model.predict(test_data.data_dict())\n",
    "    embedding_name = '{}'.format(embedding)\n",
    "    score = TCLSTM.score(test_data.sentiment_data(), predictions, accuracy_score)\n",
    "    print('end {}'.format(embedding))\n",
    "    print('score {}'.format(score))\n",
    "    return score\n",
    "\n",
    "def repeated_results(n_results, n_jobs, train_data, test_data, embedding, validation_size):\n",
    "    '''\n",
    "    n_results number of result samples to produce. n_jobs number of \n",
    "    processors to use the more processors the more memory required \n",
    "    10 = ~36GB Ram for Glove 50 memory very much depends on the word \n",
    "    vectors and Glove vectors are large. embedding the embedding to \n",
    "    use e.g. Glove 50. validation size, size of the validation \n",
    "    set e.g. 0.3\n",
    "    '''\n",
    "    batch_size = math.ceil(n_results / n_jobs)\n",
    "    train_data = [train_data]\n",
    "    test_data = [test_data]\n",
    "    reproducible = [False]\n",
    "    results = []\n",
    "    print('batch size {} jobs {}'.format(batch_size, n_jobs))\n",
    "    for i in range(batch_size):\n",
    "        num_results = n_jobs\n",
    "        left_to_process = n_results - len(results)\n",
    "        if n_jobs > left_to_process:\n",
    "            num_results = left_to_process\n",
    "        with tempfile.NamedTemporaryFile() as embedding_file:\n",
    "            embedding_path = embedding_file.name\n",
    "            pickle.dump(embedding, embedding_file)\n",
    "            val_pred_params = list(itertools.product(train_data, test_data, [embedding_path], \n",
    "                                                     [validation_size] * num_results, reproducible))\n",
    "            with Pool(n_jobs) as pool:\n",
    "                results.extend(pool.starmap(validation_prediction, val_pred_params))\n",
    "                print('done')\n",
    "    return results\n",
    "\n",
    "\n",
    "results_30_glove_200 = repeated_results(60, 2, dong_train, dong_test, glove_200, 0.3)\n",
    "results_30_sswe = repeated_results(60, 3, dong_train, dong_test, sswe, 0.3)\n",
    "#results_30_glove_100 = repeated_results(30, 3, dong_train, dong_test, glove_100, 0.3)\n",
    "#results_30_glove_50 = repeated_results(30, 3, dong_train, dong_test, glove_50, 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_30_sswe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-49913bf11e71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_30_sswe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results_30_sswe' is not defined"
     ]
    }
   ],
   "source": [
    "results_30_sswe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "repeated_results_folder = os.path.join(result_folder, 'TCLstm Repeated Results')\n",
    "os.makedirs(repeated_results_folder, exist_ok=True)\n",
    "repeated_results_file = os.path.join(repeated_results_folder, 'Glove 200.json')\n",
    "with open(repeated_results_file, 'w') as fp:\n",
    "    json.dump(results_30_glove_200, fp)\n",
    "repeated_results_file = os.path.join(repeated_results_folder, 'sswe.json')\n",
    "with open(repeated_results_file, 'w') as fp:\n",
    "    json.dump(results_30_sswe, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0034035908>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f002d00e7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(results_30_sswe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tclstm_model = TCLSTM(ark_twokenize, sswe, lower=True)\n",
    "tclstm_model.fit(dong_train.data_dict(), dong_train.sentiment_data(), reproducible=True,\n",
    "                 validation_size=0.3, patience=10, epochs=300, verbose=0,\n",
    "                 org_initialisers=True, lstm_dimension=100)\n",
    "predictions_keras = tclstm_model.predict(dong_test.data_dict())\n",
    "score_keras = TCLSTM.score(dong_test.sentiment_data(), predictions_keras, accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"476pt\" viewBox=\"0.00 0.00 1401.00 476.00\" width=\"1401pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 472)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-472 1397,-472 1397,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139638748900432 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139638748900432</title>\n",
       "<polygon fill=\"none\" points=\"31.5,-421 31.5,-467 326.5,-467 326.5,-421 31.5,-421\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113.5\" y=\"-440.3\">left_text_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"195.5,-421 195.5,-467 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223\" y=\"-451.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"195.5,-444 250.5,-444 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223\" y=\"-428.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"250.5,-421 250.5,-467 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"288.5\" y=\"-451.8\">(None, 33)</text>\n",
       "<polyline fill=\"none\" points=\"250.5,-444 326.5,-444 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"288.5\" y=\"-428.8\">(None, 33)</text>\n",
       "</g>\n",
       "<!-- 139638751686384 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139638751686384</title>\n",
       "<polygon fill=\"none\" points=\"0,-337 0,-383 358,-383 358,-337 0,-337\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103\" y=\"-356.3\">left_embedding_layer: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"206,-337 206,-383 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233.5\" y=\"-367.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"206,-360 261,-360 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233.5\" y=\"-344.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"261,-337 261,-383 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309.5\" y=\"-367.8\">(None, 33)</text>\n",
       "<polyline fill=\"none\" points=\"261,-360 358,-360 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309.5\" y=\"-344.8\">(None, 33, 50)</text>\n",
       "</g>\n",
       "<!-- 139638748900432&#45;&gt;139638751686384 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139638748900432-&gt;139638751686384</title>\n",
       "<path d=\"M179,-420.593C179,-412.118 179,-402.297 179,-393.104\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"182.5,-393.096 179,-383.096 175.5,-393.096 182.5,-393.096\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139638634319320 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139638634319320</title>\n",
       "<polygon fill=\"none\" points=\"742,-421 742,-467 1044,-467 1044,-421 742,-421\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"827.5\" y=\"-440.3\">right_text_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"913,-421 913,-467 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"940.5\" y=\"-451.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"913,-444 968,-444 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"940.5\" y=\"-428.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"968,-421 968,-467 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1006\" y=\"-451.8\">(None, 37)</text>\n",
       "<polyline fill=\"none\" points=\"968,-444 1044,-444 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1006\" y=\"-428.8\">(None, 37)</text>\n",
       "</g>\n",
       "<!-- 139638634317024 -->\n",
       "<g class=\"node\" id=\"node5\"><title>139638634317024</title>\n",
       "<polygon fill=\"none\" points=\"710,-337 710,-383 1076,-383 1076,-337 710,-337\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"817\" y=\"-356.3\">right_embedding_layer: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"924,-337 924,-383 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"951.5\" y=\"-367.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"924,-360 979,-360 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"951.5\" y=\"-344.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"979,-337 979,-383 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1027.5\" y=\"-367.8\">(None, 37)</text>\n",
       "<polyline fill=\"none\" points=\"979,-360 1076,-360 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1027.5\" y=\"-344.8\">(None, 37, 50)</text>\n",
       "</g>\n",
       "<!-- 139638634319320&#45;&gt;139638634317024 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139638634319320-&gt;139638634317024</title>\n",
       "<path d=\"M893,-420.593C893,-412.118 893,-402.297 893,-393.104\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"896.5,-393.096 893,-383.096 889.5,-393.096 896.5,-393.096\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139638698690992 -->\n",
       "<g class=\"node\" id=\"node7\"><title>139638698690992</title>\n",
       "<polygon fill=\"none\" points=\"286,-253 286,-299 708,-299 708,-253 286,-253\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-272.3\">left_text_target: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"459,-253 459,-299 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"486.5\" y=\"-283.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"459,-276 514,-276 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"486.5\" y=\"-260.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"514,-253 514,-299 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"611\" y=\"-283.8\">[(None, 33, 50), (None, 33, 50)]</text>\n",
       "<polyline fill=\"none\" points=\"514,-276 708,-276 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"611\" y=\"-260.8\">(None, 33, 100)</text>\n",
       "</g>\n",
       "<!-- 139638751686384&#45;&gt;139638698690992 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139638751686384-&gt;139638698690992</title>\n",
       "<path d=\"M264.636,-336.918C306.965,-326.003 358.178,-312.797 401.741,-301.564\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"402.677,-304.937 411.487,-299.051 400.929,-298.159 402.677,-304.937\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139638698692056 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139638698692056</title>\n",
       "<polygon fill=\"none\" points=\"376,-337 376,-383 668,-383 668,-337 376,-337\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"446\" y=\"-356.3\">left_target: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"516,-337 516,-383 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"543.5\" y=\"-367.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"516,-360 571,-360 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"543.5\" y=\"-344.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"571,-337 571,-383 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"619.5\" y=\"-367.8\">(None, 33, 50)</text>\n",
       "<polyline fill=\"none\" points=\"571,-360 668,-360 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"619.5\" y=\"-344.8\">(None, 33, 50)</text>\n",
       "</g>\n",
       "<!-- 139638698692056&#45;&gt;139638698690992 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>139638698692056-&gt;139638698690992</title>\n",
       "<path d=\"M515.169,-336.593C512.557,-328.027 509.526,-318.086 506.697,-308.808\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"510.001,-307.64 503.737,-299.096 503.305,-309.682 510.001,-307.64\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139638869191872 -->\n",
       "<g class=\"node\" id=\"node8\"><title>139638869191872</title>\n",
       "<polygon fill=\"none\" points=\"726,-253 726,-299 1156,-299 1156,-253 726,-253\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"816.5\" y=\"-272.3\">right_text_target: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"907,-253 907,-299 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"934.5\" y=\"-283.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"907,-276 962,-276 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"934.5\" y=\"-260.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"962,-253 962,-299 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1059\" y=\"-283.8\">[(None, 37, 50), (None, 37, 50)]</text>\n",
       "<polyline fill=\"none\" points=\"962,-276 1156,-276 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1059\" y=\"-260.8\">(None, 37, 100)</text>\n",
       "</g>\n",
       "<!-- 139638634317024&#45;&gt;139638869191872 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>139638634317024-&gt;139638869191872</title>\n",
       "<path d=\"M906.116,-336.593C911.291,-327.753 917.322,-317.45 922.9,-307.921\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"926.035,-309.494 928.066,-299.096 919.993,-305.958 926.035,-309.494\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139638715380176 -->\n",
       "<g class=\"node\" id=\"node6\"><title>139638715380176</title>\n",
       "<polygon fill=\"none\" points=\"1094.5,-337 1094.5,-383 1393.5,-383 1393.5,-337 1094.5,-337\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1168\" y=\"-356.3\">right_target: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"1241.5,-337 1241.5,-383 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1269\" y=\"-367.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1241.5,-360 1296.5,-360 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1269\" y=\"-344.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1296.5,-337 1296.5,-383 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1345\" y=\"-367.8\">(None, 37, 50)</text>\n",
       "<polyline fill=\"none\" points=\"1296.5,-360 1393.5,-360 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1345\" y=\"-344.8\">(None, 37, 50)</text>\n",
       "</g>\n",
       "<!-- 139638715380176&#45;&gt;139638869191872 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>139638715380176-&gt;139638869191872</title>\n",
       "<path d=\"M1162.4,-336.918C1122.24,-326.049 1073.68,-312.908 1032.29,-301.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1033.05,-298.285 1022.48,-299.051 1031.22,-305.041 1033.05,-298.285\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139638751684144 -->\n",
       "<g class=\"node\" id=\"node9\"><title>139638751684144</title>\n",
       "<polygon fill=\"none\" points=\"438.5,-169 438.5,-215 707.5,-215 707.5,-169 438.5,-169\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"493.5\" y=\"-188.3\">left_lstm: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"548.5,-169 548.5,-215 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"576\" y=\"-199.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"548.5,-192 603.5,-192 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"576\" y=\"-176.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"603.5,-169 603.5,-215 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"655.5\" y=\"-199.8\">(None, 33, 100)</text>\n",
       "<polyline fill=\"none\" points=\"603.5,-192 707.5,-192 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"655.5\" y=\"-176.8\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 139638698690992&#45;&gt;139638751684144 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>139638698690992-&gt;139638751684144</title>\n",
       "<path d=\"M517.767,-252.593C526.383,-243.297 536.498,-232.384 545.701,-222.454\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"548.29,-224.809 552.521,-215.096 543.156,-220.051 548.29,-224.809\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139638648357272 -->\n",
       "<g class=\"node\" id=\"node10\"><title>139638648357272</title>\n",
       "<polygon fill=\"none\" points=\"764,-169 764,-215 1040,-215 1040,-169 764,-169\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"822.5\" y=\"-188.3\">right_lstm: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"881,-169 881,-215 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"908.5\" y=\"-199.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"881,-192 936,-192 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"908.5\" y=\"-176.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"936,-169 936,-215 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"988\" y=\"-199.8\">(None, 37, 100)</text>\n",
       "<polyline fill=\"none\" points=\"936,-192 1040,-192 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"988\" y=\"-176.8\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 139638869191872&#45;&gt;139638648357272 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>139638869191872-&gt;139638648357272</title>\n",
       "<path d=\"M930.343,-252.593C926.182,-243.844 921.339,-233.662 916.847,-224.216\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"919.965,-222.623 912.509,-215.096 913.643,-225.63 919.965,-222.623\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139638785457792 -->\n",
       "<g class=\"node\" id=\"node11\"><title>139638785457792</title>\n",
       "<polygon fill=\"none\" points=\"520.5,-85 520.5,-131 953.5,-131 953.5,-85 520.5,-85\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"627\" y=\"-104.3\">left_right_lstm_merge: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"733.5,-85 733.5,-131 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"761\" y=\"-115.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"733.5,-108 788.5,-108 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"761\" y=\"-92.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"788.5,-85 788.5,-131 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"871\" y=\"-115.8\">[(None, 100), (None, 100)]</text>\n",
       "<polyline fill=\"none\" points=\"788.5,-108 953.5,-108 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"871\" y=\"-92.8\">(None, 200)</text>\n",
       "</g>\n",
       "<!-- 139638751684144&#45;&gt;139638785457792 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>139638751684144-&gt;139638785457792</title>\n",
       "<path d=\"M617.381,-168.81C637.883,-158.559 662.391,-146.305 683.869,-135.565\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"685.489,-138.669 692.868,-131.066 682.359,-132.408 685.489,-138.669\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139638648357272&#45;&gt;139638785457792 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>139638648357272-&gt;139638785457792</title>\n",
       "<path d=\"M857.349,-168.81C836.722,-158.559 812.064,-146.305 790.455,-135.565\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"791.914,-132.382 781.401,-131.066 788.798,-138.651 791.914,-132.382\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139638678411976 -->\n",
       "<g class=\"node\" id=\"node12\"><title>139638678411976</title>\n",
       "<polygon fill=\"none\" points=\"622,-1 622,-47 852,-47 852,-1 622,-1\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"668\" y=\"-20.3\">output: Dense</text>\n",
       "<polyline fill=\"none\" points=\"714,-1 714,-47 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"741.5\" y=\"-31.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"714,-24 769,-24 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"741.5\" y=\"-8.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"769,-1 769,-47 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"810.5\" y=\"-31.8\">(None, 200)</text>\n",
       "<polyline fill=\"none\" points=\"769,-24 852,-24 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"810.5\" y=\"-8.8\">(None, 3)</text>\n",
       "</g>\n",
       "<!-- 139638785457792&#45;&gt;139638678411976 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>139638785457792-&gt;139638678411976</title>\n",
       "<path d=\"M737,-84.5931C737,-76.1177 737,-66.2974 737,-57.104\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"740.5,-57.0958 737,-47.0959 733.5,-57.0959 740.5,-57.0958\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tclstm_model.visulaise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66040462427745661"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8c5d9f14a1e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_sizes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0membedding_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_df\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(np.zeros((len(embeddings), len(validation_sizes))))\n",
    "results_df.index = ['{}'.format(embedding) for  embedding in embeddings]\n",
    "results_df.columns = ['{}'.format(int(val_size * 100)) for val_size in validation_sizes]\n",
    "for result in results:\n",
    "    embedding_name, val_size, score = result\n",
    "    results_df[val_size][embedding_name] = score\n",
    "round(results_df * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results above the validation set matters however we can see 20% validation appears to be the worse. I should imagine this is due to the 30 and 15% sets containing data that is more representive of the test data therefore allowing the model to find features/weights that can better sperate the classes within the test data.\n",
    "\n",
    "However it could just be due to the random seed we used and that the validation split that was found is just not very reprenative of the test data. We think this is the case as it is unusall that a smaller sample size 0.1 is better representative of the test data than a large sample size 0.2. Therefore below we use the Glove 50 dimension vectors and re-run the experiment using a different random validation split each time to see the affect over a larger sample size to see what the affect of validation size is with a larger sample size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start sswe\n",
      "start sswe\n",
      "start sswe\n",
      "start glove twitter 50d\n",
      "start glove twitter 50d\n",
      "end glove twitter 50d\n",
      "start glove twitter 50d\n",
      "end sswe\n",
      "start glove twitter 100d\n",
      "end glove twitter 50d\n",
      "end sswe\n",
      "start glove twitter 100d\n",
      "start glove twitter 100d\n",
      "end sswe\n",
      "start glove twitter 200d\n",
      "end glove twitter 50d\n",
      "start glove twitter 200d\n",
      "end glove twitter 100d\n",
      "start glove twitter 200d\n",
      "end glove twitter 100d\n",
      "end glove twitter 100d\n"
     ]
    }
   ],
   "source": [
    "def validation_prediction(train_data, test_data, embedding, validation_size, reproducible):\n",
    "    print('start {}'.format(embedding))\n",
    "    pad_size = half_average_dataset(train_data)\n",
    "    tclstm_model = TCLSTM(ark_twokenize, embedding, lower=True, pad_size=pad_size)\n",
    "    tclstm_model.fit(train_data.data_dict(), train_data.sentiment_data(), reproducible=reproducible,\n",
    "                     validation_size=validation_size, patience=10, epochs=300, verbose=0,\n",
    "                     org_initialisers=True)\n",
    "    predictions = tclstm_model.predict(test_data.data_dict())\n",
    "    embedding_name = '{}'.format(embedding)\n",
    "    score = TCLSTM.score(test_data.sentiment_data(), predictions, accuracy_score)\n",
    "    print('end {}'.format(embedding))\n",
    "    return score\n",
    "def val_pred_wrapper(train_data, test_data, embedding, validation_size, reproducible):\n",
    "    score = validation_prediction(embedding, validation_size, reproducible)\n",
    "    return ('{}'.format(embedding), \n",
    "            'Validation Size {}%'.format(int(validation_size * 100)),\n",
    "            score)\n",
    "\n",
    "train_data = [dong_train]\n",
    "test_data = [dong_test]\n",
    "embeddings = [sswe, glove_50, glove_100, glove_200]\n",
    "validation_sizes = [0.3, 0.2, 0.1]\n",
    "reproducible = [True]\n",
    "val_pred_params = list(itertools.product(train_data, test_data, embeddings, validation_sizes, reproducible))\n",
    "results = None\n",
    "time_taken = time.time()\n",
    "#print(val_pred_wrapper(*val_pred_params[0]))\n",
    "with Pool(10) as pool:\n",
    "    results = pool.starmap(val_pred_wrapper, val_pred_params)\n",
    "time_taken = time.time() - time_taken\n",
    "time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_20 = repeated_results(100, 3, glove_200, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_10 = repeated_results(100, 3, glove_200, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_30' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f111b3c617ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results_30' is not defined"
     ]
    }
   ],
   "source": [
    "results_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_repeated_df = pd.DataFrame({'Validation 10%' : results_10,\n",
    "                                    'Validation 20%' : results_20,\n",
    "                                    'Validation 30%' : results_30})\n",
    "results_repeated_df = round(results_repeated_df * 100, 2)\n",
    "ax = sns.boxplot(data=results_repeated_df)\n",
    "ax = sns.swarmplot(data=results_repeated_df, color=\".25\")\n",
    "ax.set_ylabel('Test Accuracy (%)')\n",
    "ax.set_title('Comparison of Validation Set Sizes')\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_repeated_summary = results_repeated_df.describe().T.round(2)\n",
    "results_repeated_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the different between the two validation sets is slight. The 30% has a higher minimum and maximum value as well as it deviate less suggesting that on average this set better represents the test data. Even though the training set of the LSTM will be 10% lower it is worth having a better validation set for training the LSTM algorthim."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
