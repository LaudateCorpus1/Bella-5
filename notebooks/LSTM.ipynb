{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Models\n",
    "from tdparse.models.tdlstm import TLSTM\n",
    "# Tokenisers\n",
    "from tdparse.tokenisers import whitespace, ark_twokenize\n",
    "# Word Vectors\n",
    "from tdparse.word_vectors import PreTrained, GloveTwitterVectors\n",
    "# Get the data\n",
    "from tdparse.helper import read_config, full_path\n",
    "from tdparse.parsers import dong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "dong_train = dong(full_path(read_config('dong_twit_train_data')))\n",
    "dong_test = dong(full_path(read_config('dong_twit_test_data')))\n",
    "# Load the word vectors\n",
    "sswe_path = full_path(read_config('sswe_files')['vo_zhang'])\n",
    "sswe = PreTrained(sswe_path, name='sswe')\n",
    "#glove_50 = GloveTwitterVectors(50)\n",
    "#glove_100 = GloveTwitterVectors(100)\n",
    "#glove_200 = GloveTwitterVectors(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "This is an implementation of the LSTM model that is shown in [Tang et al. paper](https://aclanthology.info/papers/C16-1311/c16-1311).\n",
    "\n",
    "The LSTM is a single LSTM layer that outputs to a softmax function. The LSTM hidden layer dimension is the same as the embedding layer dimension. The optimiser is Stochastic Gradient Descent with a learning rate of 0.01.\n",
    "\n",
    "The number of epochs to run the model is unknown and Early Stopping is not mentioned in the paper.\n",
    "\n",
    "However to estimate the unknown number of epochs we are using Early Stopping and setting different patience values. We are experimenting between the values 1-5.\n",
    "\n",
    "**Variables that can be changed through the contructor**\n",
    "1. Tokeniser used\n",
    "2. Pre-Trained Embedding e.g. Glove or SSWE (Sentiment Specific Word Embedding)\n",
    "3. Pad Size of the sentences\n",
    "4. To lower case the words. This should always be used as all the pre-trained word embeddings are trained on lower cased words\n",
    "5. Dimension of the LSTM hidden layer\n",
    "6. optimiser to use. To change this provide a valid Keras [optimiser](https://keras.io/optimizers/)\n",
    "7. Early stopping patience value\n",
    "\n",
    "Below we perform 5 fold cross validation on the five different LSTM models with different patience values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_1 = TLSTM(whitespace, sswe, patience=1)\n",
    "lstm_model_2 = TLSTM(whitespace, sswe, patience=2)\n",
    "lstm_model_3 = TLSTM(whitespace, sswe, patience=3)\n",
    "lstm_model_4 = TLSTM(whitespace, sswe, patience=4)\n",
    "lstm_model_5 = TLSTM(whitespace, sswe, patience=5)\n",
    "all_models = {'patience 1' : lstm_model_1, 'patience 2' : lstm_model_2,\n",
    "              'patience 3' : lstm_model_3, 'patience 4' : lstm_model_4,\n",
    "              'patience 5' : lstm_model_5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3997 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "3997/3997 [==============================] - 10s 2ms/step - loss: 1.0550 - acc: 0.4954 - val_loss: 1.0475 - val_acc: 0.4960\n",
      "Epoch 2/100\n",
      "3997/3997 [==============================] - 10s 2ms/step - loss: 1.0374 - acc: 0.4986 - val_loss: 1.0334 - val_acc: 0.4980\n",
      "Epoch 3/100\n",
      "3997/3997 [==============================] - 10s 2ms/step - loss: 1.0255 - acc: 0.4996 - val_loss: 1.0207 - val_acc: 0.4950\n",
      "Epoch 4/100\n",
      "3997/3997 [==============================] - 10s 2ms/step - loss: 1.0145 - acc: 0.5031 - val_loss: 1.0098 - val_acc: 0.4980\n",
      "Epoch 5/100\n",
      "3997/3997 [==============================] - 10s 3ms/step - loss: 1.0046 - acc: 0.5074 - val_loss: 1.0014 - val_acc: 0.4970\n",
      "Epoch 6/100\n",
      "3997/3997 [==============================] - 9s 2ms/step - loss: 0.9955 - acc: 0.5081 - val_loss: 0.9948 - val_acc: 0.5010\n",
      "Epoch 7/100\n",
      "3997/3997 [==============================] - 9s 2ms/step - loss: 0.9857 - acc: 0.5161 - val_loss: 0.9939 - val_acc: 0.5010\n",
      "Epoch 8/100\n",
      "3997/3997 [==============================] - 9s 2ms/step - loss: 0.9788 - acc: 0.5211 - val_loss: 0.9750 - val_acc: 0.5180\n",
      "Epoch 9/100\n",
      "3997/3997 [==============================] - 9s 2ms/step - loss: 0.9684 - acc: 0.5286 - val_loss: 0.9632 - val_acc: 0.5360\n",
      "Epoch 10/100\n",
      "3997/3997 [==============================] - 8s 2ms/step - loss: 0.9594 - acc: 0.5349 - val_loss: 0.9592 - val_acc: 0.5330\n",
      "Epoch 11/100\n",
      "3997/3997 [==============================] - 9s 2ms/step - loss: 0.9548 - acc: 0.5372 - val_loss: 0.9502 - val_acc: 0.5330\n",
      "Epoch 12/100\n",
      "3997/3997 [==============================] - 9s 2ms/step - loss: 0.9473 - acc: 0.5402 - val_loss: 0.9638 - val_acc: 0.5260\n",
      "Train on 3998 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "3998/3998 [==============================] - 10s 2ms/step - loss: 1.0321 - acc: 0.4962 - val_loss: 1.0241 - val_acc: 0.4990\n",
      "Epoch 2/100\n",
      "3998/3998 [==============================] - 9s 2ms/step - loss: 1.0154 - acc: 0.5043 - val_loss: 1.0089 - val_acc: 0.4990\n",
      "Epoch 3/100\n",
      "3998/3998 [==============================] - 8s 2ms/step - loss: 1.0043 - acc: 0.5073 - val_loss: 0.9968 - val_acc: 0.5030\n",
      "Epoch 4/100\n",
      "3998/3998 [==============================] - 9s 2ms/step - loss: 0.9951 - acc: 0.5083 - val_loss: 0.9843 - val_acc: 0.5060\n",
      "Epoch 5/100\n",
      "3998/3998 [==============================] - 9s 2ms/step - loss: 0.9871 - acc: 0.5120 - val_loss: 0.9764 - val_acc: 0.5170\n",
      "Epoch 6/100\n",
      "3998/3998 [==============================] - 9s 2ms/step - loss: 0.9810 - acc: 0.5155 - val_loss: 0.9675 - val_acc: 0.5180\n",
      "Epoch 7/100\n",
      "3998/3998 [==============================] - 9s 2ms/step - loss: 0.9740 - acc: 0.5220 - val_loss: 0.9660 - val_acc: 0.5240\n",
      "Epoch 8/100\n",
      "3998/3998 [==============================] - 9s 2ms/step - loss: 0.9682 - acc: 0.5258 - val_loss: 0.9594 - val_acc: 0.5300\n",
      "Epoch 9/100\n",
      "3998/3998 [==============================] - 9s 2ms/step - loss: 0.9645 - acc: 0.5335 - val_loss: 0.9551 - val_acc: 0.5340\n",
      "Epoch 10/100\n",
      "1248/3998 [========>.....................] - ETA: 6s - loss: 0.9478 - acc: 0.5489"
     ]
    }
   ],
   "source": [
    "model_results = {}\n",
    "for model_name, model in all_models.items():\n",
    "    train_data = dong_train.data_dict()\n",
    "    train_y = dong_train.sentiment_data()\n",
    "    predictions, scores = TLSTM.cross_val(train_data, train_y, \n",
    "                                          model, scorer=accuracy_score)\n",
    "    model_results[model_name] = (predictions, scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of how to fit, predict, and score a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4998 samples, validate on 1250 samples\n",
      "Epoch 1/100\n",
      "4998/4998 [==============================] - 14s 3ms/step - loss: 0.9867 - acc: 0.5174 - val_loss: 1.0210 - val_acc: 0.4840\n",
      "Epoch 2/100\n",
      "4998/4998 [==============================] - 14s 3ms/step - loss: 0.9230 - acc: 0.5494 - val_loss: 0.9598 - val_acc: 0.5432\n",
      "Epoch 3/100\n",
      "4998/4998 [==============================] - 14s 3ms/step - loss: 0.8819 - acc: 0.5834 - val_loss: 1.0155 - val_acc: 0.5432\n",
      "Epoch 4/100\n",
      "4998/4998 [==============================] - 15s 3ms/step - loss: 0.8421 - acc: 0.6078 - val_loss: 0.9130 - val_acc: 0.5624\n",
      "Epoch 5/100\n",
      "4998/4998 [==============================] - 15s 3ms/step - loss: 0.8076 - acc: 0.6301 - val_loss: 0.9280 - val_acc: 0.5736\n",
      "Epoch 6/100\n",
      "4998/4998 [==============================] - 15s 3ms/step - loss: 0.7720 - acc: 0.6519 - val_loss: 0.9381 - val_acc: 0.5680\n",
      "Epoch 7/100\n",
      "4998/4998 [==============================] - 14s 3ms/step - loss: 0.7340 - acc: 0.6753 - val_loss: 0.9080 - val_acc: 0.5960\n",
      "Epoch 8/100\n",
      "4998/4998 [==============================] - 14s 3ms/step - loss: 0.6925 - acc: 0.7009 - val_loss: 1.0450 - val_acc: 0.5176\n",
      "Epoch 9/100\n",
      "4998/4998 [==============================] - 14s 3ms/step - loss: 0.6604 - acc: 0.7135 - val_loss: 1.0326 - val_acc: 0.5312\n",
      "Epoch 10/100\n",
      "4998/4998 [==============================] - 14s 3ms/step - loss: 0.6165 - acc: 0.7379 - val_loss: 1.0313 - val_acc: 0.5928\n",
      "Epoch 11/100\n",
      "4998/4998 [==============================] - 14s 3ms/step - loss: 0.5746 - acc: 0.7627 - val_loss: 1.0969 - val_acc: 0.5232\n",
      "Epoch 12/100\n",
      "4998/4998 [==============================] - 14s 3ms/step - loss: 0.5425 - acc: 0.7771 - val_loss: 1.0230 - val_acc: 0.5768\n"
     ]
    }
   ],
   "source": [
    "lstm_model = TLSTM(whitespace, sswe)\n",
    "lstm_model.fit(dong_train.data_dict(), dong_train.sentiment_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55780346820809246"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_values = np.argmax(lstm_model.predict(dong_test.data()), axis=1)\n",
    "test_res = np.argmax(keras.utils.to_categorical(dong_test.sentiment_data(), num_classes=3), axis=1)\n",
    "accuracy_score(test_res, predicted_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
