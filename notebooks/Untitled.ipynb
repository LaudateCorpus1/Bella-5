{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASSIVE PROBLEM NOW NEED TO SAVE THE TOKENISER TO MAKE THE CODE FULLY REPRODUCIBLE.\n",
    "\n",
    "## ALSO I HAVE CHANGED THE TRAINING AND CODE SO THAT NEEDS CHANGING BACK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, str(Path('..').resolve()))\n",
    "\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Tuple, Callable, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from bella.error_analysis import same_one_sentiment, same_multi_sentiment,\\\n",
    "                                 similar_sentiment, different_sentiment, unknown_targets\n",
    "from bella.evaluation import datasets_df, plot_acc_f1, summary_errors\n",
    "from bella.parsers import semeval_14, semeval_15_16\n",
    "from bella.preprocessing import tokeniser\n",
    "from bella.tokenisers import stanford\n",
    "from bella.word_embeddings import GloveCommonEmbedding\n",
    "from bella.dataloaders import TargetSequence, LeftRightTargetSequence, LeftRightAugmentSequence, TargetAugmentation\n",
    "from bella import keras_models\n",
    "from bella.data_types import TargetCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_word_list(tokeniser, *datasets):\n",
    "    words = []\n",
    "    for dataset in datasets:\n",
    "        words.extend(dataset.word_list(tokeniser))\n",
    "    return list(set(words))\n",
    "\n",
    "# Load the data\n",
    "## Restaurants\n",
    "rest_sem_dir = Path('..', '..', 'aspect datasets', 'semeval_2014')\n",
    "rest_train = semeval_14(Path(rest_sem_dir, 'restaurants_train.xml'))\n",
    "rest_test = semeval_14(Path(rest_sem_dir, 'restaurants_test.xml'))\n",
    "\n",
    "rest_train_fp, rest_dev_fp = rest_train.to_json_file(['restaurants train', 'restaurants dev'], \n",
    "                                                     0.2, cache=False, random_state=42)\n",
    "rest_test_fp = rest_test.to_json_file('restaurants test', cache=False)\n",
    "## Laptop\n",
    "laptop_sem_dir = Path('..', '..', 'aspect datasets', 'semeval_2014')\n",
    "laptop_train = semeval_14(Path(rest_sem_dir, 'laptop_train.xml'))\n",
    "laptop_test = semeval_14(Path(rest_sem_dir, 'laptop_test.xml'))\n",
    "\n",
    "laptop_train_fp, laptop_dev_fp = laptop_train.to_json_file(['laptop train', 'laptop dev'], \n",
    "                                                     0.2, cache=False, random_state=42)\n",
    "laptop_test_fp = laptop_test.to_json_file('laptop test', cache=False)\n",
    "\n",
    "# Load the embeddings\n",
    "## Need to create a tokeniser that maps words to embedding id first\n",
    "rest_tok = tokeniser(rest_train_fp, rest_dev_fp, rest_test_fp, \n",
    "                     tokeniser_function=stanford, \n",
    "                     lower=True, filters='', oov_token='<UNK>')\n",
    "glove_rest = GloveCommonEmbedding(840, rest_tok.word_index)\n",
    "\n",
    "laptop_tok = tokeniser(laptop_train_fp, laptop_dev_fp, laptop_test_fp, \n",
    "                       tokeniser_function=stanford, \n",
    "                       lower=True, filters='', oov_token='<UNK>')\n",
    "glove_laptop = GloveCommonEmbedding(840, laptop_tok.word_index)\n",
    "\n",
    "rest_fps = [rest_train_fp, rest_dev_fp, rest_test_fp]\n",
    "laptop_fps = [laptop_train_fp, laptop_dev_fp, laptop_test_fp]\n",
    "domain_data = {'laptop': (laptop_fps, laptop_tok, glove_laptop), \n",
    "               'restaurant': (rest_fps, rest_tok, glove_rest)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 1,\n",
       " 'the': 2,\n",
       " ',': 3,\n",
       " '.': 4,\n",
       " 'and': 5,\n",
       " 'a': 6,\n",
       " 'is': 7,\n",
       " 'to': 8,\n",
       " 'was': 9,\n",
       " 'food': 10,\n",
       " 'of': 11,\n",
       " 'i': 12,\n",
       " 'for': 13,\n",
       " 'with': 14,\n",
       " 'in': 15,\n",
       " 'it': 16,\n",
       " 'service': 17,\n",
       " 'you': 18,\n",
       " 'are': 19,\n",
       " 'great': 20,\n",
       " 'we': 21,\n",
       " 'but': 22,\n",
       " 'good': 23,\n",
       " 'that': 24,\n",
       " '(': 25,\n",
       " 'not': 26,\n",
       " ')': 27,\n",
       " 'they': 28,\n",
       " 'on': 29,\n",
       " 'have': 30,\n",
       " 'had': 31,\n",
       " 'at': 32,\n",
       " 'this': 33,\n",
       " 'were': 34,\n",
       " 'very': 35,\n",
       " \"n't\": 36,\n",
       " \"'s\": 37,\n",
       " 'place': 38,\n",
       " 'as': 39,\n",
       " '-': 40,\n",
       " 'my': 41,\n",
       " '!': 42,\n",
       " 'so': 43,\n",
       " 'be': 44,\n",
       " 'menu': 45,\n",
       " 'all': 46,\n",
       " 'from': 47,\n",
       " 'an': 48,\n",
       " 'like': 49,\n",
       " 'wine': 50,\n",
       " 'our': 51,\n",
       " 'if': 52,\n",
       " 'or': 53,\n",
       " 'there': 54,\n",
       " 'their': 55,\n",
       " 'excellent': 56,\n",
       " 'delicious': 57,\n",
       " 'best': 58,\n",
       " 'get': 59,\n",
       " 'staff': 60,\n",
       " '$': 61,\n",
       " 'out': 62,\n",
       " 'one': 63,\n",
       " 'fresh': 64,\n",
       " 'pizza': 65,\n",
       " 'restaurant': 66,\n",
       " 'nice': 67,\n",
       " 'prices': 68,\n",
       " 'has': 69,\n",
       " 'when': 70,\n",
       " 'dinner': 71,\n",
       " 'chicken': 72,\n",
       " 'us': 73,\n",
       " 'by': 74,\n",
       " 'do': 75,\n",
       " 'dishes': 76,\n",
       " 'atmosphere': 77,\n",
       " 'your': 78,\n",
       " 'some': 79,\n",
       " 'no': 80,\n",
       " 'been': 81,\n",
       " 'even': 82,\n",
       " 'here': 83,\n",
       " 'friendly': 84,\n",
       " 'well': 85,\n",
       " 'go': 86,\n",
       " 'table': 87,\n",
       " 'sushi': 88,\n",
       " 'about': 89,\n",
       " 'would': 90,\n",
       " 'only': 91,\n",
       " 'drinks': 92,\n",
       " 'always': 93,\n",
       " 'price': 94,\n",
       " 'which': 95,\n",
       " 'cheese': 96,\n",
       " 'really': 97,\n",
       " 'meal': 98,\n",
       " 'can': 99,\n",
       " 'more': 100,\n",
       " 'bar': 101,\n",
       " \"'ve\": 102,\n",
       " 'wait': 103,\n",
       " 'just': 104,\n",
       " 'also': 105,\n",
       " 'sauce': 106,\n",
       " 'than': 107,\n",
       " 'while': 108,\n",
       " 'what': 109,\n",
       " 'fish': 110,\n",
       " 'other': 111,\n",
       " 'rice': 112,\n",
       " 'try': 113,\n",
       " 'too': 114,\n",
       " 'up': 115,\n",
       " 'small': 116,\n",
       " 'me': 117,\n",
       " 'most': 118,\n",
       " 'lunch': 119,\n",
       " 'better': 120,\n",
       " '--': 121,\n",
       " 'did': 122,\n",
       " 'order': 123,\n",
       " 'made': 124,\n",
       " 'will': 125,\n",
       " 'dish': 126,\n",
       " 'because': 127,\n",
       " 'ordered': 128,\n",
       " 'little': 129,\n",
       " 'portions': 130,\n",
       " 'quality': 131,\n",
       " 'people': 132,\n",
       " 'waiter': 133,\n",
       " 'could': 134,\n",
       " 'never': 135,\n",
       " 'over': 136,\n",
       " 'who': 137,\n",
       " 'recommend': 138,\n",
       " 'special': 139,\n",
       " 'eat': 140,\n",
       " 'make': 141,\n",
       " 'tasty': 142,\n",
       " 'back': 143,\n",
       " 'ever': 144,\n",
       " 'hot': 145,\n",
       " 'time': 146,\n",
       " 'much': 147,\n",
       " 'salad': 148,\n",
       " 'appetizer': 149,\n",
       " 'though': 150,\n",
       " 'times': 151,\n",
       " 'both': 152,\n",
       " 'appetizers': 153,\n",
       " 'thai': 154,\n",
       " 'attentive': 155,\n",
       " 'spicy': 156,\n",
       " 'decor': 157,\n",
       " 'two': 158,\n",
       " 'taste': 159,\n",
       " '2': 160,\n",
       " 'everything': 161,\n",
       " 'served': 162,\n",
       " 'restaurants': 163,\n",
       " 'then': 164,\n",
       " 'ambiance': 165,\n",
       " 'experience': 166,\n",
       " 'wonderful': 167,\n",
       " 'being': 168,\n",
       " 'lamb': 169,\n",
       " 'dessert': 170,\n",
       " 'amazing': 171,\n",
       " '...': 172,\n",
       " 'after': 173,\n",
       " ':': 174,\n",
       " 'new': 175,\n",
       " 'average': 176,\n",
       " 'shrimp': 177,\n",
       " 'perfect': 178,\n",
       " 'went': 179,\n",
       " 'love': 180,\n",
       " 'ambience': 181,\n",
       " 'its': 182,\n",
       " 'soup': 183,\n",
       " 'chef': 184,\n",
       " 'actually': 185,\n",
       " 'them': 186,\n",
       " 'steak': 187,\n",
       " 'night': 188,\n",
       " 'selection': 189,\n",
       " 'such': 190,\n",
       " 'way': 191,\n",
       " 'cheap': 192,\n",
       " 'he': 193,\n",
       " 'large': 194,\n",
       " 'quite': 195,\n",
       " 'got': 196,\n",
       " 'before': 197,\n",
       " 'list': 198,\n",
       " 'salads': 199,\n",
       " 'entree': 200,\n",
       " 'seated': 201,\n",
       " \"'re\": 202,\n",
       " 'bit': 203,\n",
       " 'however': 204,\n",
       " 'reasonable': 205,\n",
       " 'indian': 206,\n",
       " 'area': 207,\n",
       " 'bread': 208,\n",
       " 'priced': 209,\n",
       " 'came': 210,\n",
       " \"'\": 211,\n",
       " 'any': 212,\n",
       " 'bad': 213,\n",
       " 'minutes': 214,\n",
       " 'ca': 215,\n",
       " 'still': 216,\n",
       " 'each': 217,\n",
       " 'how': 218,\n",
       " 'romantic': 219,\n",
       " 'glass': 220,\n",
       " 'authentic': 221,\n",
       " 'must': 222,\n",
       " 'fried': 223,\n",
       " 'worth': 224,\n",
       " 'down': 225,\n",
       " 'dining': 226,\n",
       " 'pretty': 227,\n",
       " 'makes': 228,\n",
       " 'tuna': 229,\n",
       " 'tables': 230,\n",
       " 'does': 231,\n",
       " 'know': 232,\n",
       " 'want': 233,\n",
       " 'specials': 234,\n",
       " 'warm': 235,\n",
       " 'seafood': 236,\n",
       " 'rolls': 237,\n",
       " 'cuisine': 238,\n",
       " 'looking': 239,\n",
       " 'french': 240,\n",
       " 'lot': 241,\n",
       " 'friends': 242,\n",
       " 'pasta': 243,\n",
       " 'three': 244,\n",
       " 'beef': 245,\n",
       " 'take': 246,\n",
       " 'full': 247,\n",
       " 'around': 248,\n",
       " 'dumplings': 249,\n",
       " 'vegetables': 250,\n",
       " 'drink': 251,\n",
       " 'definitely': 252,\n",
       " 'bartender': 253,\n",
       " 'main': 254,\n",
       " 'big': 255,\n",
       " 'cool': 256,\n",
       " 'especially': 257,\n",
       " 'italian': 258,\n",
       " 'tried': 259,\n",
       " 'cold': 260,\n",
       " 'highly': 261,\n",
       " 'nyc': 262,\n",
       " 'manager': 263,\n",
       " 'plate': 264,\n",
       " 'yet': 265,\n",
       " 'come': 266,\n",
       " 'fine': 267,\n",
       " 'city': 268,\n",
       " 'where': 269,\n",
       " 'date': 270,\n",
       " 'rude': 271,\n",
       " 'right': 272,\n",
       " 'find': 273,\n",
       " 'desserts': 274,\n",
       " 'without': 275,\n",
       " 'course': 276,\n",
       " 'entrees': 277,\n",
       " 'flavor': 278,\n",
       " 'bill': 279,\n",
       " 'few': 280,\n",
       " 'music': 281,\n",
       " 'cooked': 282,\n",
       " 'waiters': 283,\n",
       " 'away': 284,\n",
       " 'roll': 285,\n",
       " 'thing': 286,\n",
       " 'reservation': 287,\n",
       " 'decent': 288,\n",
       " 'fast': 289,\n",
       " \"'ll\": 290,\n",
       " 'enjoyed': 291,\n",
       " 'sweet': 292,\n",
       " 'slow': 293,\n",
       " 'sit': 294,\n",
       " 'fantastic': 295,\n",
       " 'world': 296,\n",
       " 'usually': 297,\n",
       " 'enough': 298,\n",
       " 'nothing': 299,\n",
       " 'garlic': 300,\n",
       " 'his': 301,\n",
       " 'expensive': 302,\n",
       " 'noodles': 303,\n",
       " 'first': 304,\n",
       " 'crab': 305,\n",
       " 'room': 306,\n",
       " 'simple': 307,\n",
       " 'waitress': 308,\n",
       " 'ingredients': 309,\n",
       " 'chocolate': 310,\n",
       " 'hour': 311,\n",
       " 'sandwiches': 312,\n",
       " 'many': 313,\n",
       " 'portion': 314,\n",
       " 'sandwich': 315,\n",
       " 'couple': 316,\n",
       " 'curry': 317,\n",
       " 'last': 318,\n",
       " 'again': 319,\n",
       " 'value': 320,\n",
       " 'dim': 321,\n",
       " 'sum': 322,\n",
       " '20': 323,\n",
       " 'into': 324,\n",
       " 'below': 325,\n",
       " 'pork': 326,\n",
       " 'beer': 327,\n",
       " 'outside': 328,\n",
       " 'different': 329,\n",
       " 'choice': 330,\n",
       " 'spot': 331,\n",
       " 'black': 332,\n",
       " \"'m\": 333,\n",
       " 'say': 334,\n",
       " 'sake': 335,\n",
       " 'huge': 336,\n",
       " 'side': 337,\n",
       " 'including': 338,\n",
       " 'husband': 339,\n",
       " 'took': 340,\n",
       " 'every': 341,\n",
       " 'owner': 342,\n",
       " 'during': 343,\n",
       " 'comes': 344,\n",
       " 'although': 345,\n",
       " 'eating': 346,\n",
       " 'space': 347,\n",
       " 'water': 348,\n",
       " 'less': 349,\n",
       " 'reservations': 350,\n",
       " 'favorite': 351,\n",
       " 'should': 352,\n",
       " 'style': 353,\n",
       " 'salmon': 354,\n",
       " 'york': 355,\n",
       " 'asked': 356,\n",
       " 'ok': 357,\n",
       " 'years': 358,\n",
       " 'need': 359,\n",
       " 'lobster': 360,\n",
       " 'kind': 361,\n",
       " 'having': 362,\n",
       " 'almost': 363,\n",
       " 'day': 364,\n",
       " 'money': 365,\n",
       " \"'d\": 366,\n",
       " 'prompt': 367,\n",
       " 'enjoy': 368,\n",
       " 'places': 369,\n",
       " 'delivery': 370,\n",
       " 'half': 371,\n",
       " 'those': 372,\n",
       " 'overall': 373,\n",
       " 'least': 374,\n",
       " 'quick': 375,\n",
       " 'give': 376,\n",
       " 'japanese': 377,\n",
       " 'top': 378,\n",
       " 'next': 379,\n",
       " 'high': 380,\n",
       " 'whole': 381,\n",
       " 'sea': 382,\n",
       " 'same': 383,\n",
       " 'fabulous': 384,\n",
       " 'see': 385,\n",
       " 'extremely': 386,\n",
       " 'server': 387,\n",
       " 'group': 388,\n",
       " 'bring': 389,\n",
       " 'include': 390,\n",
       " 'outstanding': 391,\n",
       " 'left': 392,\n",
       " 'feel': 393,\n",
       " 'look': 394,\n",
       " 'off': 395,\n",
       " 'pay': 396,\n",
       " 'size': 397,\n",
       " 'happy': 398,\n",
       " 'cod': 399,\n",
       " 'brunch': 400,\n",
       " 'beautiful': 401,\n",
       " 'horrible': 402,\n",
       " 'seating': 403,\n",
       " 'fries': 404,\n",
       " 'often': 405,\n",
       " 'kitchen': 406,\n",
       " 'fare': 407,\n",
       " 'sure': 408,\n",
       " 'fun': 409,\n",
       " 'mediocre': 410,\n",
       " 'soft': 411,\n",
       " 'neighborhood': 412,\n",
       " 'she': 413,\n",
       " 'expect': 414,\n",
       " 'incredible': 415,\n",
       " '?': 416,\n",
       " 'crust': 417,\n",
       " 'yummy': 418,\n",
       " 'things': 419,\n",
       " 'bagel': 420,\n",
       " 'tasting': 421,\n",
       " 'entire': 422,\n",
       " 'variety': 423,\n",
       " 'live': 424,\n",
       " 'looked': 425,\n",
       " 'spinach': 426,\n",
       " 'friend': 427,\n",
       " 'work': 428,\n",
       " ';': 429,\n",
       " 'terrible': 430,\n",
       " 'bistro': 431,\n",
       " 'bagels': 432,\n",
       " 'waiting': 433,\n",
       " 'ny': 434,\n",
       " 'may': 435,\n",
       " 'wines': 436,\n",
       " 'interesting': 437,\n",
       " 'pad': 438,\n",
       " 'cute': 439,\n",
       " 'life': 440,\n",
       " 'seems': 441,\n",
       " 'deal': 442,\n",
       " 'crowded': 443,\n",
       " 'house': 444,\n",
       " 'overpriced': 445,\n",
       " 'includes': 446,\n",
       " 'reasonably': 447,\n",
       " 'sashimi': 448,\n",
       " 'sharing': 449,\n",
       " 'green': 450,\n",
       " 'oil': 451,\n",
       " 'pleasant': 452,\n",
       " 'given': 453,\n",
       " 'real': 454,\n",
       " 'family': 455,\n",
       " 'part': 456,\n",
       " 'going': 457,\n",
       " 'offer': 458,\n",
       " 'late': 459,\n",
       " 'regular': 460,\n",
       " 'tiny': 461,\n",
       " 'else': 462,\n",
       " 'choices': 463,\n",
       " 'offers': 464,\n",
       " 'am': 465,\n",
       " 'think': 466,\n",
       " 'reviews': 467,\n",
       " 'chinese': 468,\n",
       " 'getting': 469,\n",
       " 'red': 470,\n",
       " 'since': 471,\n",
       " 'duck': 472,\n",
       " 'bottle': 473,\n",
       " 'waitstaff': 474,\n",
       " '5': 475,\n",
       " 'coming': 476,\n",
       " 'might': 477,\n",
       " 'keep': 478,\n",
       " 'terrific': 479,\n",
       " 'told': 480,\n",
       " 'comfortable': 481,\n",
       " 'check': 482,\n",
       " 'poor': 483,\n",
       " 'burgers': 484,\n",
       " '\"': 485,\n",
       " 'foods': 486,\n",
       " 'bland': 487,\n",
       " 'blue': 488,\n",
       " 'prepared': 489,\n",
       " 'ask': 490,\n",
       " 'complimentary': 491,\n",
       " 'mix': 492,\n",
       " '+': 493,\n",
       " 'found': 494,\n",
       " 'wrong': 495,\n",
       " 'person': 496,\n",
       " 'end': 497,\n",
       " 'these': 498,\n",
       " 'party': 499,\n",
       " '15': 500,\n",
       " 'creative': 501,\n",
       " 'offered': 502,\n",
       " 'owners': 503,\n",
       " 'impressed': 504,\n",
       " 'return': 505,\n",
       " 'falafel': 506,\n",
       " 'door': 507,\n",
       " 'anything': 508,\n",
       " 'etc': 509,\n",
       " 'beers': 510,\n",
       " 'soggy': 511,\n",
       " 'close': 512,\n",
       " 'hostess': 513,\n",
       " 'visit': 514,\n",
       " 'once': 515,\n",
       " 'gave': 516,\n",
       " 'garden': 517,\n",
       " 'something': 518,\n",
       " 'sat': 519,\n",
       " 'homemade': 520,\n",
       " 'super': 521,\n",
       " 'far': 522,\n",
       " 'tastes': 523,\n",
       " 'cream': 524,\n",
       " 'white': 525,\n",
       " 'ravioli': 526,\n",
       " 'location': 527,\n",
       " 'said': 528,\n",
       " 'worst': 529,\n",
       " 'personal': 530,\n",
       " 'instead': 531,\n",
       " 'saturday': 532,\n",
       " 'classic': 533,\n",
       " 'middle': 534,\n",
       " 'sometimes': 535,\n",
       " 'items': 536,\n",
       " 'disappointed': 537,\n",
       " 'cozy': 538,\n",
       " 'simply': 539,\n",
       " 'spectacular': 540,\n",
       " 'jazz': 541,\n",
       " 'done': 542,\n",
       " 'ate': 543,\n",
       " 'favorites': 544,\n",
       " 'helpful': 545,\n",
       " 'considering': 546,\n",
       " 'plain': 547,\n",
       " 'casual': 548,\n",
       " 'pieces': 549,\n",
       " 'sitting': 550,\n",
       " 'anyone': 551,\n",
       " 'probably': 552,\n",
       " 'busy': 553,\n",
       " 'tomato': 554,\n",
       " 'rest': 555,\n",
       " 'until': 556,\n",
       " 'mussels': 557,\n",
       " 'started': 558,\n",
       " 'except': 559,\n",
       " 'glasses': 560,\n",
       " 'stick': 561,\n",
       " 'awesome': 562,\n",
       " 'extensive': 563,\n",
       " 'empty': 564,\n",
       " 'easily': 565,\n",
       " 'used': 566,\n",
       " 'wife': 567,\n",
       " 'burger': 568,\n",
       " 'thin': 569,\n",
       " 'eaten': 570,\n",
       " 'spreads': 571,\n",
       " 'let': 572,\n",
       " '7': 573,\n",
       " 'courses': 574,\n",
       " '3': 575,\n",
       " 'forgot': 576,\n",
       " 'absolutely': 577,\n",
       " 'conversation': 578,\n",
       " 'fact': 579,\n",
       " 'mushrooms': 580,\n",
       " 'sizes': 581,\n",
       " 'saw': 582,\n",
       " 'pastrami': 583,\n",
       " 'felt': 584,\n",
       " 'certainly': 585,\n",
       " 'street': 586,\n",
       " 'walk': 587,\n",
       " 'spice': 588,\n",
       " 'completely': 589,\n",
       " 'another': 590,\n",
       " 'itself': 591,\n",
       " 'cake': 592,\n",
       " 'beat': 593,\n",
       " 'quantity': 594,\n",
       " 'rather': 595,\n",
       " 'cost': 596,\n",
       " 'old': 597,\n",
       " 'tasted': 598,\n",
       " 'awful': 599,\n",
       " 'long': 600,\n",
       " '100': 601,\n",
       " 'pie': 602,\n",
       " 'available': 603,\n",
       " 'unique': 604,\n",
       " 'fatty': 605,\n",
       " 'selections': 606,\n",
       " 'goat': 607,\n",
       " 'arrived': 608,\n",
       " 'received': 609,\n",
       " '1': 610,\n",
       " 'lack': 611,\n",
       " 'sichuan': 612,\n",
       " 'addition': 613,\n",
       " 'either': 614,\n",
       " 'mushroom': 615,\n",
       " 'name': 616,\n",
       " 'home': 617,\n",
       " 'mind': 618,\n",
       " 'superb': 619,\n",
       " 'add': 620,\n",
       " 'east': 621,\n",
       " 'salty': 622,\n",
       " 'fixe': 623,\n",
       " 'tempura': 624,\n",
       " 'use': 625,\n",
       " 'smaller': 626,\n",
       " 'above': 627,\n",
       " 'tip': 628,\n",
       " 'delivered': 629,\n",
       " 'summer': 630,\n",
       " 'eggplant': 631,\n",
       " 'her': 632,\n",
       " 'touch': 633,\n",
       " 'basil': 634,\n",
       " 'pick': 635,\n",
       " 'serving': 636,\n",
       " 'popular': 637,\n",
       " 'fancy': 638,\n",
       " 'servers': 639,\n",
       " 'past': 640,\n",
       " 'host': 641,\n",
       " 'ribs': 642,\n",
       " 'american': 643,\n",
       " 'incredibly': 644,\n",
       " 'recommended': 645,\n",
       " 'several': 646,\n",
       " 'evening': 647,\n",
       " 'setting': 648,\n",
       " 'naan': 649,\n",
       " 'modern': 650,\n",
       " 'looks': 651,\n",
       " 'despite': 652,\n",
       " 'presented': 653,\n",
       " 'per': 654,\n",
       " 'butter': 655,\n",
       " 'twice': 656,\n",
       " 'mouth': 657,\n",
       " 'seen': 658,\n",
       " 'ordering': 659,\n",
       " 'midtown': 660,\n",
       " 'yes': 661,\n",
       " 'myself': 662,\n",
       " 'root': 663,\n",
       " 'chinatown': 664,\n",
       " 'extra': 665,\n",
       " 'ice': 666,\n",
       " 'dark': 667,\n",
       " 'vibe': 668,\n",
       " 'care': 669,\n",
       " 'town': 670,\n",
       " 'typical': 671,\n",
       " 'lasagna': 672,\n",
       " 'traditional': 673,\n",
       " 'grilled': 674,\n",
       " 'prix': 675,\n",
       " 'loud': 676,\n",
       " 'mahi': 677,\n",
       " 'meat': 678,\n",
       " 'inexpensive': 679,\n",
       " 'chops': 680,\n",
       " '*': 681,\n",
       " 'line': 682,\n",
       " 'steaks': 683,\n",
       " 'soups': 684,\n",
       " 'pastas': 685,\n",
       " 'lovely': 686,\n",
       " 'plus': 687,\n",
       " 'assortment': 688,\n",
       " 'village': 689,\n",
       " 'hip': 690,\n",
       " 'delicate': 691,\n",
       " 'truffle': 692,\n",
       " 'included': 693,\n",
       " 'filet': 694,\n",
       " 'range': 695,\n",
       " '!!!': 696,\n",
       " 'wo': 697,\n",
       " 'toast': 698,\n",
       " 'eggs': 699,\n",
       " 'someone': 700,\n",
       " 'shows': 701,\n",
       " 'enjoying': 702,\n",
       " 'meals': 703,\n",
       " 'leave': 704,\n",
       " 'now': 705,\n",
       " 'beyond': 706,\n",
       " 'week': 707,\n",
       " 'stuff': 708,\n",
       " 'noodle': 709,\n",
       " 'spent': 710,\n",
       " 'hand': 711,\n",
       " 'beginning': 712,\n",
       " 'scallops': 713,\n",
       " 'limited': 714,\n",
       " 'management': 715,\n",
       " 'unless': 716,\n",
       " 'thought': 717,\n",
       " '10': 718,\n",
       " 'outdoor': 719,\n",
       " '6': 720,\n",
       " 'soy': 721,\n",
       " 'whether': 722,\n",
       " 'free': 723,\n",
       " 'kebabs': 724,\n",
       " 'veggies': 725,\n",
       " 'slice': 726,\n",
       " 'gets': 727,\n",
       " 'delightful': 728,\n",
       " 'liked': 729,\n",
       " 'pot': 730,\n",
       " 'squid': 731,\n",
       " 'hang': 732,\n",
       " 'pricey': 733,\n",
       " 'please': 734,\n",
       " 'second': 735,\n",
       " 'shredded': 736,\n",
       " 'shell': 737,\n",
       " 'gras': 738,\n",
       " 'birthday': 739,\n",
       " 'business': 740,\n",
       " 'forget': 741,\n",
       " 'through': 742,\n",
       " 'call': 743,\n",
       " 'open': 744,\n",
       " 'customer': 745,\n",
       " 'sliced': 746,\n",
       " 'consistently': 747,\n",
       " 'finally': 748,\n",
       " 'potatoes': 749,\n",
       " 'wanted': 750,\n",
       " 'secret': 751,\n",
       " 'put': 752,\n",
       " 'unlike': 753,\n",
       " 'calamari': 754,\n",
       " 'nights': 755,\n",
       " 'piece': 756,\n",
       " 'seat': 757,\n",
       " 'filling': 758,\n",
       " 'impressive': 759,\n",
       " 'view': 760,\n",
       " 'potato': 761,\n",
       " 'known': 762,\n",
       " 'barely': 763,\n",
       " 'breakfast': 764,\n",
       " 'sides': 765,\n",
       " 'understand': 766,\n",
       " 'basic': 767,\n",
       " 'toppings': 768,\n",
       " 'chips': 769,\n",
       " 'accomodating': 770,\n",
       " 'aside': 771,\n",
       " 'otherwise': 772,\n",
       " 'fix': 773,\n",
       " 'across': 774,\n",
       " 'meats': 775,\n",
       " 'banana': 776,\n",
       " 'expected': 777,\n",
       " 'lots': 778,\n",
       " 'dry': 779,\n",
       " 'orders': 780,\n",
       " 'plan': 781,\n",
       " 'lemon': 782,\n",
       " 'picks': 783,\n",
       " 'vegetable': 784,\n",
       " 'juice': 785,\n",
       " 'comfort': 786,\n",
       " 'guests': 787,\n",
       " 'save': 788,\n",
       " 'quiet': 789,\n",
       " 'hard': 790,\n",
       " 'sent': 791,\n",
       " 'intimate': 792,\n",
       " 'ive': 793,\n",
       " 'affordable': 794,\n",
       " 'boyfriend': 795,\n",
       " 'making': 796,\n",
       " 'clean': 797,\n",
       " 'famous': 798,\n",
       " 'scrumptious': 799,\n",
       " '%': 800,\n",
       " 'point': 801,\n",
       " 'changed': 802,\n",
       " 'anywhere': 803,\n",
       " 'baked': 804,\n",
       " 'heard': 805,\n",
       " 'classics': 806,\n",
       " 'par': 807,\n",
       " 'egg': 808,\n",
       " 'share': 809,\n",
       " 'olive': 810,\n",
       " 'tapas': 811,\n",
       " 'relaxed': 812,\n",
       " 'serve': 813,\n",
       " 'cramped': 814,\n",
       " 'filled': 815,\n",
       " 'burnt': 816,\n",
       " '!!': 817,\n",
       " 'dressing': 818,\n",
       " '8': 819,\n",
       " 'sorts': 820,\n",
       " 'eastern': 821,\n",
       " 'medley': 822,\n",
       " 'baclava': 823,\n",
       " 'soooo': 824,\n",
       " 'fooood': 825,\n",
       " 'evident': 826,\n",
       " 'chance': 827,\n",
       " 'avenue': 828,\n",
       " 'buffet': 829,\n",
       " 'split': 830,\n",
       " 'slightly': 831,\n",
       " 'amount': 832,\n",
       " 'udon': 833,\n",
       " 'horribly': 834,\n",
       " 'paid': 835,\n",
       " 'delectable': 836,\n",
       " 'offering': 837,\n",
       " 'written': 838,\n",
       " 'crispy': 839,\n",
       " 'foie': 840,\n",
       " 'everyone': 841,\n",
       " '30': 842,\n",
       " 'smile': 843,\n",
       " 'counter': 844,\n",
       " 'bacon': 845,\n",
       " 'matter': 846,\n",
       " 'employees': 847,\n",
       " 'square': 848,\n",
       " 'interior': 849,\n",
       " 'solid': 850,\n",
       " 'freshness': 851,\n",
       " 'perfection': 852,\n",
       " 'promptly': 853,\n",
       " 'dogs': 854,\n",
       " 'brulee': 855,\n",
       " 'surprise': 856,\n",
       " '4': 857,\n",
       " 'roast': 858,\n",
       " 'champagne': 859,\n",
       " 'tell': 860,\n",
       " 'beans': 861,\n",
       " 'relaxing': 862,\n",
       " 'flavors': 863,\n",
       " 'ended': 864,\n",
       " 'choose': 865,\n",
       " 'blend': 866,\n",
       " 'baby': 867,\n",
       " 'taken': 868,\n",
       " 'hamburger': 869,\n",
       " 'rushed': 870,\n",
       " 'mom': 871,\n",
       " 'serves': 872,\n",
       " 'phone': 873,\n",
       " 'relax': 874,\n",
       " 'asian': 875,\n",
       " 'fusion': 876,\n",
       " 'suggestions': 877,\n",
       " 'greatest': 878,\n",
       " 'pepper': 879,\n",
       " 'promised': 880,\n",
       " '45': 881,\n",
       " 'truly': 882,\n",
       " 'trying': 883,\n",
       " 'guizhou': 884,\n",
       " 'asking': 885,\n",
       " '50': 886,\n",
       " 'type': 887,\n",
       " 'feeling': 888,\n",
       " 'able': 889,\n",
       " 'tea': 890,\n",
       " 'rush': 891,\n",
       " 'suggest': 892,\n",
       " 'lox': 893,\n",
       " 'combination': 894,\n",
       " 'various': 895,\n",
       " 'busboy': 896,\n",
       " 'believe': 897,\n",
       " 'ago': 898,\n",
       " 'packed': 899,\n",
       " 'manhattan': 900,\n",
       " 'greens': 901,\n",
       " 'scene': 902,\n",
       " 'phenomenal': 903,\n",
       " 'heavy': 904,\n",
       " 'veal': 905,\n",
       " 'cafe': 906,\n",
       " 'low': 907,\n",
       " 'takeout': 908,\n",
       " 'congee': 909,\n",
       " 'lacked': 910,\n",
       " 'mention': 911,\n",
       " 'cocktail': 912,\n",
       " 'joint': 913,\n",
       " 'girls': 914,\n",
       " 'guest': 915,\n",
       " 'surprisingly': 916,\n",
       " 'avocado': 917,\n",
       " 'diner': 918,\n",
       " 'round': 919,\n",
       " 'mediterranean': 920,\n",
       " 'chelsea': 921,\n",
       " 'mac': 922,\n",
       " 'five': 923,\n",
       " 'pudding': 924,\n",
       " 'sangria': 925,\n",
       " 'assorted': 926,\n",
       " 'types': 927,\n",
       " 'platter': 928,\n",
       " 'portobello': 929,\n",
       " 'inedible': 930,\n",
       " 'pancake': 931,\n",
       " 'minute': 932,\n",
       " 'easy': 933,\n",
       " 'lacking': 934,\n",
       " 'bass': 935,\n",
       " 'called': 936,\n",
       " 'joints': 937,\n",
       " 'similar': 938,\n",
       " 'containers': 939,\n",
       " 'trout': 940,\n",
       " 'behind': 941,\n",
       " 'fondue': 942,\n",
       " 'tanks': 943,\n",
       " 'alain': 944,\n",
       " 'ducasse': 945,\n",
       " 'section': 946,\n",
       " 'knows': 947,\n",
       " 'disappointing': 948,\n",
       " 'crowd': 949,\n",
       " 'platters': 950,\n",
       " 'roasted': 951,\n",
       " 'creme': 952,\n",
       " 'spend': 953,\n",
       " 'caviar': 954,\n",
       " 'seem': 955,\n",
       " 'run': 956,\n",
       " 'together': 957,\n",
       " 'unfortunately': 958,\n",
       " 'short': 959,\n",
       " 'miso': 960,\n",
       " 'works': 961,\n",
       " 'grab': 962,\n",
       " 'professional': 963,\n",
       " 'oysters': 964,\n",
       " 'waitresses': 965,\n",
       " 'explain': 966,\n",
       " 'own': 967,\n",
       " 'options': 968,\n",
       " 'tofu': 969,\n",
       " 'sauces': 970,\n",
       " 'refreshing': 971,\n",
       " 'pizzas': 972,\n",
       " 'stay': 973,\n",
       " 'problem': 974,\n",
       " 'mozzarella': 975,\n",
       " 'bartenders': 976,\n",
       " 'level': 977,\n",
       " 'prosciutto': 978,\n",
       " 'arugula': 979,\n",
       " 'sesame': 980,\n",
       " 'walking': 981,\n",
       " '12': 982,\n",
       " 'wraps': 983,\n",
       " 'japan': 984,\n",
       " 'sampler': 985,\n",
       " 'folks': 986,\n",
       " 'star': 987,\n",
       " 'somewhat': 988,\n",
       " 'passion': 989,\n",
       " 'remember': 990,\n",
       " 'trendy': 991,\n",
       " 'agree': 992,\n",
       " 'particular': 993,\n",
       " 'frites': 994,\n",
       " 'salt': 995,\n",
       " 'scallion': 996,\n",
       " 'pancakes': 997,\n",
       " 'hungry': 998,\n",
       " 'supposed': 999,\n",
       " 'besides': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 1,\n",
       " 'the': 2,\n",
       " ',': 3,\n",
       " '.': 4,\n",
       " 'and': 5,\n",
       " 'a': 6,\n",
       " 'is': 7,\n",
       " 'to': 8,\n",
       " 'was': 9,\n",
       " 'food': 10,\n",
       " 'of': 11,\n",
       " 'i': 12,\n",
       " 'for': 13,\n",
       " 'with': 14,\n",
       " 'in': 15,\n",
       " 'it': 16,\n",
       " 'service': 17,\n",
       " 'you': 18,\n",
       " 'are': 19,\n",
       " 'great': 20,\n",
       " 'we': 21,\n",
       " 'but': 22,\n",
       " 'good': 23,\n",
       " 'that': 24,\n",
       " '(': 25,\n",
       " 'not': 26,\n",
       " ')': 27,\n",
       " 'they': 28,\n",
       " 'on': 29,\n",
       " 'have': 30,\n",
       " 'had': 31,\n",
       " 'at': 32,\n",
       " 'this': 33,\n",
       " 'were': 34,\n",
       " 'very': 35,\n",
       " \"n't\": 36,\n",
       " \"'s\": 37,\n",
       " 'place': 38,\n",
       " 'as': 39,\n",
       " '-': 40,\n",
       " 'my': 41,\n",
       " '!': 42,\n",
       " 'so': 43,\n",
       " 'be': 44,\n",
       " 'menu': 45,\n",
       " 'all': 46,\n",
       " 'from': 47,\n",
       " 'an': 48,\n",
       " 'like': 49,\n",
       " 'wine': 50,\n",
       " 'our': 51,\n",
       " 'if': 52,\n",
       " 'or': 53,\n",
       " 'there': 54,\n",
       " 'their': 55,\n",
       " 'excellent': 56,\n",
       " 'delicious': 57,\n",
       " 'best': 58,\n",
       " 'get': 59,\n",
       " 'staff': 60,\n",
       " 'out': 61,\n",
       " '$': 62,\n",
       " 'one': 63,\n",
       " 'fresh': 64,\n",
       " 'pizza': 65,\n",
       " 'restaurant': 66,\n",
       " 'nice': 67,\n",
       " 'prices': 68,\n",
       " 'has': 69,\n",
       " 'when': 70,\n",
       " 'dinner': 71,\n",
       " 'us': 72,\n",
       " 'chicken': 73,\n",
       " 'do': 74,\n",
       " 'by': 75,\n",
       " 'dishes': 76,\n",
       " 'atmosphere': 77,\n",
       " 'your': 78,\n",
       " 'some': 79,\n",
       " 'no': 80,\n",
       " 'even': 81,\n",
       " 'been': 82,\n",
       " 'here': 83,\n",
       " 'well': 84,\n",
       " 'friendly': 85,\n",
       " 'go': 86,\n",
       " 'table': 87,\n",
       " 'sushi': 88,\n",
       " 'would': 89,\n",
       " 'about': 90,\n",
       " 'drinks': 91,\n",
       " 'only': 92,\n",
       " 'which': 93,\n",
       " 'price': 94,\n",
       " 'always': 95,\n",
       " 'meal': 96,\n",
       " 'really': 97,\n",
       " 'cheese': 98,\n",
       " 'can': 99,\n",
       " 'more': 100,\n",
       " 'bar': 101,\n",
       " \"'ve\": 102,\n",
       " 'wait': 103,\n",
       " 'just': 104,\n",
       " 'sauce': 105,\n",
       " 'also': 106,\n",
       " 'than': 107,\n",
       " 'while': 108,\n",
       " 'what': 109,\n",
       " 'fish': 110,\n",
       " 'other': 111,\n",
       " 'rice': 112,\n",
       " 'try': 113,\n",
       " 'too': 114,\n",
       " 'up': 115,\n",
       " 'small': 116,\n",
       " 'most': 117,\n",
       " 'me': 118,\n",
       " 'lunch': 119,\n",
       " 'better': 120,\n",
       " '--': 121,\n",
       " 'did': 122,\n",
       " 'order': 123,\n",
       " 'made': 124,\n",
       " 'will': 125,\n",
       " 'dish': 126,\n",
       " 'because': 127,\n",
       " 'ordered': 128,\n",
       " 'little': 129,\n",
       " 'portions': 130,\n",
       " 'quality': 131,\n",
       " 'people': 132,\n",
       " 'waiter': 133,\n",
       " 'could': 134,\n",
       " 'never': 135,\n",
       " 'over': 136,\n",
       " 'recommend': 137,\n",
       " 'who': 138,\n",
       " 'special': 139,\n",
       " 'eat': 140,\n",
       " 'tasty': 141,\n",
       " 'make': 142,\n",
       " 'hot': 143,\n",
       " 'ever': 144,\n",
       " 'back': 145,\n",
       " 'time': 146,\n",
       " 'much': 147,\n",
       " 'salad': 148,\n",
       " 'appetizer': 149,\n",
       " 'though': 150,\n",
       " 'times': 151,\n",
       " 'thai': 152,\n",
       " 'appetizers': 153,\n",
       " 'both': 154,\n",
       " 'decor': 155,\n",
       " 'spicy': 156,\n",
       " 'attentive': 157,\n",
       " 'two': 158,\n",
       " 'restaurants': 159,\n",
       " 'taste': 160,\n",
       " 'everything': 161,\n",
       " 'served': 162,\n",
       " 'then': 163,\n",
       " '2': 164,\n",
       " 'experience': 165,\n",
       " 'ambiance': 166,\n",
       " 'wonderful': 167,\n",
       " 'lamb': 168,\n",
       " 'being': 169,\n",
       " 'dessert': 170,\n",
       " 'amazing': 171,\n",
       " '...': 172,\n",
       " 'after': 173,\n",
       " ':': 174,\n",
       " 'new': 175,\n",
       " 'average': 176,\n",
       " 'shrimp': 177,\n",
       " 'went': 178,\n",
       " 'perfect': 179,\n",
       " 'love': 180,\n",
       " 'ambience': 181,\n",
       " 'soup': 182,\n",
       " 'its': 183,\n",
       " 'chef': 184,\n",
       " 'them': 185,\n",
       " 'actually': 186,\n",
       " 'steak': 187,\n",
       " 'cheap': 188,\n",
       " 'night': 189,\n",
       " 'way': 190,\n",
       " 'such': 191,\n",
       " 'he': 192,\n",
       " 'selection': 193,\n",
       " 'quite': 194,\n",
       " 'large': 195,\n",
       " 'got': 196,\n",
       " 'list': 197,\n",
       " 'before': 198,\n",
       " 'seated': 199,\n",
       " \"'re\": 200,\n",
       " 'salads': 201,\n",
       " 'entree': 202,\n",
       " 'indian': 203,\n",
       " 'however': 204,\n",
       " 'reasonable': 205,\n",
       " 'bit': 206,\n",
       " 'bread': 207,\n",
       " 'came': 208,\n",
       " 'priced': 209,\n",
       " 'area': 210,\n",
       " 'bad': 211,\n",
       " 'any': 212,\n",
       " 'minutes': 213,\n",
       " \"'\": 214,\n",
       " 'each': 215,\n",
       " 'how': 216,\n",
       " 'ca': 217,\n",
       " 'still': 218,\n",
       " 'romantic': 219,\n",
       " 'glass': 220,\n",
       " 'must': 221,\n",
       " 'worth': 222,\n",
       " 'authentic': 223,\n",
       " 'fried': 224,\n",
       " 'dining': 225,\n",
       " 'down': 226,\n",
       " 'pretty': 227,\n",
       " 'makes': 228,\n",
       " 'does': 229,\n",
       " 'know': 230,\n",
       " 'tables': 231,\n",
       " 'tuna': 232,\n",
       " 'seafood': 233,\n",
       " 'want': 234,\n",
       " 'specials': 235,\n",
       " 'warm': 236,\n",
       " 'rolls': 237,\n",
       " 'cuisine': 238,\n",
       " 'looking': 239,\n",
       " 'three': 240,\n",
       " 'pasta': 241,\n",
       " 'beef': 242,\n",
       " 'take': 243,\n",
       " 'french': 244,\n",
       " 'friends': 245,\n",
       " 'lot': 246,\n",
       " 'around': 247,\n",
       " 'full': 248,\n",
       " 'drink': 249,\n",
       " 'dumplings': 250,\n",
       " 'definitely': 251,\n",
       " 'vegetables': 252,\n",
       " 'main': 253,\n",
       " 'bartender': 254,\n",
       " 'big': 255,\n",
       " 'tried': 256,\n",
       " 'especially': 257,\n",
       " 'highly': 258,\n",
       " 'italian': 259,\n",
       " 'cool': 260,\n",
       " 'cold': 261,\n",
       " 'yet': 262,\n",
       " 'nyc': 263,\n",
       " 'plate': 264,\n",
       " 'manager': 265,\n",
       " 'date': 266,\n",
       " 'find': 267,\n",
       " 'without': 268,\n",
       " 'right': 269,\n",
       " 'come': 270,\n",
       " 'city': 271,\n",
       " 'where': 272,\n",
       " 'course': 273,\n",
       " 'desserts': 274,\n",
       " 'fine': 275,\n",
       " 'rude': 276,\n",
       " 'entrees': 277,\n",
       " 'flavor': 278,\n",
       " 'music': 279,\n",
       " 'few': 280,\n",
       " 'bill': 281,\n",
       " 'reservation': 282,\n",
       " 'fast': 283,\n",
       " 'cooked': 284,\n",
       " 'enjoyed': 285,\n",
       " 'waiters': 286,\n",
       " 'decent': 287,\n",
       " 'roll': 288,\n",
       " 'away': 289,\n",
       " \"'ll\": 290,\n",
       " 'thing': 291,\n",
       " 'sweet': 292,\n",
       " 'world': 293,\n",
       " 'nothing': 294,\n",
       " 'his': 295,\n",
       " 'garlic': 296,\n",
       " 'enough': 297,\n",
       " 'fantastic': 298,\n",
       " 'noodles': 299,\n",
       " 'expensive': 300,\n",
       " 'slow': 301,\n",
       " 'sit': 302,\n",
       " 'usually': 303,\n",
       " 'simple': 304,\n",
       " 'ingredients': 305,\n",
       " 'room': 306,\n",
       " 'crab': 307,\n",
       " 'first': 308,\n",
       " 'waitress': 309,\n",
       " 'chocolate': 310,\n",
       " 'sandwich': 311,\n",
       " 'many': 312,\n",
       " 'portion': 313,\n",
       " 'sandwiches': 314,\n",
       " 'hour': 315,\n",
       " 'curry': 316,\n",
       " 'couple': 317,\n",
       " 'again': 318,\n",
       " 'last': 319,\n",
       " 'value': 320,\n",
       " 'dim': 321,\n",
       " 'sum': 322,\n",
       " 'below': 323,\n",
       " '20': 324,\n",
       " 'into': 325,\n",
       " 'pork': 326,\n",
       " 'choice': 327,\n",
       " 'different': 328,\n",
       " 'outside': 329,\n",
       " \"'m\": 330,\n",
       " 'say': 331,\n",
       " 'spot': 332,\n",
       " 'beer': 333,\n",
       " 'black': 334,\n",
       " 'husband': 335,\n",
       " 'eating': 336,\n",
       " 'comes': 337,\n",
       " 'space': 338,\n",
       " 'every': 339,\n",
       " 'huge': 340,\n",
       " 'took': 341,\n",
       " 'sake': 342,\n",
       " 'water': 343,\n",
       " 'including': 344,\n",
       " 'side': 345,\n",
       " 'although': 346,\n",
       " 'during': 347,\n",
       " 'owner': 348,\n",
       " 'less': 349,\n",
       " 'salmon': 350,\n",
       " 'favorite': 351,\n",
       " 'asked': 352,\n",
       " 'ok': 353,\n",
       " 'should': 354,\n",
       " 'reservations': 355,\n",
       " 'style': 356,\n",
       " 'york': 357,\n",
       " 'years': 358,\n",
       " \"'d\": 359,\n",
       " 'lobster': 360,\n",
       " 'almost': 361,\n",
       " 'kind': 362,\n",
       " 'enjoy': 363,\n",
       " 'money': 364,\n",
       " 'need': 365,\n",
       " 'prompt': 366,\n",
       " 'having': 367,\n",
       " 'day': 368,\n",
       " 'give': 369,\n",
       " 'places': 370,\n",
       " 'overall': 371,\n",
       " 'top': 372,\n",
       " 'half': 373,\n",
       " 'quick': 374,\n",
       " 'delivery': 375,\n",
       " 'those': 376,\n",
       " 'japanese': 377,\n",
       " 'least': 378,\n",
       " 'left': 379,\n",
       " 'include': 380,\n",
       " 'whole': 381,\n",
       " 'same': 382,\n",
       " 'see': 383,\n",
       " 'group': 384,\n",
       " 'bring': 385,\n",
       " 'server': 386,\n",
       " 'sea': 387,\n",
       " 'high': 388,\n",
       " 'outstanding': 389,\n",
       " 'extremely': 390,\n",
       " 'fabulous': 391,\n",
       " 'next': 392,\n",
       " 'brunch': 393,\n",
       " 'happy': 394,\n",
       " 'feel': 395,\n",
       " 'pay': 396,\n",
       " 'size': 397,\n",
       " 'look': 398,\n",
       " 'beautiful': 399,\n",
       " 'seating': 400,\n",
       " 'off': 401,\n",
       " 'horrible': 402,\n",
       " 'fries': 403,\n",
       " 'cod': 404,\n",
       " 'she': 405,\n",
       " 'often': 406,\n",
       " 'yummy': 407,\n",
       " 'kitchen': 408,\n",
       " 'fun': 409,\n",
       " 'mediocre': 410,\n",
       " '?': 411,\n",
       " 'expect': 412,\n",
       " 'soft': 413,\n",
       " 'fare': 414,\n",
       " 'incredible': 415,\n",
       " 'crust': 416,\n",
       " 'sure': 417,\n",
       " 'neighborhood': 418,\n",
       " 'entire': 419,\n",
       " 'spinach': 420,\n",
       " 'friend': 421,\n",
       " 'looked': 422,\n",
       " 'bagel': 423,\n",
       " 'things': 424,\n",
       " 'live': 425,\n",
       " 'tasting': 426,\n",
       " 'variety': 427,\n",
       " 'work': 428,\n",
       " ';': 429,\n",
       " 'cute': 430,\n",
       " 'life': 431,\n",
       " 'pad': 432,\n",
       " 'ny': 433,\n",
       " 'interesting': 434,\n",
       " 'seems': 435,\n",
       " 'deal': 436,\n",
       " 'overpriced': 437,\n",
       " 'waiting': 438,\n",
       " 'crowded': 439,\n",
       " 'bagels': 440,\n",
       " 'bistro': 441,\n",
       " 'may': 442,\n",
       " 'terrible': 443,\n",
       " 'wines': 444,\n",
       " 'house': 445,\n",
       " 'includes': 446,\n",
       " 'offer': 447,\n",
       " 'am': 448,\n",
       " 'family': 449,\n",
       " 'real': 450,\n",
       " 'green': 451,\n",
       " 'reasonably': 452,\n",
       " 'oil': 453,\n",
       " 'regular': 454,\n",
       " 'sashimi': 455,\n",
       " 'sharing': 456,\n",
       " 'part': 457,\n",
       " 'late': 458,\n",
       " 'choices': 459,\n",
       " 'going': 460,\n",
       " 'else': 461,\n",
       " 'tiny': 462,\n",
       " 'given': 463,\n",
       " 'offers': 464,\n",
       " 'pleasant': 465,\n",
       " 'red': 466,\n",
       " 'terrific': 467,\n",
       " 'check': 468,\n",
       " 'poor': 469,\n",
       " 'chinese': 470,\n",
       " 'bottle': 471,\n",
       " 'reviews': 472,\n",
       " 'comfortable': 473,\n",
       " 'duck': 474,\n",
       " 'might': 475,\n",
       " 'keep': 476,\n",
       " 'told': 477,\n",
       " 'coming': 478,\n",
       " 'think': 479,\n",
       " 'waitstaff': 480,\n",
       " 'getting': 481,\n",
       " 'since': 482,\n",
       " '5': 483,\n",
       " 'burgers': 484,\n",
       " '\"': 485,\n",
       " 'party': 486,\n",
       " 'offered': 487,\n",
       " 'return': 488,\n",
       " 'ask': 489,\n",
       " 'impressed': 490,\n",
       " 'found': 491,\n",
       " '15': 492,\n",
       " 'mix': 493,\n",
       " 'owners': 494,\n",
       " '+': 495,\n",
       " 'person': 496,\n",
       " 'creative': 497,\n",
       " 'end': 498,\n",
       " 'bland': 499,\n",
       " 'blue': 500,\n",
       " 'foods': 501,\n",
       " 'complimentary': 502,\n",
       " 'wrong': 503,\n",
       " 'these': 504,\n",
       " 'prepared': 505,\n",
       " 'falafel': 506,\n",
       " 'homemade': 507,\n",
       " 'beers': 508,\n",
       " 'soggy': 509,\n",
       " 'anything': 510,\n",
       " 'door': 511,\n",
       " 'super': 512,\n",
       " 'visit': 513,\n",
       " 'hostess': 514,\n",
       " 'garden': 515,\n",
       " 'close': 516,\n",
       " 'sat': 517,\n",
       " 'something': 518,\n",
       " 'once': 519,\n",
       " 'etc': 520,\n",
       " 'gave': 521,\n",
       " 'cream': 522,\n",
       " 'worst': 523,\n",
       " 'saturday': 524,\n",
       " 'instead': 525,\n",
       " 'spectacular': 526,\n",
       " 'simply': 527,\n",
       " 'disappointed': 528,\n",
       " 'ravioli': 529,\n",
       " 'sometimes': 530,\n",
       " 'personal': 531,\n",
       " 'tastes': 532,\n",
       " 'far': 533,\n",
       " 'jazz': 534,\n",
       " 'classic': 535,\n",
       " 'white': 536,\n",
       " 'location': 537,\n",
       " 'said': 538,\n",
       " 'cozy': 539,\n",
       " 'middle': 540,\n",
       " 'done': 541,\n",
       " 'items': 542,\n",
       " 'easily': 543,\n",
       " 'wife': 544,\n",
       " 'helpful': 545,\n",
       " 'busy': 546,\n",
       " 'except': 547,\n",
       " 'considering': 548,\n",
       " 'anyone': 549,\n",
       " 'sitting': 550,\n",
       " 'spreads': 551,\n",
       " 'rest': 552,\n",
       " 'started': 553,\n",
       " 'pieces': 554,\n",
       " 'stick': 555,\n",
       " 'awesome': 556,\n",
       " 'favorites': 557,\n",
       " 'used': 558,\n",
       " 'ate': 559,\n",
       " 'casual': 560,\n",
       " 'tomato': 561,\n",
       " 'probably': 562,\n",
       " 'until': 563,\n",
       " 'thin': 564,\n",
       " 'mussels': 565,\n",
       " 'plain': 566,\n",
       " 'eaten': 567,\n",
       " 'burger': 568,\n",
       " 'glasses': 569,\n",
       " 'extensive': 570,\n",
       " 'empty': 571,\n",
       " 'let': 572,\n",
       " 'forgot': 573,\n",
       " 'walk': 574,\n",
       " 'completely': 575,\n",
       " 'beat': 576,\n",
       " 'quantity': 577,\n",
       " '100': 578,\n",
       " 'fact': 579,\n",
       " 'cost': 580,\n",
       " 'pie': 581,\n",
       " 'spice': 582,\n",
       " 'rather': 583,\n",
       " 'absolutely': 584,\n",
       " '7': 585,\n",
       " 'long': 586,\n",
       " 'sizes': 587,\n",
       " 'certainly': 588,\n",
       " 'pastrami': 589,\n",
       " 'conversation': 590,\n",
       " 'saw': 591,\n",
       " '3': 592,\n",
       " 'courses': 593,\n",
       " 'old': 594,\n",
       " 'tasted': 595,\n",
       " 'awful': 596,\n",
       " 'felt': 597,\n",
       " 'cake': 598,\n",
       " 'itself': 599,\n",
       " 'street': 600,\n",
       " 'mushrooms': 601,\n",
       " 'another': 602,\n",
       " 'unique': 603,\n",
       " 'add': 604,\n",
       " 'mind': 605,\n",
       " 'arrived': 606,\n",
       " 'selections': 607,\n",
       " 'addition': 608,\n",
       " 'home': 609,\n",
       " 'either': 610,\n",
       " 'lack': 611,\n",
       " 'name': 612,\n",
       " 'sichuan': 613,\n",
       " 'fatty': 614,\n",
       " 'superb': 615,\n",
       " 'available': 616,\n",
       " '1': 617,\n",
       " 'mushroom': 618,\n",
       " 'goat': 619,\n",
       " 'received': 620,\n",
       " 'touch': 621,\n",
       " 'smaller': 622,\n",
       " 'above': 623,\n",
       " 'her': 624,\n",
       " 'servers': 625,\n",
       " 'salty': 626,\n",
       " 'basil': 627,\n",
       " 'delivered': 628,\n",
       " 'summer': 629,\n",
       " 'east': 630,\n",
       " 'fixe': 631,\n",
       " 'use': 632,\n",
       " 'tip': 633,\n",
       " 'eggplant': 634,\n",
       " 'popular': 635,\n",
       " 'pick': 636,\n",
       " 'fancy': 637,\n",
       " 'tempura': 638,\n",
       " 'serving': 639,\n",
       " 'past': 640,\n",
       " 'host': 641,\n",
       " 'ribs': 642,\n",
       " 'lasagna': 643,\n",
       " 'ice': 644,\n",
       " 'presented': 645,\n",
       " 'myself': 646,\n",
       " 'despite': 647,\n",
       " 'per': 648,\n",
       " 'naan': 649,\n",
       " 'meat': 650,\n",
       " 'line': 651,\n",
       " 'town': 652,\n",
       " 'chops': 653,\n",
       " 'chinatown': 654,\n",
       " 'seen': 655,\n",
       " 'setting': 656,\n",
       " 'prix': 657,\n",
       " 'american': 658,\n",
       " 'loud': 659,\n",
       " 'midtown': 660,\n",
       " 'modern': 661,\n",
       " 'dark': 662,\n",
       " 'several': 663,\n",
       " 'twice': 664,\n",
       " 'yes': 665,\n",
       " 'ordering': 666,\n",
       " 'looks': 667,\n",
       " 'vibe': 668,\n",
       " 'traditional': 669,\n",
       " 'care': 670,\n",
       " 'mouth': 671,\n",
       " 'recommended': 672,\n",
       " 'inexpensive': 673,\n",
       " 'evening': 674,\n",
       " 'extra': 675,\n",
       " 'incredibly': 676,\n",
       " 'typical': 677,\n",
       " '*': 678,\n",
       " 'mahi': 679,\n",
       " 'root': 680,\n",
       " 'grilled': 681,\n",
       " 'butter': 682,\n",
       " 'eggs': 683,\n",
       " 'meals': 684,\n",
       " 'hand': 685,\n",
       " 'range': 686,\n",
       " 'whether': 687,\n",
       " 'now': 688,\n",
       " 'included': 689,\n",
       " 'stuff': 690,\n",
       " 'pastas': 691,\n",
       " 'shows': 692,\n",
       " 'toast': 693,\n",
       " 'delicate': 694,\n",
       " 'outdoor': 695,\n",
       " 'village': 696,\n",
       " 'plus': 697,\n",
       " 'wo': 698,\n",
       " 'enjoying': 699,\n",
       " 'beyond': 700,\n",
       " 'week': 701,\n",
       " '!!!': 702,\n",
       " 'soy': 703,\n",
       " 'truffle': 704,\n",
       " 'limited': 705,\n",
       " 'someone': 706,\n",
       " '10': 707,\n",
       " 'spent': 708,\n",
       " 'noodle': 709,\n",
       " 'unless': 710,\n",
       " 'filet': 711,\n",
       " 'leave': 712,\n",
       " 'assortment': 713,\n",
       " 'lovely': 714,\n",
       " 'free': 715,\n",
       " 'beginning': 716,\n",
       " 'scallops': 717,\n",
       " 'thought': 718,\n",
       " 'management': 719,\n",
       " '6': 720,\n",
       " 'soups': 721,\n",
       " 'hip': 722,\n",
       " 'kebabs': 723,\n",
       " 'steaks': 724,\n",
       " 'veggies': 725,\n",
       " 'open': 726,\n",
       " 'through': 727,\n",
       " 'view': 728,\n",
       " 'finally': 729,\n",
       " 'seat': 730,\n",
       " 'pricey': 731,\n",
       " 'wanted': 732,\n",
       " 'barely': 733,\n",
       " 'liked': 734,\n",
       " 'pot': 735,\n",
       " 'squid': 736,\n",
       " 'nights': 737,\n",
       " 'gras': 738,\n",
       " 'shredded': 739,\n",
       " 'shell': 740,\n",
       " 'hang': 741,\n",
       " 'forget': 742,\n",
       " 'delightful': 743,\n",
       " 'consistently': 744,\n",
       " 'put': 745,\n",
       " 'gets': 746,\n",
       " 'calamari': 747,\n",
       " 'birthday': 748,\n",
       " 'piece': 749,\n",
       " 'potato': 750,\n",
       " 'unlike': 751,\n",
       " 'business': 752,\n",
       " 'sliced': 753,\n",
       " 'second': 754,\n",
       " 'call': 755,\n",
       " 'impressive': 756,\n",
       " 'filling': 757,\n",
       " 'customer': 758,\n",
       " 'please': 759,\n",
       " 'slice': 760,\n",
       " 'known': 761,\n",
       " 'secret': 762,\n",
       " 'potatoes': 763,\n",
       " 'breakfast': 764,\n",
       " 'ive': 765,\n",
       " 'par': 766,\n",
       " 'basic': 767,\n",
       " 'lots': 768,\n",
       " 'lemon': 769,\n",
       " 'toppings': 770,\n",
       " 'plan': 771,\n",
       " 'heard': 772,\n",
       " 'cramped': 773,\n",
       " 'fix': 774,\n",
       " 'banana': 775,\n",
       " 'intimate': 776,\n",
       " 'classics': 777,\n",
       " '!!': 778,\n",
       " 'comfort': 779,\n",
       " 'anywhere': 780,\n",
       " 'save': 781,\n",
       " 'picks': 782,\n",
       " 'vegetable': 783,\n",
       " 'juice': 784,\n",
       " 'hard': 785,\n",
       " 'share': 786,\n",
       " 'sides': 787,\n",
       " 'dry': 788,\n",
       " 'aside': 789,\n",
       " 'affordable': 790,\n",
       " 'sent': 791,\n",
       " 'quiet': 792,\n",
       " 'expected': 793,\n",
       " 'relaxed': 794,\n",
       " 'meats': 795,\n",
       " 'making': 796,\n",
       " 'understand': 797,\n",
       " 'filled': 798,\n",
       " '%': 799,\n",
       " 'point': 800,\n",
       " 'accomodating': 801,\n",
       " 'olive': 802,\n",
       " 'clean': 803,\n",
       " 'boyfriend': 804,\n",
       " '8': 805,\n",
       " 'famous': 806,\n",
       " 'scrumptious': 807,\n",
       " 'orders': 808,\n",
       " 'tapas': 809,\n",
       " 'across': 810,\n",
       " 'serve': 811,\n",
       " 'otherwise': 812,\n",
       " 'baked': 813,\n",
       " 'guests': 814,\n",
       " 'changed': 815,\n",
       " 'burnt': 816,\n",
       " 'dressing': 817,\n",
       " 'egg': 818,\n",
       " 'chips': 819,\n",
       " 'sorts': 820,\n",
       " 'eastern': 821,\n",
       " 'medley': 822,\n",
       " 'baclava': 823,\n",
       " 'soooo': 824,\n",
       " 'fooood': 825,\n",
       " 'offering': 826,\n",
       " 'written': 827,\n",
       " 'tell': 828,\n",
       " 'everyone': 829,\n",
       " 'promptly': 830,\n",
       " 'amount': 831,\n",
       " 'matter': 832,\n",
       " 'chance': 833,\n",
       " 'split': 834,\n",
       " 'champagne': 835,\n",
       " 'bacon': 836,\n",
       " 'foie': 837,\n",
       " 'horribly': 838,\n",
       " 'beans': 839,\n",
       " 'relaxing': 840,\n",
       " 'brulee': 841,\n",
       " 'surprise': 842,\n",
       " 'taken': 843,\n",
       " 'udon': 844,\n",
       " 'counter': 845,\n",
       " 'flavors': 846,\n",
       " 'crispy': 847,\n",
       " 'ended': 848,\n",
       " '30': 849,\n",
       " 'evident': 850,\n",
       " 'employees': 851,\n",
       " 'perfection': 852,\n",
       " 'solid': 853,\n",
       " 'interior': 854,\n",
       " 'buffet': 855,\n",
       " 'freshness': 856,\n",
       " '4': 857,\n",
       " 'roast': 858,\n",
       " 'avenue': 859,\n",
       " 'paid': 860,\n",
       " 'choose': 861,\n",
       " 'baby': 862,\n",
       " 'square': 863,\n",
       " 'slightly': 864,\n",
       " 'smile': 865,\n",
       " 'dogs': 866,\n",
       " 'blend': 867,\n",
       " 'delectable': 868,\n",
       " 'hamburger': 869,\n",
       " 'ago': 870,\n",
       " 'fusion': 871,\n",
       " 'guest': 872,\n",
       " 'low': 873,\n",
       " 'congee': 874,\n",
       " 'tea': 875,\n",
       " 'asking': 876,\n",
       " 'packed': 877,\n",
       " 'greens': 878,\n",
       " 'rush': 879,\n",
       " 'trying': 880,\n",
       " 'rushed': 881,\n",
       " 'avocado': 882,\n",
       " 'greatest': 883,\n",
       " 'serves': 884,\n",
       " 'promised': 885,\n",
       " 'pepper': 886,\n",
       " 'guizhou': 887,\n",
       " 'manhattan': 888,\n",
       " 'mom': 889,\n",
       " 'asian': 890,\n",
       " 'relax': 891,\n",
       " 'believe': 892,\n",
       " 'phenomenal': 893,\n",
       " 'veal': 894,\n",
       " 'takeout': 895,\n",
       " 'lox': 896,\n",
       " 'truly': 897,\n",
       " 'feeling': 898,\n",
       " 'various': 899,\n",
       " 'combination': 900,\n",
       " 'lacked': 901,\n",
       " '50': 902,\n",
       " 'diner': 903,\n",
       " 'suggestions': 904,\n",
       " 'suggest': 905,\n",
       " 'mention': 906,\n",
       " '45': 907,\n",
       " 'scene': 908,\n",
       " 'joint': 909,\n",
       " 'phone': 910,\n",
       " 'busboy': 911,\n",
       " 'heavy': 912,\n",
       " 'round': 913,\n",
       " 'cafe': 914,\n",
       " 'girls': 915,\n",
       " 'able': 916,\n",
       " 'type': 917,\n",
       " 'surprisingly': 918,\n",
       " 'cocktail': 919,\n",
       " 'mediterranean': 920,\n",
       " 'chelsea': 921,\n",
       " 'mac': 922,\n",
       " 'own': 923,\n",
       " 'pudding': 924,\n",
       " 'platter': 925,\n",
       " 'together': 926,\n",
       " 'alain': 927,\n",
       " 'ducasse': 928,\n",
       " 'bass': 929,\n",
       " 'waitresses': 930,\n",
       " 'explain': 931,\n",
       " 'caviar': 932,\n",
       " '12': 933,\n",
       " 'lacking': 934,\n",
       " 'five': 935,\n",
       " 'crowd': 936,\n",
       " 'run': 937,\n",
       " 'stay': 938,\n",
       " 'bartenders': 939,\n",
       " 'mozzarella': 940,\n",
       " 'grab': 941,\n",
       " 'walking': 942,\n",
       " 'pancake': 943,\n",
       " 'spend': 944,\n",
       " 'tanks': 945,\n",
       " 'similar': 946,\n",
       " 'works': 947,\n",
       " 'roasted': 948,\n",
       " 'creme': 949,\n",
       " 'fondue': 950,\n",
       " 'tofu': 951,\n",
       " 'platters': 952,\n",
       " 'containers': 953,\n",
       " 'behind': 954,\n",
       " 'section': 955,\n",
       " 'disappointing': 956,\n",
       " 'easy': 957,\n",
       " 'problem': 958,\n",
       " 'sesame': 959,\n",
       " 'types': 960,\n",
       " 'minute': 961,\n",
       " 'called': 962,\n",
       " 'seem': 963,\n",
       " 'sangria': 964,\n",
       " 'sauces': 965,\n",
       " 'prosciutto': 966,\n",
       " 'arugula': 967,\n",
       " 'joints': 968,\n",
       " 'oysters': 969,\n",
       " 'professional': 970,\n",
       " 'level': 971,\n",
       " 'portobello': 972,\n",
       " 'inedible': 973,\n",
       " 'trout': 974,\n",
       " 'assorted': 975,\n",
       " 'unfortunately': 976,\n",
       " 'knows': 977,\n",
       " 'miso': 978,\n",
       " 'pizzas': 979,\n",
       " 'options': 980,\n",
       " 'short': 981,\n",
       " 'refreshing': 982,\n",
       " 'wraps': 983,\n",
       " 'star': 984,\n",
       " 'pita': 985,\n",
       " 'supposed': 986,\n",
       " 'freshest': 987,\n",
       " 'tasteless': 988,\n",
       " 'dine': 989,\n",
       " 'plenty': 990,\n",
       " 'pho': 991,\n",
       " 'tomatoes': 992,\n",
       " 'vegetarian': 993,\n",
       " 'halibut': 994,\n",
       " 'buns': 995,\n",
       " 'tend': 996,\n",
       " 'weekends': 997,\n",
       " 'agree': 998,\n",
       " 'hungry': 999,\n",
       " 'coffee': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bella.data_types import Target\n",
    "rest_dev = []\n",
    "with rest_dev_fp.open('r') as fp:\n",
    "    for line in fp:\n",
    "        target_dict = json.loads(line)\n",
    "        target_dict['spans'][0] = tuple(target_dict['spans'][0])\n",
    "        rest_dev.append(Target(**target_dict))\n",
    "rest_dev = TargetCollection(rest_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to test the affect of Target information in LSTM based models specifically TDLSTM and TCLSTM models of [Tang et al.](https://arxiv.org/abs/1512.01100), for context the TDLSTM model splits the sentence in two contexts the left and right context of the Target word within the text. The left context ends at the right most target word and the right context ends at the left most target word(s). An LSTM encodes the left hand side and a **Backward** LSTM enocodes the right hand side. The idea behind having the Target word(s) as the last word in the LSTM model is so that the model better captures the Target information. The TCLSTM model is an extension of TDLSTM where at each timestep which is a word the information given to the LSTM is the word embedding of that timestep as well as the Target word embedding.\n",
    "\n",
    "1. Position (POS) -- As both models do not encode the full sentence each but encode the Left and Right seperately the LSTM model in essence knows the position of the Target words which is the first type of Target information **Position**.\n",
    "2. Target within text (TWT) -- Both the TDLSTM and TCLSTM, LSTM's last timesteps take are the Target word(s) therefore this is the second type of Target information\n",
    "3. Target At Each Timestep (TAET) -- The TCLSTM LSTM's take as input the word embedding of the timesteps word as well as the Target's embedding this is the third type of Target information.\n",
    "\n",
    "Therefore we are now going to see which combination if not all performs the best. Within the original paper they belive that the TCLSTM model that encodes all three was best but this was shown not to always be the case. Also the original paper never looked at the affect of not having TWT. The combinations we are going to look at are below:\n",
    "1. POS -- Never looked at before\n",
    "2. POS + TWT -- Original TDLSTM method\n",
    "3. POS + TAET -- Never looked at before\n",
    "4. POS + TWT + TAET -- Original TCLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_loader(dataloader_class: Sequence,\n",
    "                          domain_data: Tuple[List[Path], Tokenizer, GloveCommonEmbedding], \n",
    "                          **loader_kwargs) -> List[Sequence]:\n",
    "    data_fps, tok, _ = domain_data\n",
    "    train_fp, val_fp, test_fp = data_fps\n",
    "    train = dataloader_class(train_fp, 32, tok, stanford, \n",
    "                             sort_field='text', **loader_kwargs)\n",
    "    no_transformers = copy.deepcopy(loader_kwargs)\n",
    "    if 'transformers' in no_transformers:\n",
    "        no_transformers.pop('transformers')\n",
    "    val = dataloader_class(val_fp, 32, tok, stanford, \n",
    "                           sort_field=None, **no_transformers)\n",
    "    test = dataloader_class(test_fp, 32, tok, stanford, \n",
    "                            sort_field=None, **no_transformers)\n",
    "    return train, val, test\n",
    "\n",
    "def save_and_predict(model: Model, train: Sequence, val: Sequence, test: Sequence, \n",
    "                     model_path: Path, predict_path: Path, epochs: int = 100):\n",
    "    model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    predict_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    if predict_path.suffix == '':\n",
    "        predict_path = predict_path.with_suffix('.npy')\n",
    "    temp_model_path = model_path\n",
    "    if temp_model_path.suffix == '':\n",
    "        temp_model_path = temp_model_path.with_suffix('.index')\n",
    "    if temp_model_path.exists() and predict_path.exists():\n",
    "        return np.load(predict_path)\n",
    "    print(model_path)\n",
    "    \n",
    "    model_path = str(model_path.resolve())\n",
    "    history = keras_models.fit(model, train, val, model_path, verbose=1, \n",
    "                               use_multiprocessing=True, workers=4, epochs=epochs)\n",
    "    print(history)\n",
    "    predictions = model.predict_generator(test, use_multiprocessing=True, workers=4)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # Map the sentiment back to its original labels\n",
    "    sent_conv = {2: 1, 1: 0, 0: -1}\n",
    "    predictions = np.array([sent_conv[pred] for pred in predictions])\n",
    "    np.save(predict_path, predictions)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def model_creation(model_creation_func, dataloader_class, domain_data, model_path,\n",
    "                   predict_path, num_copies, dataloader_kwargs=None, **model_creation_kwargs):\n",
    "    model_copies = []\n",
    "    if dataloader_kwargs is None:\n",
    "        dataloader_kwargs = {}\n",
    "    for copy_num in range(num_copies):\n",
    "        predict_path_name = f'{predict_path.name} {copy_num}'\n",
    "        temp_predict_path = predict_path.with_name(predict_path_name)\n",
    "        model_path_name = f'{model_path.name} {copy_num}'\n",
    "        temp_model_path = model_path.with_name(model_path_name)\n",
    "        \n",
    "        train, val, test = train_val_test_loader(dataloader_class, \n",
    "                                                 domain_data, **dataloader_kwargs)\n",
    "        domain_embeddings = domain_data[2] \n",
    "        model = model_creation_func(domain_embeddings.embedding, **model_creation_kwargs)\n",
    "        model_copies.append((model, train, val, test, temp_model_path,\n",
    "                             temp_predict_path))\n",
    "    return model_copies\n",
    "\n",
    "def multi_save_and_predict(model_settings, model_dir, predict_dir,\n",
    "                           domain_data, pred_dict, num_times=30, \n",
    "                           dataloader_class=LeftRightTargetSequence,\n",
    "                           **model_kwargs):\n",
    "    for model_type_name, dataloader_kwargs, model_name in model_settings:\n",
    "        model_type_dir = Path(model_dir, model_type_name)\n",
    "        predict_type_dir = Path(predict_dir, model_type_name)\n",
    "\n",
    "        for domain_name, data in domain_data.items():\n",
    "            model_type_path = Path(model_type_dir, f'normal {domain_name}')\n",
    "            predict_type_path = Path(predict_type_dir, f'normal {domain_name}')\n",
    "\n",
    "            model_func = getattr(keras_models, model_name)\n",
    "            model_args = model_creation(model_func, dataloader_class, \n",
    "                                        data, model_type_path, predict_type_path, num_times, \n",
    "                                        dataloader_kwargs=dataloader_kwargs,\n",
    "                                        **model_kwargs)\n",
    "            for model_arg in model_args:\n",
    "                print(model_type_path)\n",
    "                import datetime\n",
    "                print(datetime.datetime.now())\n",
    "                predictions = save_and_predict(*model_arg)\n",
    "                pred_dict[model_type_name][domain_name].append(predictions)\n",
    "    return pred_dict\n",
    "\n",
    "def return_save_data(model_settings, model_dir, predict_dir, num_copies,\n",
    "                     pred_dict, domain_data):\n",
    "    '''\n",
    "    If the data has been saved will return a dictionary of all of the \n",
    "    saved data else it will return False.\n",
    "    '''\n",
    "    for model_type_name, _, _ in model_settings:\n",
    "        model_type_dir = Path(model_dir, model_type_name)\n",
    "        predict_type_dir = Path(predict_dir, model_type_name)\n",
    "\n",
    "        for domain_name, _ in domain_data.items():\n",
    "            model_type_path = Path(model_type_dir, f'normal {domain_name}')\n",
    "            predict_type_path = Path(predict_type_dir, f'normal {domain_name}')\n",
    "            \n",
    "            for copy_num in range(num_copies):\n",
    "                predict_path_name = f'{predict_type_path.name} {copy_num}'\n",
    "                temp_predict_path = predict_type_path.with_name(predict_path_name)\n",
    "                model_path_name = f'{model_type_path.name} {copy_num}'\n",
    "                temp_model_path = model_type_path.with_name(model_path_name)\n",
    "                \n",
    "                temp_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                temp_predict_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    \n",
    "                if temp_predict_path.suffix == '':\n",
    "                    temp_predict_path = temp_predict_path.with_suffix('.npy')\n",
    "                if temp_model_path.suffix == '':\n",
    "                    temp_model_path = temp_model_path.with_suffix('.index')\n",
    "                if temp_model_path.exists() and temp_predict_path.exists():\n",
    "                    predictions = np.load(temp_predict_path)\n",
    "                    pred_dict[model_type_name][domain_name].append(predictions)\n",
    "                    continue\n",
    "                return False\n",
    "    return pred_dict\n",
    "\n",
    "def domain_predictions(model_domain_preds, domain):\n",
    "    '''\n",
    "    Returns only the domain predictions\n",
    "    '''\n",
    "    \n",
    "    model_predictions = {}\n",
    "    for model, domain_predictions in model_domain_preds.items():\n",
    "        for domain_name, predictions in domain_predictions.items():\n",
    "            if domain_name == domain:\n",
    "                model_predictions[model] = predictions\n",
    "    return model_predictions\n",
    "\n",
    "def dict_to_collection(model_preds, domain_dataset):\n",
    "    model_collections = []\n",
    "    for model, preds in model_preds.items():\n",
    "        data = domain_dataset.data()\n",
    "        model_collection = TargetCollection(data, name=model)\n",
    "        model_collection.add_pred_sentiment(np.array(preds).T)\n",
    "        model_collections.append(model_collection)\n",
    "    return model_collections\n",
    "\n",
    "def model_results_prob(result_collections: TargetCollection,\n",
    "                       metrics: List[Tuple[str, \n",
    "                                     Callable[[np.ndarray, np.ndarray], \n",
    "                                             float]]],\n",
    "                       metric_funcs_kwargs: Optional[Dict[str, \n",
    "                                                          Dict[str, Any]]] = None\n",
    "                      ) -> pd.DataFrame:\n",
    "    dfs = []\n",
    "    for collection in result_collections:\n",
    "        name = collection.name\n",
    "        temp_name = name\n",
    "        lower_name = name.lower()\n",
    "        probability = 0\n",
    "        if 'probability' in lower_name:\n",
    "            probability = float(lower_name.split()[-1])\n",
    "            temp_name = name.split()[0]\n",
    "        collection.name = temp_name\n",
    "        additional_data = {temp_name: {'Probability': probability}}\n",
    "        df = datasets_df([collection], metrics, metric_funcs_kwargs, \n",
    "                         additional_data)\n",
    "        collection.name = name\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path('..', 'Data Augmentation', 'Models')\n",
    "predict_dir = Path('..', 'Data Augmentation', 'Predictions')\n",
    "\n",
    "\n",
    "model_domain_predictions = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "model_settings = [('POS', {'include_target_in_batches': False,\n",
    "                           'include_target_in_sequence': False},\n",
    "                   'tdlstm'), \n",
    "                  ('TWT', {'include_target_in_batches': False},\n",
    "                   'tdlstm'), \n",
    "                  ('TAET', {'include_target_in_sequence': False}, \n",
    "                   'tclstm'), \n",
    "                  ('TWT_TAET', None, 'tclstm')]\n",
    "\n",
    "data_saved = return_save_data(model_settings, model_dir,\n",
    "                              predict_dir, 30, model_domain_predictions,\n",
    "                              domain_data)\n",
    "if not data_saved:\n",
    "    model_domain_predictions = multi_save_and_predict(model_settings, model_dir, \n",
    "                                                      predict_dir, domain_data, \n",
    "                                                      model_domain_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the TCLSTM method as it uses TAET this target embedding can have it's own embedding layer rather than at the moment it being shared with the words from the left and right context. The reason this might be desirable is so that the Target information is to some extent seperately encoded so that similar targets are closer together before being merged with the sentiment information.\n",
    "\n",
    "Therefore we are going to train the:\n",
    "1. POS + TAET\n",
    "2. POS + TWT + TAET\n",
    "\n",
    "So that these models first embeded the TAET in a seprate word embedding before being concatenated to the words these models will be called:\n",
    "1. POS + TAET + E\n",
    "2. POS + TWT + TAET + E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_settings = [('TAET_E', {'include_target_in_sequence': False}, \n",
    "                   'tclstm'), \n",
    "                  ('TWT_TAET_E', None, 'tclstm')]\n",
    "\n",
    "data_saved = False\n",
    "data_saved = return_save_data(model_settings, model_dir,\n",
    "                              predict_dir, 30, model_domain_predictions,\n",
    "                              domain_data)\n",
    "if not data_saved:\n",
    "    for domain_name, data in domain_data.items():\n",
    "        temp_domain_data = {f'{domain_name}': data}\n",
    "        domain_embedding = data[2].embedding\n",
    "        model_domain_predictions = multi_save_and_predict(model_settings, model_dir, \n",
    "                                                          predict_dir, temp_domain_data, \n",
    "                                                          model_domain_predictions,\n",
    "                                                          target_embedding=domain_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4IAAAG6CAYAAABdv/iIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzs3X+UXGd95/n3t/UDyTGOjFqGjMqylbS8xlkcM+k1STgEHCJFnbA4k50lciabZiYTZU5iadYOZMwOY4QDGSaTLDstnNkVwUMDAcXxJD4Ko46kBDvkhwmSgxGRjN2FMHY52FbLFlhIRi3pu39UdSi3u6XqH7equur9OqeO6j73ufd+y7K6+nOfe58bmYkkSZIkqXv0tLoASZIkSVJzGQQlSZIkqcsYBCVJkiSpyxgEJUmSJKnLGAQlSZIkqcsYBCVJkiSpyxgEJUmSJKnLGASlOYiIxyLiVESciIinIuKjEXHxPOzzx+erxvnW7vVJktqD35FSezMISnP3v2bmxcB1wGuBd7W4nmlFxOJW1yBJ6ip+R0ptyiAozZPMfArYQ/XLjoh4WUT8dkQ8HhFPR8T/GxHLa+t6I+LTEXE8Ip6NiL+MiJ6I+DiwBviT2hnUX6/1/8Pa2dRvRMRnI+L7J44bEfdHxL+uW357RPxV3XJGxK9GxCgwWmv7LxHxRER8MyIejIg31PXfFhF3R8THIuL5iDgUEf21dVPWJ0nS+fgdKbUfg6A0TyKiBAwA5VrTB4CrqH7p9QGrgdtr634NqACrgFcC/xeQmfl/AI9TO4Oamb9V6z8CrAMuA/4O+P0ZlvfTwOuAa2rL+2t1vQL4JPCHEbGsrv9bgZ3ACmAX8CGqBU5XnyRJ0/I7Umo/BkFp7u6NiOeBJ4BngPdERACbgVsy89nMfB74TWBTbZtx4HuAKzJzPDP/MjNzugNk5l2Z+XxmfhvYBvxARHz3DGr8j7U6TtX294nMPJaZZzLzd4CXAf9TXf+/yszdmXkW+DjwAzM4liRJE/yOlNqUQVCau5/OzJcDbwKuBnqpnsW8CHiwdmnLceBPa+0A/5nqWdG9EXEkIm6bbucRsSgiPhARX4mIbwKP1Vb1zqDGJybt8x0R8XDtMprjwHdP2t9Tde9PAsu8d0KSNAt+R0ptyiAozZPM/Avgo8BvA2PAKeD7M3NF7fXdtRvmqZ25/LXM/F6ql5jcGhFvntjVpF3/HHAj8ONUv4yurLVH7c9vUf1CnfCqqcqbeFO71+HXgbcBl2bmCuAbdfu74EdtsJ8kSYDfkVI7MghK8+v/AdYDrwE+DHwwIi4DiIjVEfETtfdviYi+2uUx3wDOAudq+3ga+N66fb4c+DZwjOqX2W9OOuZDwM9ExEUR0Qf84gVqfDlwBjgKLI6I24FLZvAZJ9cnSVIj/I6U2ohBUJpHmXkU+BjVG97/HdVLWz5Xu1zlz/jOPQbrassngAeA383M+2rr/iPw7trlMu+o7e9rwJPAYeBzkw77QeA01S+fYS58k/weqpfgPFrb7wtMuizmAibXJ0nSBfkdKbWXOM+9t5IkSZKkDuSIoCRJkiR1GYOgJElNFhEbI+KRiChPNSNiRHwwIh6qvR6tzVxIRFwXEQ/UHmJ9MCJ+tvnVS5I6gZeGSpLURBGxiOr9R+upPjR7P3BTZh6epv8W4LWZ+a8i4iqqD9YejYh/AjwIvDozjzepfElSh3BEUJKk5roeKGfmkcw8DeykOv39dG4CPgWQmY9m5mjt/T9QfUD3qvNsK0nSlDrm4Ze9vb155ZVXtroMSVITPPjgg2OZuVAD0GpePAthBXjdVB0j4gpgLfCZKdZdDywFvjLNtpuBzQDf9V3f9YNXX3313KqWJLW9mXw/dkwQvPLKKzlw4ECry5AkNUFEfK3VNTTJJuCezDxb3xgR3wN8HBjMzHNTbZiZO4AdAP39/el3pCR1vpl8P3ppqCRJzfUkcHndcqnWNpVN1C4LnRARlwD/A/j3mTn5mWmSJDXEIChJUnPtB9ZFxNqIWEo17O2a3CkirgYupfpA7Ym2pcAfAx/LzHuaVK8kqQMZBCVJaqLMPAPcDOwBHgbuzsxDEXFHRLy1rusmYGe+eHrvtwE/Cry97vES1zWteElSx+iYewQlSVooMnM3sHtS2+2TlrdNsd0ngE8UWpwkqSs4IihJkiRJXcYgKEmSJEldxiAoSZIkSV3GIChJkiRJXcYgKEmSJEldxiAoSZIkSV3GIChJkiRJXcYgKEmSJEldxiA4D8bGxtiyZQvHjh1rdSmSJLUVvyMlqT0ZBOfB8PAwBw8eZHh4uNWlSJLUVvyOlKT2ZBCco7GxMUZGRshMRkZGPOMpSVKN35GS1L4MgnM0PDxMZgJw7tw5z3hKklTjd6QktS+D4Bzt27eP8fFxAMbHx9m7d2+LK5IkqT34HSlJ7csgOEfr169nyZIlACxZsoQNGza0uCJJzeAEGNKF+R0pSe3LIDhHg4ODRAQAPT09DA4OtrgiSc3gBBjShfkdKUntyyA4R729vQwMDBARDAwMsHLlylaXJKlgToAhNcbvSElqXwbBeTA4OMi1117rmU6pSzgBhtQ4vyMlqT0ZBOdBb28v27dv90yn1CWcAENqnN+RktSeCg2CEbExIh6JiHJE3DbF+jURcV9EfCEiDkbET9ate1dtu0ci4ieKrFOSZsIJMCRJmpqTqS0chQXBiFgE3AkMANcAN0XENZO6vRu4OzNfC2wCfre27TW15e8HNgK/W9ufJLWcE2BIkjQ1J1NbOIocEbweKGfmkcw8DewEbpzUJ4FLau+/G/iH2vsbgZ2Z+e3M/CpQru1PklrOCTAkSXopJ1NbWIoMgquBJ+qWK7W2etuAn4+ICrAb2DKDbYmIzRFxICIOHD16dL7qlqQLcgIMSZJezMnUFpZWTxZzE/DRzCwBPwl8PCIarikzd2Rmf2b2r1q1qrAiJWkyJ8CQJOnFnExtYSkyCD4JXF63XKq11ftF4G6AzHwAWAb0NritJEmSpDbhZGoLS5FBcD+wLiLWRsRSqpO/7JrU53HgzQAR8WqqQfBord+miHhZRKwF1gGfL7BWSZIkSXPgZGoLy+KidpyZZyLiZmAPsAi4KzMPRcQdwIHM3AX8GvDhiLiF6sQxb8/qhcWHIuJu4DBwBvjVzDxbVK2SutvQ0BDlcnlG21QqFQBKpVLD2/T19bF169YZHUeSpIViYjK1Xbt2OZnaAlBYEATIzN1UJ4Gpb7u97v1h4PXTbPt+4P1F1idJs3Xq1KlWlyBJUtsZHBzkscceczRwASg0CErSQjCbUbqJbYaGhua7HEmSFqyJydTU/lo9a6gkSZIkqckMgpIkSZLUZQyCkiRJktRlDIKSJEmS1GUMgpIkSZLUZQyCkiRJktRlDIKSJEmS1GV8jqAkSZKkKQ0NDVEulxvuX6lUACiVSjM6Tl9f36ye66vZMwhKkiRJmhenTp1qdQlqkEFQkiRJ0pRmOko30X9oaKiIcjSPvEdQkiRJkrqMQVCSJEmSuoxBUJIkSZK6jEFQkiRJkrqMQVCSJEmSuoxBUJKkJouIjRHxSESUI+K2KdZ/MCIeqr0ejYjjdesGI2K09hpsbuWSpE7h4yMkSWqiiFgE3AmsByrA/ojYlZmHJ/pk5i11/bcAr629fwXwHqAfSODB2rbPNfEjSJI6gCOCkiQ11/VAOTOPZOZpYCdw43n63wR8qvb+J4B9mflsLfztAzYWWq0kqSMZBCVJaq7VwBN1y5Va20tExBXAWuAzs9h2c0QciIgDR48enXPRkqTOYhCUJKl9bQLuycyzM90wM3dkZn9m9q9ataqA0iRJC5lBUJKk5noSuLxuuVRrm8omvnNZ6Ey3lSRpWgZBSZKaaz+wLiLWRsRSqmFv1+ROEXE1cCnwQF3zHmBDRFwaEZcCG2ptkiTNiLOGSpLURJl5JiJuphrgFgF3ZeahiLgDOJCZE6FwE7AzM7Nu22cj4jeohkmAOzLz2WbWL0nqDAZBSZKaLDN3A7sntd0+aXnbNNveBdxVWHGSpK7gpaGSJEmS1GUMgpIkSZLUZQyCkiRJktRlDIKSJEmS1GUMgpIkSZLUZZw1VJIkNWRoaIhyuTyjbSqVCgClUqnhbfr6+ti6deuMjiNJmhmDoCRJKsypU6daXYIkaQoGQUmS1JDZjNJNbDM0NDTf5UiS5sB7BCVJkiSpyxgEJUmSJKnLGAQlSZIkqcsYBCVJkiSpyzhZzCTNmhobnB5bkiRJUmsYBOeBU2NLkiRJWkgMgpM4NbYkSZKkTuc9gpIkSZLUZQyCkiRJktRlvDRUqnGiIEmSJHULg6A0B04UJEmSpIXIICjVOFGQJEmSuoX3CEqSJElSlyk0CEbExoh4JCLKEXHbFOs/GBEP1V6PRsTxunX/KSL+vvb62SLrlCRJkqRuUtiloRGxCLgTWA9UgP0RsSszD0/0ycxb6vpvAV5be/9TwD8FrgNeBtwfESOZ+c2i6pUkSdLUmjWhmpOpSc1T5Ijg9UA5M49k5mlgJ3DjefrfBHyq9v4a4LOZeSYzvwUcBDYWWKskSZLm0alTp5xUTWpjRU4Wsxp4om65Arxuqo4RcQWwFvhMremLwHsi4neAi4AbgMNTbLcZ2AywZs2aeStckiRJ3+GEalLnaZfJYjYB92TmWYDM3AvsBv6G6ijhA8DZyRtl5o7M7M/M/lWrVjWzXkmSJElasIoMgk8Cl9ctl2ptU9nEdy4LBSAz35+Z12XmeiCARwupUpIkSZK6TJFBcD+wLiLWRsRSqmFv1+ROEXE1cCnVUb+JtkURsbL2/lrgWmBvgbVKkiRJUtco7B7BzDwTETcDe4BFwF2ZeSgi7gAOZOZEKNwE7MzMrNt8CfCXEQHwTeDnM/NMUbVKkiRJUjcpcrIYMnM31Xv96ttun7S8bYrtXqA6c6gkSZIkaZ61y2QxkiRJkqQmMQhKkiRJUpcxCEqSJElSlzEISpIkSVKXMQhKkiRJUpcxCEqSJElSlzEISpIkSVKXMQhKkiRJUpcxCEqSJElSlzEISpIkSVKXMQhKkiRJUpcxCEqSJElSlzEISpIkSVKXMQhKktRkEbExIh6JiHJE3DZNn7dFxOGIOBQRn6xr/61a28MRMRQR0bzKJUmdYnGrC5AkqZtExCLgTmA9UAH2R8SuzDxc12cd8C7g9Zn5XERcVmv/EeD1wLW1rn8FvBG4v3mfQJLUCRwRlCSpua4Hypl5JDNPAzuBGyf1+SXgzsx8DiAzn6m1J7AMWAq8DFgCPN2UqiVJHcURQUkdZWhoiHK5XPhxRkdHAdi6dWuhx+nr6yv8GGq61cATdcsV4HWT+lwFEBF/DSwCtmXmn2bmAxFxH/B1IIAPZebDUx0kIjYDmwHWrFkzv59AkrTgGQQldZRyucyhLz3MiosuK/Q4505Xb8t68ivHCjvG8ZPPXLiTOtViYB3wJqAEfDYiXgP0Aq+utQHsi4g3ZOZfTt5BZu4AdgD09/dnM4qWJC0cBkFJHWfFRZdxw9WbWl3GnN335Z2tLkHFeBK4vG65VGurVwH+NjPHga9GxKN8Jxh+LjNPAETECPDDwEuCoCRJ5+M9gpIkNdd+YF1ErI2IpcAmYNekPvdSDX1ERC/VS0WPAI8Db4yIxRGxhOpEMVNeGipJ0vkYBCVJaqLMPAPcDOyhGuLuzsxDEXFHRLy11m0PcCwiDgP3Ae/MzGPAPcBXgC8BXwS+mJl/0vQPIUla8Lw0VJKkJsvM3cDuSW23171P4Nbaq77PWeCXm1GjJKmzdXwQbMYMgs2aPRCcQVCSJEmz4+/FqtfxQbBcLvOFLx3m3EWvKOwYcbo6GduDX3mqsGMA9Jx8ttD9S5IkqXM1Y2btZsyqDc6sPR86PggCnLvoFbxwzVtaXcacLTv86VaXIEmSpAXMmbU1wcliJEmSJKnLdMWIoLpTJ10H7zXwkiRJmk8GQXWscrnMFw59AVYUeJBz1T++8OQXijvG8eJ2LUmSpO5kEFRnWwHn3nSu1VXMSc/97XkF99jYGO9973vZtm0bK1eubHU5kmaoGVdNQOdcOTGb/16VSgWAUqk0o+28CkRSMxgEJc3K8PAwBw8eZHh4mFtvvfXCG0hqK82YVRuaM7N2u86qferUqVaXIEnTMghKmrGxsTFGRkbITEZGRhgcHHRUUFqAnFW7cbMZoZvYZmhoaL7LkaQ5MwhKmrHh4WEyq2f5z50756igJC0gXhYsCQyCkmZh3759jI+PAzA+Ps7evXsNgpK0QDRlMjVwQjWpzRkEJc3Y+vXr2b17N+Pj4yxZsoQNGza0uiRJ0kx0wGRq0L4TqkkLgf96JM3Y4OAgEQFAT08Pg4ODLa5IkiRJM2EQlDRjvb29DAwMEBEMDAw4UYwkSdIC46WhkmZlcHCQxx57rO1GAyuVCt84+Tz3fXlnq0uZs+MnnyErTj8vSZLmn0FQ0qz09vayffv2VpchSZKkWTAISuoopVKJ+PYxbrh6U6tLmbP7vryT1SUvu5UkSfPPewQlSZIkqcsYBCVJkqR5NDY2xpYtWzh27FirS5GmZRCUJEmS5tHw8DAHDx5keHi41aVI0zIISpIkSfNkbGyMkZERMpORkRFHBdW2On6ymEqlQs/Jb7Ds8KdbXcqc9Zw8RqVyptVlSJIkaRrDw8NkJgDnzp1jeHiYW2+9tcVVSS/liKAkSZI0T/bt28f4+DgA4+Pj7N27t8UVSVMrdEQwIjYC/wVYBPxeZn5g0voPAjfUFi8CLsvMFbV1vwX8FNWwug/4tzlxemUGSqUST397MS9c85bZf5A2sezwpymVXtXqMiRJkjSN9evXs3v3bsbHx1myZAkbNmxodUnSlAoLghGxCLgTWA9UgP0RsSszD0/0ycxb6vpvAV5be/8jwOuBa2ur/wp4I3B/UfWqamhoiHK5PKNtKpUKUA3djerr62Pr1q0zOs5MVSoV+Ab03L/AB76PQyUrra5CkiQ1YHBwkJGREQB6enoYHBxscUXS1Ir8Dfl6oJyZRzLzNLATuPE8/W8CPlV7n8AyYCnwMmAJ8HSBtWoOTp06xalTp1pdhiRJUsv19vYyMDBARDAwMMDKlStbXZI0pSIvDV0NPFG3XAFeN1XHiLgCWAt8BiAzH4iI+4CvAwF8KDMfLrBW1cxmlG5im6GhofkuZ05KpRJH4yjn3nSu1aXMSc/9PZRWNz7aKkmSWmtwcJDHHnvM0UC1tXa5Zm4TcE9mngWIiD7g1UCJaqD8sYh4w+SNImJzRByIiANHjx5tasGSJEnSVHp7e9m+fbujgWprRQbBJ4HL65ZLtbapbOI7l4UC/DPgc5l5IjNPACPAD0/eKDN3ZGZ/ZvavWrVqnsqWJEmSpM5WZBDcD6yLiLURsZRq2Ns1uVNEXA1cCjxQ1/w48MaIWBwRS6hOFOOloZIkSZI0DwoLgpl5BrgZ2EM1xN2dmYci4o6IeGtd103AzkmPhrgH+ArwJeCLwBcz80+KqlWSJEmSukmhzxHMzN3A7kltt09a3jbFdmeBXy6yNkmSJEnqVu0yWYwkSZIkqUkMgpIkSZLUZQyCkiRJktRlDIKSJEmS1GUKnSxGkiRJUnuoVCp84+Tz3Pflna0uZc6On3yGrJxqdRkLmiOCkiRJktRlHBGUJKkLVSoVek5+g2WHP93qUuas5+QxKpUzrS5DanulUon49jFuuHpTq0uZs/u+vJPVpZWtLmNBc0RQkiRJkrqMI4KSJHWhUqnE099ezAvXvKXVpczZssOfplR6VavLkKQFxRFBSZIkSeoyjghKkiR1kUqlAt+Anvs7YDzgOFSy0uoqpAWpA34CSJK0sETExoh4JCLKEXHbNH3eFhGHI+JQRHyyrn1NROyNiIdr669sVt2SpM7hiKAkSU0UEYuAO4H1QAXYHxG7MvNwXZ91wLuA12fmcxFxWd0uPga8PzP3RcTFwLkmlq8OUCqVOBpHOfemhf+/Ts/9PZRWl1pdhrQgOSIoSVJzXQ+UM/NIZp4GdgI3TurzS8CdmfkcQGY+AxAR1wCLM3Nfrf1EZp5sXumSpE5hEJQkqblWA0/ULVdqbfWuAq6KiL+OiM9FxMa69uMR8UcR8YWI+M+1EUZJkmbES0MldZzjJ5/hvi/vLPQYJ154DoCLl11a2DGOn3yG1fiw3C61GFgHvAkoAZ+NiNfU2t8AvBZ4HPgD4O3ARybvICI2A5sB1qxZ04yaJUkLSFcEwZ6Tz7Ls8KcL23+88E0ActklhR0Dqp8DfE7SjBwveFa0E7U/Ly7uEBznpWMFmlZfX19TjjM6+iwAq7+vuKC2mpVN+zxqqieBy+uWS7W2ehXgbzNzHPhqRDxKNRhWgIcy8whARNwL/BBTBMHM3AHsAOjv78/5/hCSpIWt44NgM36JGh19HoB131d0SHuVvxTOQHP+7kcBWLd6XXEHWd28cNMJtm7d2tTjDA0NNeV46ij7gXURsZZqANwE/NykPvcCNwH/LSJ6qV4SeoTqqaEVEbEqM48CPwYcaFrlkqSO0fFBsBm/FPoLYXvy715SO8rMMxFxM7AHWATclZmHIuIO4EBm7qqt2xARh4GzwDsz8xhARLwD+POICOBB4MMt+SCSpAWt44OgJEntJjN3A7sntd1e9z6BW2uvydvuA64tukZJUmdz1lBJkiRJ6jIGQUmSJEnqMgZBSZIkSeoy3iMoSZIkdYmin7XbjOfsgs/anQ8GQUmSJKkLNOfRWsU/Zxd81u58MAhKkiRJXcBHa6meQVASUP2BXS6XG+5fqVQAKJVKMzpOX19f0x76LkmSpKkZBCXNyqlTp1pdgiRJkmbpgkEwIrYAn8jM55pQj6QWmekonZd+SJIkLVyNPD7ilcD+iLg7IjZGRBRdlCRJkiSpOBcMgpn5bmAd8BHg7cBoRPxmRHxfwbVJkiRJkgrQ0APlMzOBp2qvM8ClwD0R8VsF1iZJkiRJKkAj9wj+W+AXgDHg94B3ZuZ4RPQAo8CvF1uiJEmSJGk+NTJr6CuAn8nMr9U3Zua5iHhLMWVJkiRJkorSSBAcAZ6dWIiIS4BXZ+bfZubDhVUmSZKkYhyHnvsbukNo9k7U/ry4wGMcB1YXuH9m/pxd8Fm7WhgaCYL/FfindcsnpmiTJEnSAtDX19eU44yOjgKwbvW64g6yunmfZyZ81q4WgkaCYNQmiwH+8ZJQH0QvSZK0ADVrxKlTnjc7m/9enfLZ1dkaCXRHImIr1VFAgF8BjhRXkubLbC5lmI2JM35Ff7F4uYQkza+ek8+y7PCnCz1GvPBNAHLZJYUdo+fks8CrCtu/JHWiRoLgvwGGgHcDCfw5sLnIojQ/yuUyj/7937Hm4rOFHmfpePUegxce21/YMR4/saiwfUtSN2re5YHPA7Du+4oMaq9qy8sDJamdXTAIZuYzwKYm1KICrLn4LO/uP3Hhjm3ufQeKvNNckrqPlwdKUndr5DmCy4BfBL4fWDbRnpn/qsC6JEmSJEkFaWTe4I9TvfD+J4C/AErA80UWJUmSJEkqTiNBsC8z/wPwrcwcBn4KeF2xZUmSJEmSitLIZDHjtT+PR8T/DDwFXFZcSZIkScVqxszazqotqZ01EgR3RMSlVGcN3QVcDPyHQquSJEkqUDNm1nZWbUnt7LxBMCJ6gG9m5nPAZ4HvbUpVUgvM5uzwbM/2evZWklqvE2bWdlZtSbN13nsEM/Mc8Ouz3XlEbIyIRyKiHBG3TbH+gxHxUO31aEQcr7XfUNf+UES8EBE/Pds6pKIsX76c5cuXt7oMSW0mIq5udQ2SJJ1PI5eG/llEvAP4A+BbE42Z+ez5NoqIRcCdwHqgAuyPiF2ZebhuH7fU9d8CvLbWfh9wXa39FUAZ2NvgZ5JmxRE6SfNoL7Cm1UVIkjSdRoLgz9b+/NW6tuTCl4leD5Qz8whAROwEbgQOT9P/JuA9U7T/c2AkM082UKskSU0REdM9IT2AFc2sRZKkmbpgEMzMtbPc92rgibrlCtM8diIirgDWAp+ZYvUm4P+eZrvNwGaANWs88SpJaqp/Cfwa8O0p1t3U5FokSZqRCwbBiPiFqdoz82PzWMcm4J7MfNHUXRHxPcBrgD3T1LAD2AHQ39+f81iPJEkXsh/4+8z8m8krImJb88uRJKlxjVwa+r/UvV8GvBn4O+BCQfBJ4PK65VKtbSqbePGlpxPeBvxxZo5PsU6SpFb658ALU62Yw9U0kiQ1RSOXhm6pX46IFcDOBva9H1gXEWupBsBNwM9N7lSbWe1S4IEp9nET8K4GjiVJUrNdfKGJ0yRJalfnfXzENL5F9X6+88rMM8DNVC/rfBi4OzMPRcQdEfHWuq6bgJ2Z+aJLOyPiSqojin8xixolSSravRNvIuK/t7IQSZJmqpF7BP+E6iyhUA2O1wB3N7LzzNwN7J7Udvuk5W3TbPsY1QlnJElqR1H3/kIzaUuS1FYauUfwt+venwG+lpmVguqRJGmhyGneS5LU9hoJgo8DX8/MFwAiYnlEXFkbsZMkqVv9QER8k+rI4PLae2rLmZmXtK40SZLOr5F7BP8QOFe3fLbWJklS18rMRZl5SWa+PDMX195PLBsCJUltrZEguDgzT08s1N4vLa4kSZIkSVKRGgmCR+tn+YyIG4Gx4kqSJEmSJBWpkXsE/w3w+xHxodpyBfiF4kqSJEmSJBWpkQfKfwX4oYi4uLZ8ovCqJEmSJEmFueCloRHxmxGxIjNPZOaJiLg0It7XjOIkSZIkSfOvkXsEBzLz+MRCZj4H/GRxJUmSJEmSitRIEFwUES+bWIiI5cDLztNfkiSdR0RsjIhHIqIcEbdN0+dtEXE4Ig5FxCcnrbskIip19+9LkjQjjUwW8/vAn0fEf6P6kNy3A8NFFiVJap6hoSHK5XLD/SuVCgClUmlGx+nr62Pr1q0z2qYTRcQi4E5gPdUJ2PZHxK7MPFzXZx3wLuD1mflcRFw2aTe/AXy2WTVLkjpPI5PF/KeI+CLw40ACe4Arii5MktSeTp061eoSFrrrgXJmHgGIiJ3AjcDhuj6/BNxZux2DzHxmYkVE/CDwSuBPgf5mFS1J3WamJ0phdidLW3WitJERQYCnqYbA/x34KvDfC6tI86ZSqfBkqkzPAAAfWklEQVSt5xfxvgMXt7qUOfva84v4rto/LEnza6ZfPhP9h4aGiiinG6wGnqhbrgCvm9TnKoCI+GtgEbAtM/80InqA3wF+nuoJ2mlFxGZgM8CaNWvmp3JJ0nktpJOl0wbBiLgKuKn2GgP+AIjMvKFJtUmS1K0WA+uANwEl4LMR8RqqAXB3ZlYi4rw7yMwdwA6A/v7+LLRaSepAsxmlW0gnS883Ivhl4C+Bt2RmGSAibmlKVZoXpVKJF858nXf3L/xHP77vwMUsm+H9SJLUpp4ELq9bLtXa6lWAv83MceCrEfEo1WD4w8AbIuJXgIuBpRFxIjOnnHBGkqTpnG/W0J8Bvg7cFxEfjog3U50sRpIkzd5+YF1ErI2IpcAmYNekPvdSHQ0kInqpXip6JDP/RWauycwrgXcAHzMESpJmY9ogmJn3ZuYm4GrgPuD/BC6LiP8aERuaVaAkSZ0kM88AN1OdfO1h4O7MPBQRd0TEW2vd9gDHIuIw1e/gd2bmsdZULEnqRI3MGvot4JPAJyPiUqoTxvw7YG/BtUmS1JEyczewe1Lb7XXvE7i19ppuHx8FPlpMhZKkTtfIA+X/UWY+l5k7MvPNRRUkSZIkSSpWo4+P6BqzeV7I6OgoMPOZhXy4siRJkqRWMAjOg+XLl7e6BEmSJElqmEFwEkfoJEmSJHW6Gd0jKEmSJEla+BwRlCRJXadSqfCt5xfxvgMXt7qUOfna84v4rkql1WVIWoAcEZQkSZKkLuOIoCRJ6jqlUokXznydd/efaHUpc/K+AxezrFRqdRmSFiBHBCVJkiSpyxgEJUmSJKnLGAQlSZIkqct4j6Ckrjc0NES5XJ7RNqOjo8DMnj3a19fns0olSVJbMAhK0iwsX7681SVIkmZhNif/Zmo2JwtnwxOMmguDoKSu55eoJHWPcrnMlx96iFcVeIyJe6+OP/RQYcd4qrA9q1sYBCVJktRVXgX8ItHqMubkI2SrS9AC52QxkiRJktRlHBGUOlAn3f8A3gMhSZI03wyCUgfqlPsfwHsgJEmSimAQlOZgbGyM9773vWzbto2VK1e2upwX6YT7H8B7ICRJaqWZXmU02yuGvPqn+bxHUJqD4eFhDh48yPDwcKtLkSRJarnly5f7iKUFwhFBaZbGxsYYGRkhMxkZGWFwcLDtRgUlSZLmwlG6zuWIoDRLw8PDZFYvWzx37pyjgpIkSVowDILSLO3bt4/x8XEAxsfH2bt3b4srkiRJkhpjEJRmaf369SxeXL26evHixWzYsKHFFUmSJEmNMQhKszQ4OMi5c+eA6qWhg4ODLa5IkiRJaoxBUJIkSZK6jEFQmqXh4WF6eqr/hHp6epwsRpIkSQuGQVCapX379nHmzBkAzpw542QxkiRJWjAKDYIRsTEiHomIckTcNsX6D0bEQ7XXoxFxvG7dmojYGxEPR8ThiLiyyFqlmVq/fj1LliwBYMmSJU4WI0mSpAWjsAfKR8Qi4E5gPVAB9kfErsw8PNEnM2+p678FeG3dLj4GvD8z90XExcC5omrtZI+fWMT7Dlxc6DGePlk9n/DKi4r7K3r8xCKuKmzvszM4OMjIyAhQvTTUyWIkSZK0UBQWBIHrgXJmHgGIiJ3AjcDhafrfBLyn1vcaYHFm7gPIzBMF1tmx+vr6mnKc06OjACy7cl1hx7iK5n2eRvX29jIwMMCuXbsYGBhg5cqVrS5JkiRJakiRQXA18ETdcgV43VQdI+IKYC3wmVrTVcDxiPijWvufAbdl5tlJ220GNgOsWbNmXovvBFu3bm3qcYaGhppyvHYyODjIY4895mig2sbQ0BDlcrnQY4zWTv4042dMX19f036WSZLUTYoMgjOxCbinLugtBt5A9VLRx4E/AN4OfKR+o8zcAewA6O/vz2YVK03o7e1l+/btrS5D+kflcpm//+IXefnS4n68nzlT/VH9tYcPFXYMgOdPnyl0/5Kk7tGME6XQvJOl83GitMgg+CRwed1yqdY2lU3Ar9YtV4CH6i4rvRf4ISYFQUnSS7186WKuf+WlrS5jzj7/9HOtLkGS1CGacaIUmnOydL5OlBb5X2I/sC4i1lINgJuAn5vcKSKuBi4FHpi07YqIWJWZR4EfAw4UWKskSZK6QKVS4XngIyzsi8m+DpyoVFpdxoLiidIXK+zxEZl5BrgZ2AM8DNydmYci4o6IeGtd103AzszMum3PAu8A/jwivgQE8OGiapUkSZKkblLo2Ghm7gZ2T2q7fdLytmm23QdcW1hxUgfrlLOd4BlPSdL8KpVKHB8b4xeJVpcyJx8hWVEqtboMLWCFPlBekiRJktR+2mXWUEnzqFPOdoJnPCVJkorgiKAkSZIkdRlHBCVJUld6/MQi3nfg4sL2//TJ6vn2V150rrBjPH5iEVcVtvfvmM0z2GbzPLX5eDaapMYYBCVJUtfp6+sr/Bina0Fo2ZXrCjvGVTTns8zG8uXLW12CpPMwCEqSpK7TjFGniWMMDQ0VfqyiOUondR7vEZQkqckiYmNEPBIR5Yi4bZo+b4uIwxFxKCI+WWu7LiIeqLUdjIifbW7lkqRO4YigJElNFBGLgDuB9UAF2B8RuzLzcF2fdcC7gNdn5nMRcVlt1UngFzJzNCL+CfBgROzJzONN/hiSpAXOEUFJkprreqCcmUcy8zSwE7hxUp9fAu7MzOcAMvOZ2p+PZuZo7f0/AM8Aq5pWuSSpYxgEJUlqrtXAE3XLlVpbvauAqyLiryPicxGxcfJOIuJ6YCnwlakOEhGbI+JARBw4evToPJUuSeoUBkFJktrPYmAd8CbgJuDDEbFiYmVEfA/wceBfZuaUzybIzB2Z2Z+Z/atWOWgoSXoxg6AkSc31JHB53XKp1lavAuzKzPHM/CrwKNVgSERcAvwP4N9n5ueaUK8kqQMZBCVJaq79wLqIWBsRS4FNwK5Jfe6lOhpIRPRSvVT0SK3/HwMfy8x7mleyJKnTGAQlSWqizDwD3AzsAR4G7s7MQxFxR0S8tdZtD3AsIg4D9wHvzMxjwNuAHwXeHhEP1V7XteBjSJIWOB8fIUlSk2XmbmD3pLbb694ncGvtVd/nE8AnmlGjJKmzGQQlSVJDhoaGKJfLM9pmdHQUgK1btza8TV9f34z6S5JmziAoSZIKs3z58laXIEmagkFQkiQ1xFE6SeocThYjSZIkSV3GIChJkiRJXcYgKM3B2NgYW7Zs4dixY60uRZIkSWqYQVCag+HhYQ4ePMjw8HCrS5EkSZIaZhCUZmlsbIyRkREyk5GREUcFJUmStGA4a6g0S8PDw1Sf+Qznzp1jeHiYW2+99QJbScWqVCo8f/oMn3/6uVaXMmfPnz5DpVJpdRmSJHUkg6A0S/v27WN8fByA8fFx9u7daxCUJElqQ54ofSmDoDRL69evZ/fu3YyPj7NkyRI2bNjQ6pIkSqUSZ5//Bte/8tJWlzJnn3/6OUqlUqvLkCSpIxkEpVkaHBxkZGQEgJ6eHgYHB1tckSRJkqbiidKXcrIYaZZ6e3sZGBggIhgYGGDlypWtLkmSJElqiEFQmoPBwUGuvfZaRwMlaRo+b1WS2pNBUJqD3t5etm/f7migJE3D561KUnsyCEqSpEL4vFVJal8GQUmSVIipnrcqSWoPBkFJklSIqZ63KklqDwZBSZJUiPXr17NkyRIAn7cqSW3GIChJkgoxODhIRAA+b1WS2o0PlJckSYXo7e3lhhtuYM+ePdxwww3OsKy28RTwEbKw/U9Mi1Tk//FPASsK3L86n0FQkjrM86fP8Pmnnyts/yfPnAXgosWLCjsGVD+HJM23vr6+wo9xdHQUgBXr1hV2jBU057OocxkEJamDNOOXgtHaLzhXFPgLzgR/yVnYxsbGuO+++wC47777+OVf/mVHBbvI2NgY733ve9m2bVtb/b1v3bq1accYGhoq/FjSbBkEJamD+AuO2slUj4+49dZbW1yVmmV4eJiDBw/69y61KSeLkSRJhfDxEd1rbGyMkZERMpORkRGOHTt24Y0kNZVBUJIkFcLHR3SvqUaDJbUXg6AkSSqEj4/oXo4GS+3PIChJkgrR29vLwMAAEcHAwEBbTRiiYjkaLLU/g6AkSSrM4OAg1157raOBXcbRYKn9GQQlSVJhent72b59u6OBXcbRYKn9FRoEI2JjRDwSEeWIuG2K9R+MiIdqr0cj4njdurN163YVWackSZLml6PBUnsr7DmCEbEIuBNYD1SA/RGxKzMPT/TJzFvq+m8BXlu3i1OZeV1R9UmSJKk4E6PBUrt4/vQZPv/0c4Ue4+SZswBctHhRYcd4/vSZedlPkQ+Uvx4oZ+YRgIjYCdwIHJ6m/03AewqsR+oqTwEfIQvb/8QToYq+2OcpYEXBx5AkSZ2tr6+vKccZHR0F4Ip16wo9znx8niKD4GrgibrlCvC6qTpGxBXAWuAzdc3LIuIAcAb4QGbeO8V2m4HNAGvWrJmnsqWFrxk/7I7WftCtKPgH3Qqa98NbkiR1pq1btzb1OENDQ0053lwUGQRnYhNwT2aerWu7IjOfjIjvBT4TEV/KzK/Ub5SZO4AdAP39/cUNfUgLTDN+2C2kH3SSJEl6sSIni3kSuLxuuVRrm8om4FP1DZn5ZO3PI8D9vPj+QUmSJEnSLBUZBPcD6yJibUQspRr2XjL7Z0RcDVwKPFDXdmlEvKz2vhd4PdPfWyhJkiRJmoHCLg3NzDMRcTOwB1gE3JWZhyLiDuBAZk6Ewk3Azsysv7Tz1cD/FxHnqIbVD9TPNipJkiRJmr1C7xHMzN3A7kltt09a3jbFdn8DvKbI2iRJkiSpWxX6QHlJkiRJUvsxCEqSJElSlzEISpIkSVKXMQhKkiRJUpcxCEqS1GQRsTEiHomIckTcNk2ft0XE4Yg4FBGfrGsfjIjR2muweVVLkjpJobOGSpKkF4uIRcCdwHqgAuyPiF31j0mKiHXAu4DXZ+ZzEXFZrf0VwHuAfiCBB2vbPtfszyFJWtgcEZQkqbmuB8qZeSQzTwM7gRsn9fkl4M6JgJeZz9TafwLYl5nP1tbtAzY2qW5JUgcxCEqS1FyrgSfqliu1tnpXAVdFxF9HxOciYuMMtgUgIjZHxIGIOHD06NF5Kl2S1CkMgpIktZ/FwDrgTcBNwIcjYsVMdpCZOzKzPzP7V61aVUCJkqSFzCAoSVJzPQlcXrdcqrXVqwC7MnM8M78KPEo1GDayrSRJF2QQlCSpufYD6yJibUQsBTYBuyb1uZfqaCAR0Uv1UtEjwB5gQ0RcGhGXAhtqbZIkzYizhkqS1ESZeSYibqYa4BYBd2XmoYi4AziQmbv4TuA7DJwF3pmZxwAi4jeohkmAOzLz2eZ/CknSQmcQlCSpyTJzN7B7Utvtde8TuLX2mrztXcBdRdcoSepsBkG9yNDQEOVyeUbbjI6OArB169aGt+nr65tRf0mSJEnzxyCoOVu+fHmrS5AkSZI0AwZBvYijdJIkSVLnc9ZQSZIkSeoyBkFJkiRJ6jIGQUmSJEnqMgZBSZIkSeoyBkFJkiRJ6jIGQUmSJEnqMgZBSZIkSeoyBkFJkiRJ6jIGQUmSJEnqMgZBSZIkSeoyi1tdgCRJUrsbGhqiXC7PaJvR0VEAtm7dOqPt+vr6ZryNJM2UQVASMPNfcvwFp3P4dy8VY/ny5a0uQZKmZRCUNCv+gtO9/LtXN/IkhqROYxCUBPhLTjfz716SpO7jZDGSJEmS1GUMgpIkSZLUZQyCkiRJktRlDIKSJEmS1GUMgpIkSZLUZQyCkiRJktRlDIKSJEmS1GUMgpIkSZLUZXygvCRJkjSNoaEhyuXyjLYZHR0FYOvWrTParq+vb8bbSLNlEJQkSZLm0fLly1tdgnRBBkFJkiRpGo7QqVN5j6AkSZIkdRmDoCRJkiR1GYOgJEmSJHUZg6AkaUbGxsbYsmULx44da3UpkiRplgyCkqQZGR4e5uDBgwwPD7e6FEmSNEuFBsGI2BgRj0REOSJum2L9ByPiodrr0Yg4Pmn9JRFRiYgPFVmnJKkxY2NjjIyMkJmMjIw4KihJ0gJVWBCMiEXAncAAcA1wU0RcU98nM2/JzOsy8zpgO/BHk3bzG8Bni6pRkjQzw8PDZCYA586dc1RQkqQFqsgRweuBcmYeyczTwE7gxvP0vwn41MRCRPwg8Epgb4E1SpJmYN++fYyPjwMwPj7O3r3+iJYkaSEqMgiuBp6oW67U2l4iIq4A1gKfqS33AL8DvON8B4iIzRFxICIOHD16dF6KliRNb/369SxZsgSAJUuWsGHDhhZXJEmSZqNdJovZBNyTmWdry78C7M7Myvk2yswdmdmfmf2rVq0qvEhJ6naDg4NEBAA9PT0MDg62uCJJkjQbRQbBJ4HL65ZLtbapbKLuslDgh4GbI+Ix4LeBX4iIDxRRpCSpcb29vQwMDBARDAwMsHLlylaXJEmSZmFxgfveD6yLiLVUA+Am4Ocmd4qIq4FLgQcm2jLzX9StfzvQn5kvmXVUktR8g4ODPPbYY44GSpK0gBUWBDPzTETcDOwBFgF3ZeahiLgDOJCZu2pdNwE7c2IaOklSW+vt7WX79u2tLkOSJM1BofcIZubuzLwqM78vM99fa7u9LgSSmdvON9qXmR/NzJuLrFOS1LixsTG2bNniMwQlSVrA2mWyGEnSAjE8PMzBgwd9hqAkSQuYQVCS1LCxsTFGRkbITEZGRhwVnKWI2BgRj0REOSJeclVMRLw9Io5GxEO117+uW/dbEXEoIh6OiKGYmMZVkqQZKHKyGElShxkeHmbilu5z584xPDzMrbfe2uKqFpaIWATcCayn+ozd/RGxKzMPT+r6B5NvjYiIHwFeD1xba/or4I3A/YUWLUldaGhoiHK5PKNtRkdHAdi6dWvD2/T19c2o/3xxRFCS1LB9+/YxPj4OwPj4OHv37m1xRQvS9UA5M49k5mlgJ3Bjg9smsAxYCrwMWAI8XUiVkqQZW758OcuXL291GQ1xRFCS1LD169eze/duxsfHWbJkCRs2bGh1SQvRauCJuuUK8Lop+v1vEfGjwKPALZn5RGY+EBH3AV8HAvhQZj481UEiYjOwGWDNmjXzWb8kdYVWjNI1kyOCkqSGDQ4OMnFLWk9Pj88SLM6fAFdm5rXAPmAYICL6gFcDJaqB8sci4g1T7SAzd2Rmf2b2r1q1qkllS5IWCoOgJKlhvb29DAwMEBEMDAywcuXKVpe0ED0JXF63XKq1/aPMPJaZ364t/h7wg7X3/wz4XGaeyMwTwAjwwwXXK0nqQAZBSdKMDA4Ocu211zoaOHv7gXURsTYilgKbgF31HSLie+oW3wpMXP75OPDGiFgcEUuoThQz5aWhkiSdj/cISpJmpLe3l+3bt7e6jAUrM89E/P/t3WvMHFUZwPH/QwlY0qC9IKYqbaxIKrQhgaDcTAFNRPGCFmi9QI0mmkCIiI2WECmGRFTASFCjYimgUgwFRWwAA0QqLRWUpuUiKLGA+EEKUkAUpDx+mClMl17ey+zOvjv/X7LJ7pnbefbsznPOu2fmjdOAm4BxwJLMvC8ivg7cnZnXA6dHxIeBl4CngAXl5tcARwPrKW4cc2Nm/rrXMUiSxj4HgpIk9VhmrgBWdJR9rfJ8EbBoG9ttBj7f9QpKkgaeU0MlSZIkqWUcCEqSJElSyzgQlCRJkqSWcSAoSZIkSS3jQFCSJEmSWsaBoCRJkiS1jANBSZIkSWoZB4KSJEmS1DIOBCVJkiSpZRwISpIkSVLLRGY2XYdaRMQTwCMNVmEKsLHB4zetzfEbe3u1Of6mY5+WmXs1ePwxxRzZqDbHDu2O39jbq8n4h5wfB2Yg2LSIuDszD266Hk1pc/zG3s7Yod3xtzl2DV+bPy9tjh3aHb+xtzN2GDvxOzVUkiRJklrGgaAkSZIktYwDwfr8qOkKNKzN8Rt7e7U5/jbHruFr8+elzbFDu+M39vYaE/F7jaAkSZIktYy/CEqSJElSyzgQlCRJkqSW2bXpCowFEbEZWE/xfj0AnJKZz0fEW4DvAe+kGFTfACzMzBcjYg/gx8BsIICngfdn5nNNxDASETEZuKV8+SZgM/AE8HrgB5n5rXK9m4DHMvNz5esLgWeA48tt9wE2lY+NmfnengUxTDuIGeAQ4APAdcDMzPxzuc10is/Fg5VdXQScCuwOTALGA4+Xyz6amRu6FYMk9VIbc2Qb8yOYI6VB4zWCQxARz2XmhPL5z4A/At8B1lCc8C+LiHEUF4Y+lZkLI2IRsFdmfqncbj9gQ2a+0EwUoxMRi4HnMvOCiJgLnJiZJ0bELsBdwIuZeWi57mrgjMy8s3y9FLghM69ppvYjU425UnY1MBW4NTPPKcumU8R3wHb2swA4ODNP63KVq8fsSSclImYBV+5o3Yj4InA+sHdmbirL5gC/Av5W2d03gEXbqDPAIZn54gjfjtdosjMTERuAZ8tjAtyemafXGMNAtXFlkLHFssw8fydvj3qo7TmyjfkRzJHltn19/hyJQciPO4ljoNp41DkyM33s5EFxotvy/AvA94FjKD6g1fX2BJ4E9gAuBs5suu41vgeLgS+Xz6dSfGkAZgGXAzcDEym+8E8Du1W2XQrMbTqG0cRcvp5AcRJ7B/BgpXw6cO8O9rMAuKQf4gDmAr8on+9C0WFbXVl3NfDukbTd9tal6AyuBD5TKZtD0TEY0nvfy3Yuy64u63xu3e0MbACm2MZDa2Mq518f/fmg5TmSFubHzrjL1+bIHR+n5+fPutu4LBtT+XHQ25hR5kivERyGiNgVOJZi5L0/xYfnFZn5DPAo8HZgCfCViFgdEedFxL69rm+3ZOY/gJciYh/gMIovzRrgUOBgYH3W+OtNH/kIcGNmPgQ8GREHVZbNiIi1lceRDdVxZ1ZRtBMUn+F7gWcjYmJE7A7MBP5U18EiYgZF5+BsYH5d++2miJgAHAF8FpjXcHVGwjZWI8yRrc6PYI4ctrF2/hyA/Ai28VYcCA7N+IhYC9xNkcR+srMNMnMt8Dbg2xQ/id8VETO7WsveWkWR5LYkutWV13c0WK9umg8sK58vY+sv9MOZeWDlsbL31du5Bjop8yjeq5XAfhGxd2XZkR0dgxk1Hnc0etGZua2yjzNGX+VXDWAbj+/Yx0m11Fp1MkdurY35EcyRIzHWcuSYzo8wkG08qhzpzWKG5j+ZeWC1ICLup/h5uVq2J8X84L8CZHHR+7XAtRHxMsW86gd6UuPuu4PiCzSL4q8pjwFnUsyrvqzBenVFREwCjgZmRUQC44CMiIXN1mxEqp2Ui4A3l883UX8nZT5wfGa+HBHLgROAS8plKzPzuJqPV4f5wHfL51s6M1t+2Xi481wwQkdl5sYa9rM9g9TGrzn/qu+YI7fWqvwI5shRGGs5chDyIwxWG48qR/qL4MjdAuwREScDRHEh/IXA0izulnZ4REwsl+1Gcde0Rxqrbf1WAcdRXPi/OTOfAt5A8ReVVY3WrDvmAldm5rTMnJ6Zb6W4wLdfp7fsSGcn5U6KdjuMGtuuvIB6X+C35QXg8+jDaRFVlc7MpWWdFwInRkQ0WrHhs43VtDbnyLblRzBHDttYO38OUH4E2/gVDgRHKIsrNI8HToiIvwAPAf8FzipXmQH8LiLWA/dQTJlZ3kRdu2Q9MIXiy1Mt29SDv+Q0YT7FXbKqlvPqF7pzSsSI7nLVI73qpMwHFpedgumZORWYGhHTajxG3QalM2Mbq1Etz5Fty49gjhyJsXb+HJT8CLbxK5waOgRZ3hZ7G+WPAR/azrIrgCu6Wa9eyszFHa83U9wBrlq2YDvbbrO831VjzsyjtrH84srL8TvYz1KKO0n1iy2dlJ93lE2ouZMyj2KqV9V1ZfkayrnxlWXnZfO3UJ8PfLOjbHmlfEZHnZd0fA6G6rYobvkMsC4zTx7BPnZkkNp4fMc+bszMrw6/quqWtufINuZHMEfWYKzlyEHJjzBYbTyqHOn/EZQkSZKklnFqqCRJkiS1jFNDJQ1JedHzlR3FL2Tmu5qoT7+LiDUU/0C66tOZub6J+gxFU20cEZMpbi7S6ZjMfLKbx5akOpgjh24s5kcYzBzp1FBJkiRJahmnhkqSJElSyzgQlCRJkqSWcSAo9YmIyIj4aeX1rhHxRETcMMz9bIiIKaNdR5KkfmB+lLrDgaDUP/4NHBARW/7f0vuAxxusjyRJ/cD8KHWBA0Gpv6wAPlg+nw9ctWVBREyKiF9GxLqIuDMiZpflkyPi5oi4LyIuBaKyzaci4g8RsTYifhgR43oZjCRJNTE/SjVzICj1l2XAvIh4HTAbWFNZdi5wT2bOBs4CrijLzwF+n5n7A9cB+wBExEzgJODwzDwQ2Ax8sidRSJJUL/OjVDP/j6DURzJzXURMp/hr54qOxUcAHy/Xu7X8S+eewHuAj5Xlv4mIf5XrHwMcBNwVEQDjgX92OwZJkupmfpTq50BQ6j/XAxcAc4DJo9hPAJdn5qI6KiVJUsPMj1KNnBoq9Z8lwLmZub6jfCXl1JWImANszMxngNuBT5TlxwITy/VvAeZGxBvLZZMiYlr3qy9JUleYH6Ua+Yug1Gcy8+/AxdtYtBhYEhHrgOeBU8ryc4GrIuI+YBXwaLmf+yPibODmiNgF+B9wKvBIdyOQJKl+5kepXpGZTddBkiRJktRDTg2VJEmSpJZxIChJkiRJLeNAUJIkSZJaxoGgJEmSJLWMA0FJkiRJahkHgpIkSZLUMg4EJUmSJKll/g8zBx58lzW3CAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "restaurant_predictions = domain_predictions(model_domain_predictions, 'restaurant')\n",
    "laptop_predictions = domain_predictions(model_domain_predictions, 'laptop')\n",
    "restaurant_collections = dict_to_collection(restaurant_predictions, rest_test)\n",
    "laptop_collections = dict_to_collection(laptop_predictions, laptop_test)\n",
    "\n",
    "metrics = [('Accuracy', accuracy_score), ('F1', f1_score)]\n",
    "metric_kwargs = {'F1': {'average': 'macro'}}\n",
    "rest_results = datasets_df(restaurant_collections, metrics, metric_kwargs)\n",
    "laptop_results = datasets_df(laptop_collections, metrics, metric_kwargs)\n",
    "\n",
    "fig = plot_acc_f1(rest_results, 'Restaurant', figsize=(15, 7))\n",
    "fig.savefig('Restaurant Original Results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_acc_f1(laptop_results, 'Laptop', figsize=(15, 7))\n",
    "fig.savefig('Laptop Original Results.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results above the less target information incorporated into the model the better it is for the model espically for the Laptop domain. For the Restaurant domain it would appear to be just as benifical to have a Target At Each Timestep as not.\n",
    "\n",
    "Below we poke into the results further by sub-dividing the samples from the test dataset into 5 categories:\n",
    "1. Same One -- Where the samples contain a target that has been associated to one sentiment which is the same in train and test. \n",
    "2. Same Multi -- Where the samples contain a target that has been associtaed to more than one sentiment but the same sentiments in both train and test sets.\n",
    "3. Similar -- Where the samples contain a target that has been associated to more than one sentiment and there is overlap in these sentiments between train and test but they are never identical.\n",
    "4. Different -- Where the samples contain a target that has been associated to sentiments that occur only in train but never test.\n",
    "5. Unknown -- Up to now all targets have been seen in train and test. In this case the targets never occur in the train set only in test.\n",
    "\n",
    "Before showing the results for these categories we shall show the distributions of these categories over the two datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_funcs = [('Same One', same_one_sentiment), ('Same Multi', same_multi_sentiment),\n",
    "               ('Similar', similar_sentiment), ('Different', different_sentiment),\n",
    "               ('Unknown', unknown_targets)]\n",
    "\n",
    "name_test_trains = [('Restaurant', rest_test, rest_train), ('Laptop', laptop_test, laptop_train)]\n",
    "fig, axs = plt.subplots(1, len(name_test_trains), figsize=(15, 7))\n",
    "for index, name_test_train in enumerate(name_test_trains):\n",
    "    name, test, train = name_test_train\n",
    "    num_targets = [len(func(test, train)) for _, func in error_funcs]\n",
    "    x = range(1, len(num_targets) + 1)\n",
    "    \n",
    "    ax = axs[index]\n",
    "    ax.bar(x, num_targets)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Categories')\n",
    "    ax.set_ylabel('Number of Targets')\n",
    "    x_ticks = ['']\n",
    "    x_ticks.extend([cat_name for cat_name, _ in error_funcs])\n",
    "    ax.set_xticklabels(x_ticks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above the distributions of these categories are different for the two domains. Both have small representation for the `Same One` and `Different` but both have large representation for the `Unknown` category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the Restaurant domain for the 5 categories listed above.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Same One</th>\n",
       "      <th>Same Multi</th>\n",
       "      <th>Similar</th>\n",
       "      <th>Different</th>\n",
       "      <th>Unknown</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>POS</th>\n",
       "      <td>84.96 (3.75)</td>\n",
       "      <td>82.19 (1.49)</td>\n",
       "      <td>77.53 (1.06)</td>\n",
       "      <td>66.82 (3.69)</td>\n",
       "      <td>75.14 (1.17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TWT</th>\n",
       "      <td>95.78 (2.83)</td>\n",
       "      <td>80.85 (1.35)</td>\n",
       "      <td>78.31 (1.66)</td>\n",
       "      <td>53.71 (5.43)</td>\n",
       "      <td>71.33 (1.29)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAET</th>\n",
       "      <td>95.78 (2.83)</td>\n",
       "      <td>82.05 (1.28)</td>\n",
       "      <td>78.64 (1.41)</td>\n",
       "      <td>56.52 (3.60)</td>\n",
       "      <td>73.63 (1.39)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TWT_TAET</th>\n",
       "      <td>95.93 (2.08)</td>\n",
       "      <td>79.43 (1.32)</td>\n",
       "      <td>78.18 (1.47)</td>\n",
       "      <td>47.35 (6.24)</td>\n",
       "      <td>71.05 (1.22)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAET_E</th>\n",
       "      <td>95.41 (2.87)</td>\n",
       "      <td>82.01 (1.08)</td>\n",
       "      <td>78.91 (1.60)</td>\n",
       "      <td>52.42 (4.50)</td>\n",
       "      <td>74.86 (1.33)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TWT_TAET_E</th>\n",
       "      <td>96.07 (2.12)</td>\n",
       "      <td>79.39 (1.25)</td>\n",
       "      <td>77.84 (1.51)</td>\n",
       "      <td>42.65 (4.76)</td>\n",
       "      <td>70.97 (1.26)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Same One    Same Multi       Similar     Different  \\\n",
       "                     Acc           Acc           Acc           Acc   \n",
       "POS         84.96 (3.75)  82.19 (1.49)  77.53 (1.06)  66.82 (3.69)   \n",
       "TWT         95.78 (2.83)  80.85 (1.35)  78.31 (1.66)  53.71 (5.43)   \n",
       "TAET        95.78 (2.83)  82.05 (1.28)  78.64 (1.41)  56.52 (3.60)   \n",
       "TWT_TAET    95.93 (2.08)  79.43 (1.32)  78.18 (1.47)  47.35 (6.24)   \n",
       "TAET_E      95.41 (2.87)  82.01 (1.08)  78.91 (1.60)  52.42 (4.50)   \n",
       "TWT_TAET_E  96.07 (2.12)  79.39 (1.25)  77.84 (1.51)  42.65 (4.76)   \n",
       "\n",
       "                 Unknown  \n",
       "                     Acc  \n",
       "POS         75.14 (1.17)  \n",
       "TWT         71.33 (1.29)  \n",
       "TAET        73.63 (1.39)  \n",
       "TWT_TAET    71.05 (1.22)  \n",
       "TAET_E      74.86 (1.33)  \n",
       "TWT_TAET_E  70.97 (1.26)  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [('Acc', accuracy_score)]\n",
    "error_func_kwargs = {error_name: {'train_dataset': rest_train} for error_name, _ in error_funcs}\n",
    "\n",
    "print('Results for the Restaurant domain for the 5 categories listed above.')\n",
    "summary_errors(restaurant_collections, metrics=metrics,\n",
    "               error_funcs=error_funcs, error_funcs_kwargs=error_func_kwargs, std_err=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the Laptop domain for the 5 categories listed above.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Same One</th>\n",
       "      <th>Same Multi</th>\n",
       "      <th>Similar</th>\n",
       "      <th>Different</th>\n",
       "      <th>Unknown</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>POS</th>\n",
       "      <td>100.00 (0.00)</td>\n",
       "      <td>72.89 (5.69)</td>\n",
       "      <td>77.89 (6.71)</td>\n",
       "      <td>75.00 (0.00)</td>\n",
       "      <td>69.38 (1.54)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TWT</th>\n",
       "      <td>100.00 (0.00)</td>\n",
       "      <td>78.89 (7.11)</td>\n",
       "      <td>71.05 (5.88)</td>\n",
       "      <td>76.25 (3.75)</td>\n",
       "      <td>67.52 (1.89)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAET</th>\n",
       "      <td>93.33 (24.94)</td>\n",
       "      <td>74.22 (5.90)</td>\n",
       "      <td>71.93 (6.97)</td>\n",
       "      <td>78.75 (5.73)</td>\n",
       "      <td>67.79 (2.72)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TWT_TAET</th>\n",
       "      <td>86.67 (33.99)</td>\n",
       "      <td>74.67 (7.38)</td>\n",
       "      <td>65.26 (7.39)</td>\n",
       "      <td>75.00 (8.54)</td>\n",
       "      <td>63.56 (2.34)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAET_E</th>\n",
       "      <td>93.33 (24.94)</td>\n",
       "      <td>74.89 (5.36)</td>\n",
       "      <td>73.68 (6.08)</td>\n",
       "      <td>78.33 (6.40)</td>\n",
       "      <td>67.14 (3.01)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TWT_TAET_E</th>\n",
       "      <td>90.00 (30.00)</td>\n",
       "      <td>72.00 (6.30)</td>\n",
       "      <td>65.61 (7.66)</td>\n",
       "      <td>79.58 (8.83)</td>\n",
       "      <td>63.84 (1.80)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Same One    Same Multi       Similar     Different  \\\n",
       "                      Acc           Acc           Acc           Acc   \n",
       "POS         100.00 (0.00)  72.89 (5.69)  77.89 (6.71)  75.00 (0.00)   \n",
       "TWT         100.00 (0.00)  78.89 (7.11)  71.05 (5.88)  76.25 (3.75)   \n",
       "TAET        93.33 (24.94)  74.22 (5.90)  71.93 (6.97)  78.75 (5.73)   \n",
       "TWT_TAET    86.67 (33.99)  74.67 (7.38)  65.26 (7.39)  75.00 (8.54)   \n",
       "TAET_E      93.33 (24.94)  74.89 (5.36)  73.68 (6.08)  78.33 (6.40)   \n",
       "TWT_TAET_E  90.00 (30.00)  72.00 (6.30)  65.61 (7.66)  79.58 (8.83)   \n",
       "\n",
       "                 Unknown  \n",
       "                     Acc  \n",
       "POS         69.38 (1.54)  \n",
       "TWT         67.52 (1.89)  \n",
       "TAET        67.79 (2.72)  \n",
       "TWT_TAET    63.56 (2.34)  \n",
       "TAET_E      67.14 (3.01)  \n",
       "TWT_TAET_E  63.84 (1.80)  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Results for the Laptop domain for the 5 categories listed above.')\n",
    "summary_errors(laptop_collections, metrics=metrics,\n",
    "               error_funcs=error_funcs, error_funcs_kwargs=error_func_kwargs, std_err=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see stright away there is a different in results between the domains. In the restaurant domain as we add more target information into the model the results get better for the same `Same One` category and worse for the `Different` but the opposite happens within the Laptop domain. We expect these results for the restaurant domain as the more target information we get the more it could overfit to the target and hence perform better for the `Same One` and worse for the `Different`.\n",
    "\n",
    "The second interesting factor which is the same accross domain the more target information the worse it gets for the `unknown` category which is by far the most important category for the Laptop domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1 categories which are:\n",
      "FOOD#GENERAL\n"
     ]
    }
   ],
   "source": [
    "# We create a mapping of targets to related targets\n",
    "rest_15_sem_dir = Path('/home/andrew/Documents/ABSA15_RestaurantsTrain')\n",
    "rest_15_train = semeval_15_16(Path(rest_15_sem_dir, 'ABSA-15_Restaurants_Train_Final.xml'), \n",
    "                              raise_error_no_category=True, sep_15_from_14=True)\n",
    "\n",
    "rest_cat_targets, _ = rest_15_train.categories_targets()\n",
    "rest_targets = rest_train.target_set()\n",
    "target_to_targets = rest_train.target_targets(rest_targets, rest_cat_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 23:11:30.972662\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 23:11:30.974912\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 23:11:30.976614\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 23:11:30.978196\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 23:11:30.979867\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 23:11:30.981509\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 23:11:30.983164\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 23:11:30.984800\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 23:11:30.986438\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 23:11:30.987869\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant\n",
      "2018-10-24 23:11:46.877972\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 0\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/Envs/Bella/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/91 [============================>.] - ETA: 0s - loss: 0.7873 - acc: 0.6715\n",
      "Epoch 00001: val_loss improved from inf to 0.70973, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 0\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f24f31d4f28>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f24f31d4f28>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 16s 179ms/step - loss: 0.7886 - acc: 0.6710 - val_loss: 0.7097 - val_acc: 0.6713\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.5867 - acc: 0.7618\n",
      "Epoch 00002: val_loss improved from 0.70973 to 0.65468, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 0\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f24f31d4f28>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f24f31d4f28>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 11s 119ms/step - loss: 0.5872 - acc: 0.7616 - val_loss: 0.6547 - val_acc: 0.7143\n",
      "Epoch 3/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.4535 - acc: 0.8248\n",
      "Epoch 00003: val_loss improved from 0.65468 to 0.63320, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 0\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f24f31d4f28>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f24f31d4f28>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 11s 116ms/step - loss: 0.4505 - acc: 0.8255 - val_loss: 0.6332 - val_acc: 0.7462\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.8799\n",
      "Epoch 00004: val_loss did not improve from 0.63320\n",
      "91/91 [==============================] - 11s 118ms/step - loss: 0.3215 - acc: 0.8795 - val_loss: 0.6723 - val_acc: 0.7420\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9212\n",
      "Epoch 00005: val_loss did not improve from 0.63320\n",
      "91/91 [==============================] - 11s 117ms/step - loss: 0.2189 - acc: 0.9214 - val_loss: 0.6827 - val_acc: 0.7573\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9476\n",
      "Epoch 00006: val_loss did not improve from 0.63320\n",
      "91/91 [==============================] - 11s 117ms/step - loss: 0.1555 - acc: 0.9475 - val_loss: 0.8281 - val_acc: 0.7448\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9698\n",
      "Epoch 00007: val_loss did not improve from 0.63320\n",
      "91/91 [==============================] - 11s 117ms/step - loss: 0.0994 - acc: 0.9698 - val_loss: 0.9098 - val_acc: 0.7517\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9816\n",
      "Epoch 00008: val_loss did not improve from 0.63320\n",
      "91/91 [==============================] - 11s 116ms/step - loss: 0.0620 - acc: 0.9818 - val_loss: 0.9190 - val_acc: 0.7476\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'bella.keras_models' has no attribute 'save_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0a4b5d6ddf8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m                                                       \u001b[0mmodel_domain_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                                                       \u001b[0mnum_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                                                       dataloader_class=LeftRightAugmentSequence)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-9a8f6278393c>\u001b[0m in \u001b[0;36mmulti_save_and_predict\u001b[0;34m(model_settings, model_dir, predict_dir, domain_data, pred_dict, num_times, dataloader_class, **model_kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_and_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mpred_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdomain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpred_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9a8f6278393c>\u001b[0m in \u001b[0;36msave_and_predict\u001b[0;34m(model, train, val, test, model_path, predict_path, epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     history = keras_models.fit(model, train, val, model_path, verbose=1, \n\u001b[1;32m     34\u001b[0m                                use_multiprocessing=True, workers=4, epochs=epochs)\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mkeras_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'bella.keras_models' has no attribute 'save_weights'"
     ]
    }
   ],
   "source": [
    "rest_only = {'restaurant': domain_data['restaurant']}\n",
    "#model_domain_predictions = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "probabilities = [0.2, 0.5, 0.7, 1]\n",
    "for probability in probabilities:\n",
    "    model_dir = Path('..', 'Data Augmentation', f'Models Probability {probability}')\n",
    "    predict_dir = Path('..', 'Data Augmentation', f'Predictions Probability {probability}')\n",
    "    \n",
    "    target_trans = TargetAugmentation(target_to_targets, probability=probability)\n",
    "    model_settings = [(f'TAET Probability {probability}', \n",
    "                       {'include_target_in_sequence': False,\n",
    "                        'transformers': [target_trans]}, \n",
    "                       'tclstm'),\n",
    "                      (f'TWT Probability {probability}', \n",
    "                       {'include_target_in_batches': False, \n",
    "                        'transformers': [target_trans]},\n",
    "                       'tdlstm'),\n",
    "                      (f'TWT_TAET Probability {probability}', \n",
    "                       {'transformers': [target_trans]}, \n",
    "                       'tclstm'),\n",
    "                      (f'TAET_E Probability {probability}', \n",
    "                       {'include_target_in_sequence': False, \n",
    "                        'transformers': [target_trans]},\n",
    "                       'tclstm'),\n",
    "                      (f'TWT_TAET_E Probability {probability}', \n",
    "                       {'transformers': [target_trans]}, \n",
    "                       'tclstm')]\n",
    "    \n",
    "    data_saved = return_save_data(model_settings, model_dir, predict_dir, 10,\n",
    "                                  model_domain_predictions, rest_only)\n",
    "    if data_saved:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    model_domain_predictions = multi_save_and_predict(model_settings, model_dir, \n",
    "                                                      predict_dir, rest_only, \n",
    "                                                      model_domain_predictions,\n",
    "                                                      num_times=10, \n",
    "                                                      dataloader_class=LeftRightAugmentSequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 11:31:49.521569\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 0\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8216 - acc: 0.6281\n",
      "Epoch 00001: val_loss improved from inf to 0.71993, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 0\n",
      "91/91 [==============================] - 19s 208ms/step - loss: 0.8195 - acc: 0.6292 - val_loss: 0.7199 - val_acc: 0.7018\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.5741 - acc: 0.7538\n",
      "Epoch 00002: val_loss improved from 0.71993 to 0.62057, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 0\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.5771 - acc: 0.7524 - val_loss: 0.6206 - val_acc: 0.7379\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4426 - acc: 0.8323\n",
      "Epoch 00003: val_loss improved from 0.62057 to 0.60931, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 0\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.4415 - acc: 0.8324 - val_loss: 0.6093 - val_acc: 0.7476\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8698\n",
      "Epoch 00004: val_loss improved from 0.60931 to 0.58763, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 0\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.3266 - acc: 0.8691 - val_loss: 0.5876 - val_acc: 0.7601\n",
      "Epoch 5/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9126\n",
      "Epoch 00005: val_loss did not improve from 0.58763\n",
      "91/91 [==============================] - 13s 138ms/step - loss: 0.2274 - acc: 0.9135 - val_loss: 0.6434 - val_acc: 0.7490\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9458\n",
      "Epoch 00006: val_loss did not improve from 0.58763\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.1590 - acc: 0.9450 - val_loss: 0.7375 - val_acc: 0.7517\n",
      "Epoch 7/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9666\n",
      "Epoch 00007: val_loss did not improve from 0.58763\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.0989 - acc: 0.9674 - val_loss: 0.7211 - val_acc: 0.7601\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9854\n",
      "Epoch 00008: val_loss did not improve from 0.58763\n",
      "91/91 [==============================] - 13s 137ms/step - loss: 0.0481 - acc: 0.9856 - val_loss: 0.8380 - val_acc: 0.7517\n",
      "Epoch 9/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9898\n",
      "Epoch 00009: val_loss did not improve from 0.58763\n",
      "91/91 [==============================] - 12s 136ms/step - loss: 0.0342 - acc: 0.9897 - val_loss: 0.9639 - val_acc: 0.7712\n",
      "Epoch 00009: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 11:33:54.196879\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 1\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8089 - acc: 0.6403\n",
      "Epoch 00001: val_loss improved from inf to 0.73921, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 1\n",
      "91/91 [==============================] - 17s 190ms/step - loss: 0.8103 - acc: 0.6392 - val_loss: 0.7392 - val_acc: 0.6657\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.7906\n",
      "Epoch 00002: val_loss improved from 0.73921 to 0.57417, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 1\n",
      "91/91 [==============================] - 13s 139ms/step - loss: 0.5101 - acc: 0.7915 - val_loss: 0.5742 - val_acc: 0.7531\n",
      "Epoch 3/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.3192 - acc: 0.8739\n",
      "Epoch 00003: val_loss did not improve from 0.57417\n",
      "91/91 [==============================] - 13s 141ms/step - loss: 0.3190 - acc: 0.8743 - val_loss: 0.6073 - val_acc: 0.7642\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9240\n",
      "Epoch 00004: val_loss did not improve from 0.57417\n",
      "91/91 [==============================] - 13s 139ms/step - loss: 0.2051 - acc: 0.9241 - val_loss: 0.6246 - val_acc: 0.7822\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9590\n",
      "Epoch 00005: val_loss did not improve from 0.57417\n",
      "91/91 [==============================] - 12s 137ms/step - loss: 0.1265 - acc: 0.9595 - val_loss: 0.6782 - val_acc: 0.7684\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9792\n",
      "Epoch 00006: val_loss did not improve from 0.57417\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.0692 - acc: 0.9794 - val_loss: 0.7468 - val_acc: 0.7809\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9885\n",
      "Epoch 00007: val_loss did not improve from 0.57417\n",
      "91/91 [==============================] - 13s 138ms/step - loss: 0.0377 - acc: 0.9883 - val_loss: 0.8374 - val_acc: 0.7767\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 11:35:32.219595\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 2\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8152 - acc: 0.6385\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68347, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 2\n",
      "91/91 [==============================] - 19s 205ms/step - loss: 0.8119 - acc: 0.6405 - val_loss: 0.6835 - val_acc: 0.7129\n",
      "Epoch 2/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.5031 - acc: 0.7995\n",
      "Epoch 00002: val_loss improved from 0.68347 to 0.55299, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 2\n",
      "91/91 [==============================] - 13s 141ms/step - loss: 0.5057 - acc: 0.7977 - val_loss: 0.5530 - val_acc: 0.7684\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3158 - acc: 0.8753\n",
      "Epoch 00003: val_loss did not improve from 0.55299\n",
      "91/91 [==============================] - 13s 139ms/step - loss: 0.3182 - acc: 0.8753 - val_loss: 0.5854 - val_acc: 0.7587\n",
      "Epoch 4/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.1925 - acc: 0.9329\n",
      "Epoch 00004: val_loss did not improve from 0.55299\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.1915 - acc: 0.9334 - val_loss: 0.6190 - val_acc: 0.7781\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9670\n",
      "Epoch 00005: val_loss did not improve from 0.55299\n",
      "91/91 [==============================] - 13s 143ms/step - loss: 0.1074 - acc: 0.9667 - val_loss: 0.7578 - val_acc: 0.7836\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9816\n",
      "Epoch 00006: val_loss did not improve from 0.55299\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.0608 - acc: 0.9815 - val_loss: 0.8283 - val_acc: 0.7781\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9913\n",
      "Epoch 00007: val_loss did not improve from 0.55299\n",
      "91/91 [==============================] - 13s 144ms/step - loss: 0.0307 - acc: 0.9911 - val_loss: 0.8788 - val_acc: 0.7739\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 11:37:13.075925\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 3\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8258 - acc: 0.6267\n",
      "Epoch 00001: val_loss improved from inf to 0.63487, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 3\n",
      "91/91 [==============================] - 18s 196ms/step - loss: 0.8228 - acc: 0.6295 - val_loss: 0.6349 - val_acc: 0.7143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4884 - acc: 0.8031\n",
      "Epoch 00002: val_loss improved from 0.63487 to 0.58250, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 3\n",
      "91/91 [==============================] - 13s 139ms/step - loss: 0.4881 - acc: 0.8029 - val_loss: 0.5825 - val_acc: 0.7503\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2940 - acc: 0.8955\n",
      "Epoch 00003: val_loss did not improve from 0.58250\n",
      "91/91 [==============================] - 13s 137ms/step - loss: 0.2933 - acc: 0.8952 - val_loss: 0.6333 - val_acc: 0.7573\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9396\n",
      "Epoch 00004: val_loss did not improve from 0.58250\n",
      "91/91 [==============================] - 13s 139ms/step - loss: 0.1771 - acc: 0.9396 - val_loss: 0.6992 - val_acc: 0.7434\n",
      "Epoch 5/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9673\n",
      "Epoch 00005: val_loss did not improve from 0.58250\n",
      "91/91 [==============================] - 13s 138ms/step - loss: 0.0986 - acc: 0.9677 - val_loss: 0.7661 - val_acc: 0.7559\n",
      "Epoch 6/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9824\n",
      "Epoch 00006: val_loss did not improve from 0.58250\n",
      "91/91 [==============================] - 13s 142ms/step - loss: 0.0531 - acc: 0.9828 - val_loss: 0.8285 - val_acc: 0.7517\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9944\n",
      "Epoch 00007: val_loss did not improve from 0.58250\n",
      "91/91 [==============================] - 13s 138ms/step - loss: 0.0255 - acc: 0.9942 - val_loss: 0.9111 - val_acc: 0.7698\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 11:38:52.144252\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 4\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8799 - acc: 0.6243\n",
      "Epoch 00001: val_loss improved from inf to 0.67062, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 4\n",
      "91/91 [==============================] - 19s 209ms/step - loss: 0.8777 - acc: 0.6254 - val_loss: 0.6706 - val_acc: 0.7129\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.5133 - acc: 0.7917\n",
      "Epoch 00002: val_loss improved from 0.67062 to 0.59660, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 4\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.5143 - acc: 0.7908 - val_loss: 0.5966 - val_acc: 0.7476\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3293 - acc: 0.8733\n",
      "Epoch 00003: val_loss did not improve from 0.59660\n",
      "91/91 [==============================] - 13s 139ms/step - loss: 0.3365 - acc: 0.8702 - val_loss: 0.6008 - val_acc: 0.7628\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9271\n",
      "Epoch 00004: val_loss did not improve from 0.59660\n",
      "91/91 [==============================] - 13s 137ms/step - loss: 0.2073 - acc: 0.9275 - val_loss: 0.7313 - val_acc: 0.7503\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9549\n",
      "Epoch 00005: val_loss did not improve from 0.59660\n",
      "91/91 [==============================] - 13s 139ms/step - loss: 0.1217 - acc: 0.9554 - val_loss: 0.7554 - val_acc: 0.7587\n",
      "Epoch 6/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9765\n",
      "Epoch 00006: val_loss did not improve from 0.59660\n",
      "91/91 [==============================] - 13s 141ms/step - loss: 0.0707 - acc: 0.9770 - val_loss: 0.8344 - val_acc: 0.7531\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9906\n",
      "Epoch 00007: val_loss did not improve from 0.59660\n",
      "91/91 [==============================] - 13s 141ms/step - loss: 0.0359 - acc: 0.9904 - val_loss: 0.8722 - val_acc: 0.7753\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 11:40:32.528661\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 5\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8506 - acc: 0.6205\n",
      "Epoch 00001: val_loss improved from inf to 0.65638, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 5\n",
      "91/91 [==============================] - 19s 204ms/step - loss: 0.8477 - acc: 0.6220 - val_loss: 0.6564 - val_acc: 0.7393\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.5047 - acc: 0.7753\n",
      "Epoch 00002: val_loss improved from 0.65638 to 0.58818, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 5\n",
      "91/91 [==============================] - 13s 141ms/step - loss: 0.5031 - acc: 0.7762 - val_loss: 0.5882 - val_acc: 0.7393\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3068 - acc: 0.8851\n",
      "Epoch 00003: val_loss improved from 0.58818 to 0.57516, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 5\n",
      "91/91 [==============================] - 13s 139ms/step - loss: 0.3065 - acc: 0.8853 - val_loss: 0.5752 - val_acc: 0.7601\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9385\n",
      "Epoch 00004: val_loss did not improve from 0.57516\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.1772 - acc: 0.9382 - val_loss: 0.6532 - val_acc: 0.7767\n",
      "Epoch 5/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9712\n",
      "Epoch 00005: val_loss did not improve from 0.57516\n",
      "91/91 [==============================] - 13s 143ms/step - loss: 0.0893 - acc: 0.9712 - val_loss: 0.7704 - val_acc: 0.7587\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9854\n",
      "Epoch 00006: val_loss did not improve from 0.57516\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.0484 - acc: 0.9856 - val_loss: 0.8472 - val_acc: 0.7642\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9948\n",
      "Epoch 00007: val_loss did not improve from 0.57516\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.0221 - acc: 0.9948 - val_loss: 0.9111 - val_acc: 0.7753\n",
      "Epoch 8/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9968\n",
      "Epoch 00008: val_loss did not improve from 0.57516\n",
      "91/91 [==============================] - 13s 139ms/step - loss: 0.0138 - acc: 0.9969 - val_loss: 0.9894 - val_acc: 0.7767\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 11:42:25.805087\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 6\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.7987 - acc: 0.6438\n",
      "Epoch 00001: val_loss improved from inf to 0.61298, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 6\n",
      "91/91 [==============================] - 18s 202ms/step - loss: 0.7976 - acc: 0.6440 - val_loss: 0.6130 - val_acc: 0.7351\n",
      "Epoch 2/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.4531 - acc: 0.8136\n",
      "Epoch 00002: val_loss improved from 0.61298 to 0.54203, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 6\n",
      "91/91 [==============================] - 13s 141ms/step - loss: 0.4505 - acc: 0.8142 - val_loss: 0.5420 - val_acc: 0.7684\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.8993\n",
      "Epoch 00003: val_loss did not improve from 0.54203\n",
      "91/91 [==============================] - 13s 143ms/step - loss: 0.2598 - acc: 0.8994 - val_loss: 0.5480 - val_acc: 0.7920\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9486\n",
      "Epoch 00004: val_loss did not improve from 0.54203\n",
      "91/91 [==============================] - 13s 142ms/step - loss: 0.1437 - acc: 0.9492 - val_loss: 0.6449 - val_acc: 0.7809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9760\n",
      "Epoch 00005: val_loss did not improve from 0.54203\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.0805 - acc: 0.9760 - val_loss: 0.7392 - val_acc: 0.7739\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9938\n",
      "Epoch 00006: val_loss did not improve from 0.54203\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.0311 - acc: 0.9938 - val_loss: 0.8546 - val_acc: 0.7698\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9969\n",
      "Epoch 00007: val_loss did not improve from 0.54203\n",
      "91/91 [==============================] - 13s 142ms/step - loss: 0.0151 - acc: 0.9969 - val_loss: 0.9014 - val_acc: 0.7809\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 11:44:06.769262\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 7\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8551 - acc: 0.6167\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65199, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 7\n",
      "91/91 [==============================] - 19s 213ms/step - loss: 0.8502 - acc: 0.6199 - val_loss: 0.6520 - val_acc: 0.7282\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4929 - acc: 0.7965- ETA: 1s - loss: 0.50\n",
      "Epoch 00002: val_loss improved from 0.65199 to 0.57184, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 7\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.4921 - acc: 0.7967 - val_loss: 0.5718 - val_acc: 0.7628\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.8844\n",
      "Epoch 00003: val_loss did not improve from 0.57184\n",
      "91/91 [==============================] - 13s 141ms/step - loss: 0.3089 - acc: 0.8843 - val_loss: 0.5757 - val_acc: 0.7712\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9340\n",
      "Epoch 00004: val_loss did not improve from 0.57184\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.1923 - acc: 0.9327 - val_loss: 0.6466 - val_acc: 0.7670\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9705\n",
      "Epoch 00005: val_loss did not improve from 0.57184\n",
      "91/91 [==============================] - 13s 139ms/step - loss: 0.0949 - acc: 0.9705 - val_loss: 0.7513 - val_acc: 0.7698\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9865\n",
      "Epoch 00006: val_loss did not improve from 0.57184\n",
      "91/91 [==============================] - 13s 138ms/step - loss: 0.0500 - acc: 0.9863 - val_loss: 0.7996 - val_acc: 0.7684\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9899\n",
      "Epoch 00007: val_loss did not improve from 0.57184\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.0336 - acc: 0.9900 - val_loss: 0.8349 - val_acc: 0.7850\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 11:45:47.966118\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 8\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8252 - acc: 0.6312\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65273, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 8\n",
      "91/91 [==============================] - 19s 210ms/step - loss: 0.8247 - acc: 0.6319 - val_loss: 0.6527 - val_acc: 0.7074\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.8073\n",
      "Epoch 00002: val_loss improved from 0.65273 to 0.58310, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 8\n",
      "91/91 [==============================] - 13s 143ms/step - loss: 0.4773 - acc: 0.8087 - val_loss: 0.5831 - val_acc: 0.7490\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2918 - acc: 0.8906\n",
      "Epoch 00003: val_loss did not improve from 0.58310\n",
      "91/91 [==============================] - 13s 141ms/step - loss: 0.2896 - acc: 0.8918 - val_loss: 0.6261 - val_acc: 0.7601\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9403\n",
      "Epoch 00004: val_loss did not improve from 0.58310\n",
      "91/91 [==============================] - 13s 141ms/step - loss: 0.1642 - acc: 0.9399 - val_loss: 0.6458 - val_acc: 0.7670\n",
      "Epoch 5/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9747\n",
      "Epoch 00005: val_loss did not improve from 0.58310\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.0877 - acc: 0.9746 - val_loss: 0.6764 - val_acc: 0.7767\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9910\n",
      "Epoch 00006: val_loss did not improve from 0.58310\n",
      "91/91 [==============================] - 13s 144ms/step - loss: 0.0405 - acc: 0.9907 - val_loss: 0.8305 - val_acc: 0.7809\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9938\n",
      "Epoch 00007: val_loss did not improve from 0.58310\n",
      "91/91 [==============================] - 13s 139ms/step - loss: 0.0258 - acc: 0.9935 - val_loss: 0.9043 - val_acc: 0.7684\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant\n",
      "2018-10-24 11:47:29.768465\n",
      "../Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 9\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8699 - acc: 0.6087- ETA: 2s - loss: 0.8811\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67253, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 9\n",
      "91/91 [==============================] - 19s 214ms/step - loss: 0.8651 - acc: 0.6113 - val_loss: 0.6725 - val_acc: 0.7268\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4995 - acc: 0.7962\n",
      "Epoch 00002: val_loss improved from 0.67253 to 0.55234, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET Probability 1/normal restaurant 9\n",
      "91/91 [==============================] - 13s 141ms/step - loss: 0.5061 - acc: 0.7943 - val_loss: 0.5523 - val_acc: 0.7656\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2869 - acc: 0.8872\n",
      "Epoch 00003: val_loss did not improve from 0.55234\n",
      "91/91 [==============================] - 13s 138ms/step - loss: 0.2866 - acc: 0.8873 - val_loss: 0.5622 - val_acc: 0.7850\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9465\n",
      "Epoch 00004: val_loss did not improve from 0.55234\n",
      "91/91 [==============================] - 13s 139ms/step - loss: 0.1585 - acc: 0.9471 - val_loss: 0.5970 - val_acc: 0.7836\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9747\n",
      "Epoch 00005: val_loss did not improve from 0.55234\n",
      "91/91 [==============================] - 13s 143ms/step - loss: 0.0864 - acc: 0.9746 - val_loss: 0.6668 - val_acc: 0.7864\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9875\n",
      "Epoch 00006: val_loss did not improve from 0.55234\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.0481 - acc: 0.9876 - val_loss: 0.7742 - val_acc: 0.7795\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9944\n",
      "Epoch 00007: val_loss did not improve from 0.55234\n",
      "91/91 [==============================] - 13s 139ms/step - loss: 0.0206 - acc: 0.9945 - val_loss: 0.8246 - val_acc: 0.7920\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant\n",
      "2018-10-24 11:49:47.080805\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 0\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8326 - acc: 0.6253\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62453, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 19s 204ms/step - loss: 0.8320 - acc: 0.6275 - val_loss: 0.6245 - val_acc: 0.7365\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4927 - acc: 0.8083\n",
      "Epoch 00002: val_loss improved from 0.62453 to 0.57478, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 0\n",
      "91/91 [==============================] - 12s 127ms/step - loss: 0.4924 - acc: 0.8084 - val_loss: 0.5748 - val_acc: 0.7767\n",
      "Epoch 3/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.3165 - acc: 0.8817\n",
      "Epoch 00003: val_loss improved from 0.57478 to 0.54987, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 0\n",
      "91/91 [==============================] - 12s 127ms/step - loss: 0.3161 - acc: 0.8819 - val_loss: 0.5499 - val_acc: 0.8086\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9399\n",
      "Epoch 00004: val_loss did not improve from 0.54987\n",
      "91/91 [==============================] - 11s 123ms/step - loss: 0.1882 - acc: 0.9396 - val_loss: 0.6352 - val_acc: 0.7795\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9615\n",
      "Epoch 00005: val_loss did not improve from 0.54987\n",
      "91/91 [==============================] - 11s 123ms/step - loss: 0.1169 - acc: 0.9608 - val_loss: 0.6482 - val_acc: 0.7850\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9816\n",
      "Epoch 00006: val_loss did not improve from 0.54987\n",
      "91/91 [==============================] - 11s 126ms/step - loss: 0.0659 - acc: 0.9811 - val_loss: 0.8379 - val_acc: 0.7725\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9892\n",
      "Epoch 00007: val_loss did not improve from 0.54987\n",
      "91/91 [==============================] - 11s 124ms/step - loss: 0.0357 - acc: 0.9890 - val_loss: 0.9023 - val_acc: 0.7892\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9941\n",
      "Epoch 00008: val_loss did not improve from 0.54987\n",
      "91/91 [==============================] - 11s 123ms/step - loss: 0.0192 - acc: 0.9942 - val_loss: 0.9177 - val_acc: 0.7920\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant\n",
      "2018-10-24 11:51:30.360887\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 1\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9052 - acc: 0.6049\n",
      "Epoch 00001: val_loss improved from inf to 0.68460, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 1\n",
      "91/91 [==============================] - 19s 209ms/step - loss: 0.9022 - acc: 0.6062 - val_loss: 0.6846 - val_acc: 0.7406\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.5331 - acc: 0.7885\n",
      "Epoch 00002: val_loss improved from 0.68460 to 0.56579, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 1\n",
      "91/91 [==============================] - 12s 128ms/step - loss: 0.5326 - acc: 0.7891 - val_loss: 0.5658 - val_acc: 0.7545\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.8809\n",
      "Epoch 00003: val_loss did not improve from 0.56579\n",
      "91/91 [==============================] - 11s 125ms/step - loss: 0.3220 - acc: 0.8812 - val_loss: 0.5707 - val_acc: 0.7573\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9368\n",
      "Epoch 00004: val_loss did not improve from 0.56579\n",
      "91/91 [==============================] - 12s 127ms/step - loss: 0.1852 - acc: 0.9375 - val_loss: 0.5896 - val_acc: 0.7975\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9667\n",
      "Epoch 00005: val_loss did not improve from 0.56579\n",
      "91/91 [==============================] - 12s 127ms/step - loss: 0.1126 - acc: 0.9670 - val_loss: 0.7152 - val_acc: 0.7961\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9799\n",
      "Epoch 00006: val_loss did not improve from 0.56579\n",
      "91/91 [==============================] - 12s 127ms/step - loss: 0.0614 - acc: 0.9801 - val_loss: 0.8269 - val_acc: 0.7864\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9878\n",
      "Epoch 00007: val_loss did not improve from 0.56579\n",
      "91/91 [==============================] - 12s 127ms/step - loss: 0.0407 - acc: 0.9880 - val_loss: 0.8123 - val_acc: 0.7920\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant\n",
      "2018-10-24 11:53:05.542811\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 2\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8081 - acc: 0.6500\n",
      "Epoch 00001: val_loss improved from inf to 0.64689, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 2\n",
      "91/91 [==============================] - 20s 216ms/step - loss: 0.8059 - acc: 0.6512 - val_loss: 0.6469 - val_acc: 0.7323\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4799 - acc: 0.8063\n",
      "Epoch 00002: val_loss improved from 0.64689 to 0.58071, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 2\n",
      "91/91 [==============================] - 11s 126ms/step - loss: 0.4794 - acc: 0.8070 - val_loss: 0.5807 - val_acc: 0.7642\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2772 - acc: 0.8924\n",
      "Epoch 00003: val_loss did not improve from 0.58071\n",
      "91/91 [==============================] - 12s 128ms/step - loss: 0.2772 - acc: 0.8925 - val_loss: 0.6029 - val_acc: 0.7712\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1658 - acc: 0.9451\n",
      "Epoch 00004: val_loss did not improve from 0.58071\n",
      "91/91 [==============================] - 12s 130ms/step - loss: 0.1655 - acc: 0.9454 - val_loss: 0.6591 - val_acc: 0.7850\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9670\n",
      "Epoch 00005: val_loss did not improve from 0.58071\n",
      "91/91 [==============================] - 12s 129ms/step - loss: 0.1004 - acc: 0.9667 - val_loss: 0.7080 - val_acc: 0.7975\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9785\n",
      "Epoch 00006: val_loss did not improve from 0.58071\n",
      "91/91 [==============================] - 12s 128ms/step - loss: 0.0614 - acc: 0.9780 - val_loss: 0.8137 - val_acc: 0.7809\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9903\n",
      "Epoch 00007: val_loss did not improve from 0.58071\n",
      "91/91 [==============================] - 12s 129ms/step - loss: 0.0292 - acc: 0.9904 - val_loss: 0.9520 - val_acc: 0.7822\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant\n",
      "2018-10-24 11:54:41.087206\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 3\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8286 - acc: 0.6392\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.77221, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 3\n",
      "91/91 [==============================] - 20s 217ms/step - loss: 0.8313 - acc: 0.6378 - val_loss: 0.7722 - val_acc: 0.6741\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.5019 - acc: 0.7976\n",
      "Epoch 00002: val_loss improved from 0.77221 to 0.60628, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 3\n",
      "91/91 [==============================] - 12s 131ms/step - loss: 0.4995 - acc: 0.7987 - val_loss: 0.6063 - val_acc: 0.7656\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.8885\n",
      "Epoch 00003: val_loss improved from 0.60628 to 0.58812, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 3\n",
      "91/91 [==============================] - 12s 130ms/step - loss: 0.2912 - acc: 0.8887 - val_loss: 0.5881 - val_acc: 0.7892\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/91 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9378\n",
      "Epoch 00004: val_loss did not improve from 0.58812\n",
      "91/91 [==============================] - 12s 131ms/step - loss: 0.1752 - acc: 0.9375 - val_loss: 0.7025 - val_acc: 0.7795\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9625\n",
      "Epoch 00005: val_loss did not improve from 0.58812\n",
      "91/91 [==============================] - 12s 128ms/step - loss: 0.1067 - acc: 0.9629 - val_loss: 0.7431 - val_acc: 0.7684\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9781\n",
      "Epoch 00006: val_loss did not improve from 0.58812\n",
      "91/91 [==============================] - 12s 127ms/step - loss: 0.0686 - acc: 0.9784 - val_loss: 0.8364 - val_acc: 0.7795\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9913\n",
      "Epoch 00007: val_loss did not improve from 0.58812\n",
      "91/91 [==============================] - 12s 128ms/step - loss: 0.0310 - acc: 0.9914 - val_loss: 0.9177 - val_acc: 0.7739\n",
      "Epoch 8/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9937\n",
      "Epoch 00008: val_loss did not improve from 0.58812\n",
      "91/91 [==============================] - 12s 131ms/step - loss: 0.0194 - acc: 0.9938 - val_loss: 1.0660 - val_acc: 0.7753\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant\n",
      "2018-10-24 11:56:29.461302\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 4\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8390 - acc: 0.6448\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65220, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 4\n",
      "91/91 [==============================] - 20s 225ms/step - loss: 0.8351 - acc: 0.6466 - val_loss: 0.6522 - val_acc: 0.7490\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4917 - acc: 0.8021\n",
      "Epoch 00002: val_loss improved from 0.65220 to 0.58400, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 4\n",
      "91/91 [==============================] - 12s 134ms/step - loss: 0.4893 - acc: 0.8032 - val_loss: 0.5840 - val_acc: 0.7628\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3012 - acc: 0.8806\n",
      "Epoch 00003: val_loss did not improve from 0.58400\n",
      "91/91 [==============================] - 12s 131ms/step - loss: 0.3033 - acc: 0.8801 - val_loss: 0.6190 - val_acc: 0.7670\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9365\n",
      "Epoch 00004: val_loss did not improve from 0.58400\n",
      "91/91 [==============================] - 12s 132ms/step - loss: 0.1835 - acc: 0.9365 - val_loss: 0.6641 - val_acc: 0.7795\n",
      "Epoch 5/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9617\n",
      "Epoch 00005: val_loss did not improve from 0.58400\n",
      "91/91 [==============================] - 12s 130ms/step - loss: 0.1070 - acc: 0.9619 - val_loss: 0.7129 - val_acc: 0.7753\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9795\n",
      "Epoch 00006: val_loss did not improve from 0.58400\n",
      "91/91 [==============================] - 12s 132ms/step - loss: 0.0671 - acc: 0.9797 - val_loss: 0.7814 - val_acc: 0.7767\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9899\n",
      "Epoch 00007: val_loss did not improve from 0.58400\n",
      "91/91 [==============================] - 12s 131ms/step - loss: 0.0384 - acc: 0.9900 - val_loss: 0.9219 - val_acc: 0.7642\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant\n",
      "2018-10-24 11:58:07.945777\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 5\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8156 - acc: 0.6545\n",
      "Epoch 00001: val_loss improved from inf to 0.69627, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 5\n",
      "91/91 [==============================] - 20s 222ms/step - loss: 0.8147 - acc: 0.6548 - val_loss: 0.6963 - val_acc: 0.7087\n",
      "Epoch 2/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.4719 - acc: 0.8153\n",
      "Epoch 00002: val_loss improved from 0.69627 to 0.54506, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 5\n",
      "91/91 [==============================] - 12s 131ms/step - loss: 0.4715 - acc: 0.8145 - val_loss: 0.5451 - val_acc: 0.7795\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2768 - acc: 0.8938\n",
      "Epoch 00003: val_loss did not improve from 0.54506\n",
      "91/91 [==============================] - 12s 131ms/step - loss: 0.2764 - acc: 0.8939 - val_loss: 0.5955 - val_acc: 0.8003\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9424\n",
      "Epoch 00004: val_loss did not improve from 0.54506\n",
      "91/91 [==============================] - 12s 131ms/step - loss: 0.1660 - acc: 0.9413 - val_loss: 0.6221 - val_acc: 0.7850\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9691\n",
      "Epoch 00005: val_loss did not improve from 0.54506\n",
      "91/91 [==============================] - 12s 134ms/step - loss: 0.0955 - acc: 0.9694 - val_loss: 0.7293 - val_acc: 0.7795\n",
      "Epoch 6/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9817\n",
      "Epoch 00006: val_loss did not improve from 0.54506\n",
      "91/91 [==============================] - 12s 128ms/step - loss: 0.0591 - acc: 0.9815 - val_loss: 0.7976 - val_acc: 0.7878\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9934\n",
      "Epoch 00007: val_loss did not improve from 0.54506\n",
      "91/91 [==============================] - 12s 132ms/step - loss: 0.0284 - acc: 0.9931 - val_loss: 0.8862 - val_acc: 0.7753\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant\n",
      "2018-10-24 11:59:45.943453\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 6\n",
      "Epoch 1/100\n",
      "89/91 [============================>.] - ETA: 0s - loss: 0.8357 - acc: 0.6433\n",
      "Epoch 00001: val_loss improved from inf to 0.63395, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 6\n",
      "91/91 [==============================] - 21s 230ms/step - loss: 0.8322 - acc: 0.6445 - val_loss: 0.6339 - val_acc: 0.7323\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4930 - acc: 0.8038\n",
      "Epoch 00002: val_loss improved from 0.63395 to 0.56031, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 6\n",
      "91/91 [==============================] - 12s 136ms/step - loss: 0.4926 - acc: 0.8029 - val_loss: 0.5603 - val_acc: 0.7670\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3103 - acc: 0.8840\n",
      "Epoch 00003: val_loss did not improve from 0.56031\n",
      "91/91 [==============================] - 12s 134ms/step - loss: 0.3097 - acc: 0.8843 - val_loss: 0.5970 - val_acc: 0.7712\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9281\n",
      "Epoch 00004: val_loss did not improve from 0.56031\n",
      "91/91 [==============================] - 12s 135ms/step - loss: 0.1945 - acc: 0.9282 - val_loss: 0.6572 - val_acc: 0.7836\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9604\n",
      "Epoch 00005: val_loss did not improve from 0.56031\n",
      "91/91 [==============================] - 12s 136ms/step - loss: 0.1143 - acc: 0.9605 - val_loss: 0.6649 - val_acc: 0.7906\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9722\n",
      "Epoch 00006: val_loss did not improve from 0.56031\n",
      "91/91 [==============================] - 12s 134ms/step - loss: 0.0828 - acc: 0.9725 - val_loss: 0.7605 - val_acc: 0.7767\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9826\n",
      "Epoch 00007: val_loss did not improve from 0.56031\n",
      "91/91 [==============================] - 12s 137ms/step - loss: 0.0509 - acc: 0.9828 - val_loss: 0.8231 - val_acc: 0.7725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant\n",
      "2018-10-24 12:01:26.831462\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 7\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8572 - acc: 0.6330\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63919, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 7\n",
      "91/91 [==============================] - 22s 243ms/step - loss: 0.8534 - acc: 0.6347 - val_loss: 0.6392 - val_acc: 0.7268\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.5281 - acc: 0.7788\n",
      "Epoch 00002: val_loss improved from 0.63919 to 0.55727, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 7\n",
      "91/91 [==============================] - 12s 133ms/step - loss: 0.5294 - acc: 0.7779 - val_loss: 0.5573 - val_acc: 0.7642\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3210 - acc: 0.8715\n",
      "Epoch 00003: val_loss did not improve from 0.55727\n",
      "91/91 [==============================] - 12s 134ms/step - loss: 0.3197 - acc: 0.8726 - val_loss: 0.6170 - val_acc: 0.7725\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1918 - acc: 0.9309\n",
      "Epoch 00004: val_loss did not improve from 0.55727\n",
      "91/91 [==============================] - 12s 136ms/step - loss: 0.1918 - acc: 0.9306 - val_loss: 0.6221 - val_acc: 0.7892\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9608\n",
      "Epoch 00005: val_loss did not improve from 0.55727\n",
      "91/91 [==============================] - 12s 133ms/step - loss: 0.1139 - acc: 0.9612 - val_loss: 0.7189 - val_acc: 0.7698\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9806\n",
      "Epoch 00006: val_loss did not improve from 0.55727\n",
      "91/91 [==============================] - 12s 136ms/step - loss: 0.0615 - acc: 0.9804 - val_loss: 0.8053 - val_acc: 0.7628\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9878\n",
      "Epoch 00007: val_loss did not improve from 0.55727\n",
      "91/91 [==============================] - 12s 134ms/step - loss: 0.0401 - acc: 0.9876 - val_loss: 0.9343 - val_acc: 0.7684\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant\n",
      "2018-10-24 12:03:08.703318\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 8\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8218 - acc: 0.6531\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66986, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 8\n",
      "91/91 [==============================] - 21s 234ms/step - loss: 0.8244 - acc: 0.6524 - val_loss: 0.6699 - val_acc: 0.7157\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4923 - acc: 0.8017\n",
      "Epoch 00002: val_loss improved from 0.66986 to 0.54175, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 8\n",
      "91/91 [==============================] - 13s 138ms/step - loss: 0.4899 - acc: 0.8035 - val_loss: 0.5417 - val_acc: 0.7809\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3097 - acc: 0.8764\n",
      "Epoch 00003: val_loss improved from 0.54175 to 0.52860, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 8\n",
      "91/91 [==============================] - 13s 142ms/step - loss: 0.3098 - acc: 0.8770 - val_loss: 0.5286 - val_acc: 0.7947\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9340\n",
      "Epoch 00004: val_loss did not improve from 0.52860\n",
      "91/91 [==============================] - 12s 137ms/step - loss: 0.1800 - acc: 0.9341 - val_loss: 0.6194 - val_acc: 0.7892\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9660\n",
      "Epoch 00005: val_loss did not improve from 0.52860\n",
      "91/91 [==============================] - 13s 139ms/step - loss: 0.1091 - acc: 0.9663 - val_loss: 0.6563 - val_acc: 0.7920\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9854\n",
      "Epoch 00006: val_loss did not improve from 0.52860\n",
      "91/91 [==============================] - 13s 140ms/step - loss: 0.0562 - acc: 0.9856 - val_loss: 0.7711 - val_acc: 0.7947\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9896\n",
      "Epoch 00007: val_loss did not improve from 0.52860\n",
      "91/91 [==============================] - 13s 137ms/step - loss: 0.0379 - acc: 0.9894 - val_loss: 0.8382 - val_acc: 0.8031\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9917\n",
      "Epoch 00008: val_loss did not improve from 0.52860\n",
      "91/91 [==============================] - 12s 137ms/step - loss: 0.0255 - acc: 0.9918 - val_loss: 0.8828 - val_acc: 0.8044\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant\n",
      "2018-10-24 12:05:04.640697\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 9\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8365 - acc: 0.6385\n",
      "Epoch 00001: val_loss improved from inf to 0.79038, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 9\n",
      "91/91 [==============================] - 22s 242ms/step - loss: 0.8360 - acc: 0.6388 - val_loss: 0.7904 - val_acc: 0.6546\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.5594 - acc: 0.7618\n",
      "Epoch 00002: val_loss improved from 0.79038 to 0.56209, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 9\n",
      "91/91 [==============================] - 13s 141ms/step - loss: 0.5566 - acc: 0.7634 - val_loss: 0.5621 - val_acc: 0.7670\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3512 - acc: 0.8663\n",
      "Epoch 00003: val_loss improved from 0.56209 to 0.54605, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 9\n",
      "91/91 [==============================] - 13s 141ms/step - loss: 0.3492 - acc: 0.8671 - val_loss: 0.5461 - val_acc: 0.7850\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9281\n",
      "Epoch 00004: val_loss did not improve from 0.54605\n",
      "91/91 [==============================] - 12s 136ms/step - loss: 0.2148 - acc: 0.9286 - val_loss: 0.6303 - val_acc: 0.7753\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9580\n",
      "Epoch 00005: val_loss did not improve from 0.54605\n",
      "91/91 [==============================] - 13s 138ms/step - loss: 0.1353 - acc: 0.9581 - val_loss: 0.7123 - val_acc: 0.7725\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9774\n",
      "Epoch 00006: val_loss did not improve from 0.54605\n",
      "91/91 [==============================] - 13s 138ms/step - loss: 0.0755 - acc: 0.9777 - val_loss: 0.7558 - val_acc: 0.7795\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9875\n",
      "Epoch 00007: val_loss did not improve from 0.54605\n",
      "91/91 [==============================] - 13s 138ms/step - loss: 0.0431 - acc: 0.9876 - val_loss: 0.8563 - val_acc: 0.7822\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9927\n",
      "Epoch 00008: val_loss did not improve from 0.54605\n",
      "91/91 [==============================] - 12s 136ms/step - loss: 0.0255 - acc: 0.9928 - val_loss: 0.9662 - val_acc: 0.7725\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant\n",
      "2018-10-24 12:08:02.983667\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 0\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9392 - acc: 0.5986\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.77088, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 0\n",
      "91/91 [==============================] - 27s 297ms/step - loss: 0.9378 - acc: 0.5983 - val_loss: 0.7709 - val_acc: 0.6602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.6480 - acc: 0.7240\n",
      "Epoch 00002: val_loss improved from 0.77088 to 0.60682, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 0\n",
      "91/91 [==============================] - 14s 158ms/step - loss: 0.6448 - acc: 0.7257 - val_loss: 0.6068 - val_acc: 0.7434\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4312 - acc: 0.8368\n",
      "Epoch 00003: val_loss improved from 0.60682 to 0.58838, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 0\n",
      "91/91 [==============================] - 14s 158ms/step - loss: 0.4324 - acc: 0.8365 - val_loss: 0.5884 - val_acc: 0.7670\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3311 - acc: 0.8774\n",
      "Epoch 00004: val_loss did not improve from 0.58838\n",
      "91/91 [==============================] - 14s 159ms/step - loss: 0.3291 - acc: 0.8781 - val_loss: 0.6384 - val_acc: 0.7614\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.8972\n",
      "Epoch 00005: val_loss did not improve from 0.58838\n",
      "91/91 [==============================] - 15s 160ms/step - loss: 0.2583 - acc: 0.8981 - val_loss: 0.6657 - val_acc: 0.7712\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.9313\n",
      "Epoch 00006: val_loss did not improve from 0.58838\n",
      "91/91 [==============================] - 15s 161ms/step - loss: 0.1892 - acc: 0.9313 - val_loss: 0.6700 - val_acc: 0.7836\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9556\n",
      "Epoch 00007: val_loss did not improve from 0.58838\n",
      "91/91 [==============================] - 14s 159ms/step - loss: 0.1288 - acc: 0.9554 - val_loss: 0.7505 - val_acc: 0.7670\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9715\n",
      "Epoch 00008: val_loss did not improve from 0.58838\n",
      "91/91 [==============================] - 15s 161ms/step - loss: 0.0880 - acc: 0.9715 - val_loss: 0.8214 - val_acc: 0.7725\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant\n",
      "2018-10-24 12:10:19.892771\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 1\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9962 - acc: 0.5806\n",
      "Epoch 00001: val_loss improved from inf to 0.87302, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 1\n",
      "91/91 [==============================] - 25s 280ms/step - loss: 0.9937 - acc: 0.5815 - val_loss: 0.8730 - val_acc: 0.6172\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.7160 - acc: 0.6819\n",
      "Epoch 00002: val_loss improved from 0.87302 to 0.65635, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 1\n",
      "91/91 [==============================] - 15s 160ms/step - loss: 0.7124 - acc: 0.6835 - val_loss: 0.6563 - val_acc: 0.7212\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4744 - acc: 0.8135\n",
      "Epoch 00003: val_loss improved from 0.65635 to 0.57837, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 1\n",
      "91/91 [==============================] - 15s 160ms/step - loss: 0.4762 - acc: 0.8125 - val_loss: 0.5784 - val_acc: 0.7698\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3308 - acc: 0.8750\n",
      "Epoch 00004: val_loss improved from 0.57837 to 0.56971, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 1\n",
      "91/91 [==============================] - 14s 157ms/step - loss: 0.3326 - acc: 0.8750 - val_loss: 0.5697 - val_acc: 0.7892\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2470 - acc: 0.9118\n",
      "Epoch 00005: val_loss did not improve from 0.56971\n",
      "91/91 [==============================] - 14s 157ms/step - loss: 0.2484 - acc: 0.9114 - val_loss: 0.6041 - val_acc: 0.7684\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.9455\n",
      "Epoch 00006: val_loss did not improve from 0.56971\n",
      "91/91 [==============================] - 14s 159ms/step - loss: 0.1670 - acc: 0.9450 - val_loss: 0.6593 - val_acc: 0.7892\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9601\n",
      "Epoch 00007: val_loss did not improve from 0.56971\n",
      "91/91 [==============================] - 15s 160ms/step - loss: 0.1226 - acc: 0.9598 - val_loss: 0.7300 - val_acc: 0.7781\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9760\n",
      "Epoch 00008: val_loss did not improve from 0.56971\n",
      "91/91 [==============================] - 14s 157ms/step - loss: 0.0837 - acc: 0.9756 - val_loss: 0.7681 - val_acc: 0.7725\n",
      "Epoch 9/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9851\n",
      "Epoch 00009: val_loss did not improve from 0.56971\n",
      "91/91 [==============================] - 15s 160ms/step - loss: 0.0590 - acc: 0.9849 - val_loss: 0.7950 - val_acc: 0.7795\n",
      "Epoch 00009: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant\n",
      "2018-10-24 12:12:50.095335\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 2\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9715 - acc: 0.5774\n",
      "Epoch 00001: val_loss improved from inf to 0.83691, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 2\n",
      "91/91 [==============================] - 27s 292ms/step - loss: 0.9723 - acc: 0.5777 - val_loss: 0.8369 - val_acc: 0.6325\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.6813 - acc: 0.7010\n",
      "Epoch 00002: val_loss improved from 0.83691 to 0.68567, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 2\n",
      "91/91 [==============================] - 15s 163ms/step - loss: 0.6804 - acc: 0.7017 - val_loss: 0.6857 - val_acc: 0.7157\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4627 - acc: 0.8201\n",
      "Epoch 00003: val_loss improved from 0.68567 to 0.58698, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 2\n",
      "91/91 [==============================] - 15s 165ms/step - loss: 0.4623 - acc: 0.8207 - val_loss: 0.5870 - val_acc: 0.7573\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.8778\n",
      "Epoch 00004: val_loss improved from 0.58698 to 0.57306, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 2\n",
      "91/91 [==============================] - 15s 162ms/step - loss: 0.3160 - acc: 0.8770 - val_loss: 0.5731 - val_acc: 0.7725\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2124 - acc: 0.9281\n",
      "Epoch 00005: val_loss did not improve from 0.57306\n",
      "91/91 [==============================] - 15s 159ms/step - loss: 0.2116 - acc: 0.9286 - val_loss: 0.6683 - val_acc: 0.7614\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1600 - acc: 0.9462\n",
      "Epoch 00006: val_loss did not improve from 0.57306\n",
      "91/91 [==============================] - 15s 162ms/step - loss: 0.1599 - acc: 0.9457 - val_loss: 0.6728 - val_acc: 0.7809\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9684\n",
      "Epoch 00007: val_loss did not improve from 0.57306\n",
      "91/91 [==============================] - 15s 164ms/step - loss: 0.1040 - acc: 0.9684 - val_loss: 0.7619 - val_acc: 0.7878\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9750\n",
      "Epoch 00008: val_loss did not improve from 0.57306\n",
      "91/91 [==============================] - 15s 160ms/step - loss: 0.0795 - acc: 0.9749 - val_loss: 0.7840 - val_acc: 0.7878\n",
      "Epoch 9/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9833\n",
      "Epoch 00009: val_loss did not improve from 0.57306\n",
      "91/91 [==============================] - 15s 161ms/step - loss: 0.0517 - acc: 0.9835 - val_loss: 0.8594 - val_acc: 0.7906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant\n",
      "2018-10-24 12:15:23.874768\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 3\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9802 - acc: 0.5753\n",
      "Epoch 00001: val_loss improved from inf to 0.85798, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 3\n",
      "91/91 [==============================] - 27s 292ms/step - loss: 0.9821 - acc: 0.5736 - val_loss: 0.8580 - val_acc: 0.6172\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.7186 - acc: 0.6937\n",
      "Epoch 00002: val_loss improved from 0.85798 to 0.64354, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 3\n",
      "91/91 [==============================] - 15s 166ms/step - loss: 0.7162 - acc: 0.6941 - val_loss: 0.6435 - val_acc: 0.7434\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4760 - acc: 0.8170\n",
      "Epoch 00003: val_loss improved from 0.64354 to 0.62258, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 3\n",
      "91/91 [==============================] - 15s 166ms/step - loss: 0.4739 - acc: 0.8180 - val_loss: 0.6226 - val_acc: 0.7642\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.8736\n",
      "Epoch 00004: val_loss did not improve from 0.62258\n",
      "91/91 [==============================] - 15s 164ms/step - loss: 0.3272 - acc: 0.8729 - val_loss: 0.6451 - val_acc: 0.7559\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9153\n",
      "Epoch 00005: val_loss did not improve from 0.62258\n",
      "91/91 [==============================] - 15s 165ms/step - loss: 0.2402 - acc: 0.9155 - val_loss: 0.6631 - val_acc: 0.7670\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1735 - acc: 0.9437\n",
      "Epoch 00006: val_loss did not improve from 0.62258\n",
      "91/91 [==============================] - 15s 162ms/step - loss: 0.1728 - acc: 0.9440 - val_loss: 0.7202 - val_acc: 0.7739\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9646\n",
      "Epoch 00007: val_loss did not improve from 0.62258\n",
      "91/91 [==============================] - 15s 168ms/step - loss: 0.1225 - acc: 0.9650 - val_loss: 0.8119 - val_acc: 0.7712\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9729\n",
      "Epoch 00008: val_loss did not improve from 0.62258\n",
      "91/91 [==============================] - 15s 167ms/step - loss: 0.0902 - acc: 0.9729 - val_loss: 0.8575 - val_acc: 0.7767\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant\n",
      "2018-10-24 12:17:45.053589\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 4\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9984 - acc: 0.5785\n",
      "Epoch 00001: val_loss improved from inf to 0.89843, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 4\n",
      "91/91 [==============================] - 27s 298ms/step - loss: 0.9974 - acc: 0.5794 - val_loss: 0.8984 - val_acc: 0.6144\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.7323 - acc: 0.6830\n",
      "Epoch 00002: val_loss improved from 0.89843 to 0.70276, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 4\n",
      "91/91 [==============================] - 15s 166ms/step - loss: 0.7296 - acc: 0.6845 - val_loss: 0.7028 - val_acc: 0.7129\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.5077 - acc: 0.7899\n",
      "Epoch 00003: val_loss improved from 0.70276 to 0.57527, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 4\n",
      "91/91 [==============================] - 16s 172ms/step - loss: 0.5058 - acc: 0.7913 - val_loss: 0.5753 - val_acc: 0.7601\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3384 - acc: 0.8740\n",
      "Epoch 00004: val_loss improved from 0.57527 to 0.56791, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 4\n",
      "91/91 [==============================] - 15s 167ms/step - loss: 0.3393 - acc: 0.8733 - val_loss: 0.5679 - val_acc: 0.7809\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2578 - acc: 0.9101\n",
      "Epoch 00005: val_loss did not improve from 0.56791\n",
      "91/91 [==============================] - 15s 164ms/step - loss: 0.2585 - acc: 0.9097 - val_loss: 0.6001 - val_acc: 0.7642\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9378\n",
      "Epoch 00006: val_loss did not improve from 0.56791\n",
      "91/91 [==============================] - 15s 165ms/step - loss: 0.1843 - acc: 0.9385 - val_loss: 0.6435 - val_acc: 0.7725\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9594\n",
      "Epoch 00007: val_loss did not improve from 0.56791\n",
      "91/91 [==============================] - 16s 170ms/step - loss: 0.1274 - acc: 0.9598 - val_loss: 0.7717 - val_acc: 0.7601\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.9597\n",
      "Epoch 00008: val_loss did not improve from 0.56791\n",
      "91/91 [==============================] - 15s 168ms/step - loss: 0.1086 - acc: 0.9602 - val_loss: 0.7334 - val_acc: 0.7809\n",
      "Epoch 9/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9837\n",
      "Epoch 00009: val_loss did not improve from 0.56791\n",
      "91/91 [==============================] - 15s 163ms/step - loss: 0.0612 - acc: 0.9832 - val_loss: 0.8125 - val_acc: 0.7781\n",
      "Epoch 00009: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant\n",
      "2018-10-24 12:20:22.798841\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 5\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9969 - acc: 0.5750\n",
      "Epoch 00001: val_loss improved from inf to 0.87744, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 5\n",
      "91/91 [==============================] - 27s 300ms/step - loss: 0.9930 - acc: 0.5773 - val_loss: 0.8774 - val_acc: 0.6117\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.7311 - acc: 0.6903\n",
      "Epoch 00002: val_loss improved from 0.87744 to 0.65457, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 5\n",
      "91/91 [==============================] - 16s 171ms/step - loss: 0.7310 - acc: 0.6900 - val_loss: 0.6546 - val_acc: 0.7503\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4889 - acc: 0.8125\n",
      "Epoch 00003: val_loss improved from 0.65457 to 0.56691, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 5\n",
      "91/91 [==============================] - 16s 177ms/step - loss: 0.4873 - acc: 0.8125 - val_loss: 0.5669 - val_acc: 0.7712\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3237 - acc: 0.8799\n",
      "Epoch 00004: val_loss did not improve from 0.56691\n",
      "91/91 [==============================] - 16s 174ms/step - loss: 0.3257 - acc: 0.8788 - val_loss: 0.5698 - val_acc: 0.7822\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.9156\n",
      "Epoch 00005: val_loss did not improve from 0.56691\n",
      "91/91 [==============================] - 16s 171ms/step - loss: 0.2358 - acc: 0.9148 - val_loss: 0.6406 - val_acc: 0.7836\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9462\n",
      "Epoch 00006: val_loss did not improve from 0.56691\n",
      "91/91 [==============================] - 15s 167ms/step - loss: 0.1536 - acc: 0.9464 - val_loss: 0.7207 - val_acc: 0.7753\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9597\n",
      "Epoch 00007: val_loss did not improve from 0.56691\n",
      "91/91 [==============================] - 15s 168ms/step - loss: 0.1095 - acc: 0.9602 - val_loss: 0.7814 - val_acc: 0.7642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9747\n",
      "Epoch 00008: val_loss did not improve from 0.56691\n",
      "91/91 [==============================] - 16s 172ms/step - loss: 0.0768 - acc: 0.9746 - val_loss: 0.8439 - val_acc: 0.7656\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant\n",
      "2018-10-24 12:22:48.385755\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 6\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9646 - acc: 0.5875\n",
      "Epoch 00001: val_loss improved from inf to 0.77648, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 6\n",
      "91/91 [==============================] - 28s 303ms/step - loss: 0.9603 - acc: 0.5900 - val_loss: 0.7765 - val_acc: 0.6616\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.6526 - acc: 0.7260\n",
      "Epoch 00002: val_loss improved from 0.77648 to 0.62719, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 6\n",
      "91/91 [==============================] - 16s 176ms/step - loss: 0.6516 - acc: 0.7274 - val_loss: 0.6272 - val_acc: 0.7517\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4416 - acc: 0.8358\n",
      "Epoch 00003: val_loss improved from 0.62719 to 0.60535, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 6\n",
      "91/91 [==============================] - 16s 172ms/step - loss: 0.4428 - acc: 0.8348 - val_loss: 0.6053 - val_acc: 0.7601\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3088 - acc: 0.8819\n",
      "Epoch 00004: val_loss did not improve from 0.60535\n",
      "91/91 [==============================] - 16s 171ms/step - loss: 0.3076 - acc: 0.8822 - val_loss: 0.6144 - val_acc: 0.7684\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2170 - acc: 0.9222\n",
      "Epoch 00005: val_loss did not improve from 0.60535\n",
      "91/91 [==============================] - 16s 176ms/step - loss: 0.2162 - acc: 0.9224 - val_loss: 0.6389 - val_acc: 0.7739\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9524\n",
      "Epoch 00006: val_loss did not improve from 0.60535\n",
      "91/91 [==============================] - 16s 174ms/step - loss: 0.1456 - acc: 0.9526 - val_loss: 0.6852 - val_acc: 0.7795\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9674\n",
      "Epoch 00007: val_loss did not improve from 0.60535\n",
      "91/91 [==============================] - 16s 175ms/step - loss: 0.1045 - acc: 0.9667 - val_loss: 0.7870 - val_acc: 0.7947\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9802\n",
      "Epoch 00008: val_loss did not improve from 0.60535\n",
      "91/91 [==============================] - 15s 168ms/step - loss: 0.0669 - acc: 0.9801 - val_loss: 0.8232 - val_acc: 0.7892\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant\n",
      "2018-10-24 12:25:15.607144\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 7\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9409 - acc: 0.5854\n",
      "Epoch 00001: val_loss improved from inf to 0.83470, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 7\n",
      "91/91 [==============================] - 29s 318ms/step - loss: 0.9405 - acc: 0.5863 - val_loss: 0.8347 - val_acc: 0.6297\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.6387 - acc: 0.7382\n",
      "Epoch 00002: val_loss improved from 0.83470 to 0.62827, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 7\n",
      "91/91 [==============================] - 16s 178ms/step - loss: 0.6356 - acc: 0.7397 - val_loss: 0.6283 - val_acc: 0.7309\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4430 - acc: 0.8250\n",
      "Epoch 00003: val_loss improved from 0.62827 to 0.59094, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 7\n",
      "91/91 [==============================] - 16s 177ms/step - loss: 0.4423 - acc: 0.8269 - val_loss: 0.5909 - val_acc: 0.7490\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.8802\n",
      "Epoch 00004: val_loss improved from 0.59094 to 0.58753, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 7\n",
      "91/91 [==============================] - 16s 176ms/step - loss: 0.3225 - acc: 0.8801 - val_loss: 0.5875 - val_acc: 0.7795\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9156\n",
      "Epoch 00005: val_loss did not improve from 0.58753\n",
      "91/91 [==============================] - 16s 179ms/step - loss: 0.2264 - acc: 0.9162 - val_loss: 0.6477 - val_acc: 0.7725\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9476\n",
      "Epoch 00006: val_loss did not improve from 0.58753\n",
      "91/91 [==============================] - 16s 179ms/step - loss: 0.1543 - acc: 0.9478 - val_loss: 0.7230 - val_acc: 0.7753\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9677\n",
      "Epoch 00007: val_loss did not improve from 0.58753\n",
      "91/91 [==============================] - 16s 178ms/step - loss: 0.1023 - acc: 0.9670 - val_loss: 0.7766 - val_acc: 0.7712\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9774\n",
      "Epoch 00008: val_loss did not improve from 0.58753\n",
      "91/91 [==============================] - 16s 178ms/step - loss: 0.0731 - acc: 0.9777 - val_loss: 0.8139 - val_acc: 0.7739\n",
      "Epoch 9/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9833\n",
      "Epoch 00009: val_loss did not improve from 0.58753\n",
      "91/91 [==============================] - 16s 174ms/step - loss: 0.0578 - acc: 0.9835 - val_loss: 0.8994 - val_acc: 0.7517\n",
      "Epoch 00009: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant\n",
      "2018-10-24 12:28:03.095778\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 8\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9790 - acc: 0.5840\n",
      "Epoch 00001: val_loss improved from inf to 0.81018, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 8\n",
      "91/91 [==============================] - 31s 340ms/step - loss: 0.9795 - acc: 0.5825 - val_loss: 0.8102 - val_acc: 0.6352\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.6776 - acc: 0.7045\n",
      "Epoch 00002: val_loss improved from 0.81018 to 0.67016, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 8\n",
      "91/91 [==============================] - 16s 176ms/step - loss: 0.6747 - acc: 0.7058 - val_loss: 0.6702 - val_acc: 0.7018\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4568 - acc: 0.8208\n",
      "Epoch 00003: val_loss improved from 0.67016 to 0.60254, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 8\n",
      "91/91 [==============================] - 16s 176ms/step - loss: 0.4547 - acc: 0.8221 - val_loss: 0.6025 - val_acc: 0.7448\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3211 - acc: 0.8806\n",
      "Epoch 00004: val_loss did not improve from 0.60254\n",
      "91/91 [==============================] - 16s 178ms/step - loss: 0.3211 - acc: 0.8808 - val_loss: 0.6137 - val_acc: 0.7545\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2255 - acc: 0.9156\n",
      "Epoch 00005: val_loss did not improve from 0.60254\n",
      "91/91 [==============================] - 16s 180ms/step - loss: 0.2271 - acc: 0.9148 - val_loss: 0.6517 - val_acc: 0.7712\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.9444\n",
      "Epoch 00006: val_loss did not improve from 0.60254\n",
      "91/91 [==============================] - 16s 181ms/step - loss: 0.1592 - acc: 0.9444 - val_loss: 0.7092 - val_acc: 0.7822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9656\n",
      "Epoch 00007: val_loss did not improve from 0.60254\n",
      "91/91 [==============================] - 16s 180ms/step - loss: 0.1069 - acc: 0.9653 - val_loss: 0.7763 - val_acc: 0.7795\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9771\n",
      "Epoch 00008: val_loss did not improve from 0.60254\n",
      "91/91 [==============================] - 16s 174ms/step - loss: 0.0784 - acc: 0.9760 - val_loss: 0.8212 - val_acc: 0.7781\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant\n",
      "2018-10-24 12:30:37.050079\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 9\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9575 - acc: 0.5913\n",
      "Epoch 00001: val_loss improved from inf to 0.77710, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 9\n",
      "91/91 [==============================] - 30s 330ms/step - loss: 0.9541 - acc: 0.5938 - val_loss: 0.7771 - val_acc: 0.6505\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.6278 - acc: 0.7469\n",
      "Epoch 00002: val_loss improved from 0.77710 to 0.64199, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 9\n",
      "91/91 [==============================] - 16s 178ms/step - loss: 0.6255 - acc: 0.7479 - val_loss: 0.6420 - val_acc: 0.7184\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4321 - acc: 0.8226\n",
      "Epoch 00003: val_loss improved from 0.64199 to 0.60894, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 9\n",
      "91/91 [==============================] - 16s 178ms/step - loss: 0.4311 - acc: 0.8232 - val_loss: 0.6089 - val_acc: 0.7406\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.8795\n",
      "Epoch 00004: val_loss improved from 0.60894 to 0.60342, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET Probability 1/normal restaurant 9\n",
      "91/91 [==============================] - 16s 180ms/step - loss: 0.3219 - acc: 0.8798 - val_loss: 0.6034 - val_acc: 0.7725\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9215\n",
      "Epoch 00005: val_loss did not improve from 0.60342\n",
      "91/91 [==============================] - 16s 179ms/step - loss: 0.2291 - acc: 0.9214 - val_loss: 0.6055 - val_acc: 0.7795\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9365\n",
      "Epoch 00006: val_loss did not improve from 0.60342\n",
      "91/91 [==============================] - 17s 185ms/step - loss: 0.1767 - acc: 0.9361 - val_loss: 0.6757 - val_acc: 0.7712\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9587\n",
      "Epoch 00007: val_loss did not improve from 0.60342\n",
      "91/91 [==============================] - 17s 182ms/step - loss: 0.1287 - acc: 0.9581 - val_loss: 0.7510 - val_acc: 0.7614\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9715\n",
      "Epoch 00008: val_loss did not improve from 0.60342\n",
      "91/91 [==============================] - 16s 178ms/step - loss: 0.0931 - acc: 0.9715 - val_loss: 0.7393 - val_acc: 0.7795\n",
      "Epoch 9/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9812\n",
      "Epoch 00009: val_loss did not improve from 0.60342\n",
      "91/91 [==============================] - 17s 183ms/step - loss: 0.0663 - acc: 0.9811 - val_loss: 0.8303 - val_acc: 0.7684\n",
      "Epoch 00009: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 12:35:00.530446\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 0\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8214 - acc: 0.6385\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62601, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 0\n",
      "91/91 [==============================] - 30s 334ms/step - loss: 0.8194 - acc: 0.6405 - val_loss: 0.6260 - val_acc: 0.7337\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4614 - acc: 0.8069\n",
      "Epoch 00002: val_loss improved from 0.62601 to 0.60347, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 0\n",
      "91/91 [==============================] - 16s 177ms/step - loss: 0.4608 - acc: 0.8070 - val_loss: 0.6035 - val_acc: 0.7323\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2669 - acc: 0.8962\n",
      "Epoch 00003: val_loss improved from 0.60347 to 0.58233, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 0\n",
      "91/91 [==============================] - 16s 181ms/step - loss: 0.2678 - acc: 0.8963 - val_loss: 0.5823 - val_acc: 0.7670\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9490\n",
      "Epoch 00004: val_loss did not improve from 0.58233\n",
      "91/91 [==============================] - 16s 179ms/step - loss: 0.1502 - acc: 0.9492 - val_loss: 0.6643 - val_acc: 0.7684\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9806\n",
      "Epoch 00005: val_loss did not improve from 0.58233\n",
      "91/91 [==============================] - 16s 176ms/step - loss: 0.0753 - acc: 0.9801 - val_loss: 0.7608 - val_acc: 0.7601\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9892\n",
      "Epoch 00006: val_loss did not improve from 0.58233\n",
      "91/91 [==============================] - 16s 175ms/step - loss: 0.0381 - acc: 0.9890 - val_loss: 0.8723 - val_acc: 0.7781\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9944\n",
      "Epoch 00007: val_loss did not improve from 0.58233\n",
      "91/91 [==============================] - 16s 174ms/step - loss: 0.0220 - acc: 0.9945 - val_loss: 0.8819 - val_acc: 0.7767\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9990\n",
      "Epoch 00008: val_loss did not improve from 0.58233\n",
      "91/91 [==============================] - 16s 180ms/step - loss: 0.0088 - acc: 0.9990 - val_loss: 0.9846 - val_acc: 0.7670\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 12:37:34.900331\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 1\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8261 - acc: 0.6368\n",
      "Epoch 00001: val_loss improved from inf to 0.66561, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 1\n",
      "91/91 [==============================] - 30s 335ms/step - loss: 0.8237 - acc: 0.6385 - val_loss: 0.6656 - val_acc: 0.7226\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4717 - acc: 0.8069\n",
      "Epoch 00002: val_loss improved from 0.66561 to 0.60371, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 1\n",
      "91/91 [==============================] - 17s 182ms/step - loss: 0.4690 - acc: 0.8081 - val_loss: 0.6037 - val_acc: 0.7503\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2771 - acc: 0.8962\n",
      "Epoch 00003: val_loss improved from 0.60371 to 0.59851, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 1\n",
      "91/91 [==============================] - 17s 182ms/step - loss: 0.2767 - acc: 0.8959 - val_loss: 0.5985 - val_acc: 0.7601\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9503\n",
      "Epoch 00004: val_loss did not improve from 0.59851\n",
      "91/91 [==============================] - 16s 175ms/step - loss: 0.1519 - acc: 0.9509 - val_loss: 0.6383 - val_acc: 0.7698\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9753\n",
      "Epoch 00005: val_loss did not improve from 0.59851\n",
      "91/91 [==============================] - 16s 177ms/step - loss: 0.0794 - acc: 0.9753 - val_loss: 0.7626 - val_acc: 0.7684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9892\n",
      "Epoch 00006: val_loss did not improve from 0.59851\n",
      "91/91 [==============================] - 16s 178ms/step - loss: 0.0381 - acc: 0.9887 - val_loss: 0.8362 - val_acc: 0.7767\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9958\n",
      "Epoch 00007: val_loss did not improve from 0.59851\n",
      "91/91 [==============================] - 17s 182ms/step - loss: 0.0207 - acc: 0.9959 - val_loss: 0.8652 - val_acc: 0.7698\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9983\n",
      "Epoch 00008: val_loss did not improve from 0.59851\n",
      "91/91 [==============================] - 16s 179ms/step - loss: 0.0092 - acc: 0.9983 - val_loss: 0.9689 - val_acc: 0.7656\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 12:40:09.931144\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 2\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8426 - acc: 0.6389\n",
      "Epoch 00001: val_loss improved from inf to 0.65322, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 2\n",
      "91/91 [==============================] - 31s 342ms/step - loss: 0.8426 - acc: 0.6378 - val_loss: 0.6532 - val_acc: 0.7379\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4825 - acc: 0.8007\n",
      "Epoch 00002: val_loss improved from 0.65322 to 0.57610, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 2\n",
      "91/91 [==============================] - 16s 181ms/step - loss: 0.4810 - acc: 0.8015 - val_loss: 0.5761 - val_acc: 0.7725\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.8861\n",
      "Epoch 00003: val_loss did not improve from 0.57610\n",
      "91/91 [==============================] - 16s 179ms/step - loss: 0.2912 - acc: 0.8860 - val_loss: 0.6028 - val_acc: 0.7712\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9444\n",
      "Epoch 00004: val_loss did not improve from 0.57610\n",
      "91/91 [==============================] - 17s 184ms/step - loss: 0.1619 - acc: 0.9437 - val_loss: 0.6935 - val_acc: 0.7781\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9698\n",
      "Epoch 00005: val_loss did not improve from 0.57610\n",
      "91/91 [==============================] - 17s 183ms/step - loss: 0.0860 - acc: 0.9701 - val_loss: 0.7610 - val_acc: 0.7642\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9875\n",
      "Epoch 00006: val_loss did not improve from 0.57610\n",
      "91/91 [==============================] - 16s 179ms/step - loss: 0.0454 - acc: 0.9876 - val_loss: 0.9021 - val_acc: 0.7628\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9934\n",
      "Epoch 00007: val_loss did not improve from 0.57610\n",
      "91/91 [==============================] - 16s 181ms/step - loss: 0.0276 - acc: 0.9935 - val_loss: 0.9014 - val_acc: 0.7642\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 12:42:30.578920\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 3\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8020 - acc: 0.6531\n",
      "Epoch 00001: val_loss improved from inf to 0.61382, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 3\n",
      "91/91 [==============================] - 33s 358ms/step - loss: 0.7988 - acc: 0.6543 - val_loss: 0.6138 - val_acc: 0.7379\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4494 - acc: 0.8163\n",
      "Epoch 00002: val_loss improved from 0.61382 to 0.61121, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 3\n",
      "91/91 [==============================] - 17s 189ms/step - loss: 0.4478 - acc: 0.8169 - val_loss: 0.6112 - val_acc: 0.7309\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2601 - acc: 0.9024\n",
      "Epoch 00003: val_loss did not improve from 0.61121\n",
      "91/91 [==============================] - 17s 187ms/step - loss: 0.2593 - acc: 0.9031 - val_loss: 0.6120 - val_acc: 0.7503\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.9549\n",
      "Epoch 00004: val_loss did not improve from 0.61121\n",
      "91/91 [==============================] - 17s 183ms/step - loss: 0.1366 - acc: 0.9547 - val_loss: 0.7162 - val_acc: 0.7601\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9806\n",
      "Epoch 00005: val_loss did not improve from 0.61121\n",
      "91/91 [==============================] - 17s 182ms/step - loss: 0.0660 - acc: 0.9808 - val_loss: 0.8230 - val_acc: 0.7725\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9938\n",
      "Epoch 00006: val_loss did not improve from 0.61121\n",
      "91/91 [==============================] - 17s 183ms/step - loss: 0.0282 - acc: 0.9938 - val_loss: 0.9265 - val_acc: 0.7601\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9976\n",
      "Epoch 00007: val_loss did not improve from 0.61121\n",
      "91/91 [==============================] - 17s 182ms/step - loss: 0.0137 - acc: 0.9976 - val_loss: 1.0253 - val_acc: 0.7670\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 12:44:55.008574\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 4\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8124 - acc: 0.6521\n",
      "Epoch 00001: val_loss improved from inf to 0.73159, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 4\n",
      "91/91 [==============================] - 34s 372ms/step - loss: 0.8119 - acc: 0.6515 - val_loss: 0.7316 - val_acc: 0.6768\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4708 - acc: 0.8101\n",
      "Epoch 00002: val_loss improved from 0.73159 to 0.54081, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 4\n",
      "91/91 [==============================] - 17s 190ms/step - loss: 0.4692 - acc: 0.8108 - val_loss: 0.5408 - val_acc: 0.7614\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2732 - acc: 0.8938\n",
      "Epoch 00003: val_loss did not improve from 0.54081\n",
      "91/91 [==============================] - 17s 189ms/step - loss: 0.2717 - acc: 0.8942 - val_loss: 0.6101 - val_acc: 0.7670\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9413\n",
      "Epoch 00004: val_loss did not improve from 0.54081\n",
      "91/91 [==============================] - 18s 192ms/step - loss: 0.1552 - acc: 0.9413 - val_loss: 0.6450 - val_acc: 0.7739\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9722\n",
      "Epoch 00005: val_loss did not improve from 0.54081\n",
      "91/91 [==============================] - 17s 189ms/step - loss: 0.0829 - acc: 0.9722 - val_loss: 0.7958 - val_acc: 0.7587\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9872\n",
      "Epoch 00006: val_loss did not improve from 0.54081\n",
      "91/91 [==============================] - 17s 188ms/step - loss: 0.0448 - acc: 0.9873 - val_loss: 0.7869 - val_acc: 0.7753\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9972\n",
      "Epoch 00007: val_loss did not improve from 0.54081\n",
      "91/91 [==============================] - 17s 187ms/step - loss: 0.0191 - acc: 0.9969 - val_loss: 0.8770 - val_acc: 0.7684\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 12:47:25.897659\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 5\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8141 - acc: 0.6500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63536, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 34s 379ms/step - loss: 0.8136 - acc: 0.6505 - val_loss: 0.6354 - val_acc: 0.7212\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4569 - acc: 0.8194\n",
      "Epoch 00002: val_loss improved from 0.63536 to 0.56317, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 5\n",
      "91/91 [==============================] - 18s 194ms/step - loss: 0.4565 - acc: 0.8200 - val_loss: 0.5632 - val_acc: 0.7573\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2705 - acc: 0.8969\n",
      "Epoch 00003: val_loss did not improve from 0.56317\n",
      "91/91 [==============================] - 18s 196ms/step - loss: 0.2698 - acc: 0.8973 - val_loss: 0.5905 - val_acc: 0.7712\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1536 - acc: 0.9427\n",
      "Epoch 00004: val_loss did not improve from 0.56317\n",
      "91/91 [==============================] - 18s 196ms/step - loss: 0.1530 - acc: 0.9433 - val_loss: 0.6959 - val_acc: 0.7753\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9750\n",
      "Epoch 00005: val_loss did not improve from 0.56317\n",
      "91/91 [==============================] - 17s 190ms/step - loss: 0.0728 - acc: 0.9753 - val_loss: 0.7981 - val_acc: 0.7739\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9903\n",
      "Epoch 00006: val_loss did not improve from 0.56317\n",
      "91/91 [==============================] - 18s 196ms/step - loss: 0.0358 - acc: 0.9904 - val_loss: 0.8393 - val_acc: 0.7753\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9944\n",
      "Epoch 00007: val_loss did not improve from 0.56317\n",
      "91/91 [==============================] - 18s 195ms/step - loss: 0.0194 - acc: 0.9945 - val_loss: 0.9534 - val_acc: 0.7712\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 12:49:59.072703\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 6\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8505 - acc: 0.6264\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66204, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 6\n",
      "91/91 [==============================] - 35s 379ms/step - loss: 0.8487 - acc: 0.6285 - val_loss: 0.6620 - val_acc: 0.7240\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4959 - acc: 0.7983\n",
      "Epoch 00002: val_loss improved from 0.66204 to 0.57634, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 6\n",
      "91/91 [==============================] - 18s 198ms/step - loss: 0.4971 - acc: 0.7977 - val_loss: 0.5763 - val_acc: 0.7559\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.8913\n",
      "Epoch 00003: val_loss improved from 0.57634 to 0.56453, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 6\n",
      "91/91 [==============================] - 18s 200ms/step - loss: 0.2940 - acc: 0.8904 - val_loss: 0.5645 - val_acc: 0.7753\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9476\n",
      "Epoch 00004: val_loss did not improve from 0.56453\n",
      "91/91 [==============================] - 18s 198ms/step - loss: 0.1546 - acc: 0.9478 - val_loss: 0.6969 - val_acc: 0.7531\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9767\n",
      "Epoch 00005: val_loss did not improve from 0.56453\n",
      "91/91 [==============================] - 18s 200ms/step - loss: 0.0780 - acc: 0.9770 - val_loss: 0.7704 - val_acc: 0.7517\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9924\n",
      "Epoch 00006: val_loss did not improve from 0.56453\n",
      "91/91 [==============================] - 18s 202ms/step - loss: 0.0327 - acc: 0.9924 - val_loss: 0.9065 - val_acc: 0.7448\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9951\n",
      "Epoch 00007: val_loss did not improve from 0.56453\n",
      "91/91 [==============================] - 18s 201ms/step - loss: 0.0200 - acc: 0.9952 - val_loss: 0.9811 - val_acc: 0.7545\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9986\n",
      "Epoch 00008: val_loss did not improve from 0.56453\n",
      "91/91 [==============================] - 18s 203ms/step - loss: 0.0109 - acc: 0.9986 - val_loss: 1.0191 - val_acc: 0.7559\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 12:52:53.664164\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 7\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8664 - acc: 0.6410\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64810, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 7\n",
      "91/91 [==============================] - 35s 389ms/step - loss: 0.8645 - acc: 0.6418 - val_loss: 0.6481 - val_acc: 0.7226\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.7865\n",
      "Epoch 00002: val_loss improved from 0.64810 to 0.55395, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 7\n",
      "91/91 [==============================] - 19s 206ms/step - loss: 0.5234 - acc: 0.7879 - val_loss: 0.5539 - val_acc: 0.7878\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.8806\n",
      "Epoch 00003: val_loss improved from 0.55395 to 0.55375, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 7\n",
      "91/91 [==============================] - 19s 204ms/step - loss: 0.3214 - acc: 0.8801 - val_loss: 0.5537 - val_acc: 0.7864\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9278\n",
      "Epoch 00004: val_loss did not improve from 0.55375\n",
      "91/91 [==============================] - 19s 206ms/step - loss: 0.1972 - acc: 0.9282 - val_loss: 0.6133 - val_acc: 0.7781\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9639\n",
      "Epoch 00005: val_loss did not improve from 0.55375\n",
      "91/91 [==============================] - 19s 207ms/step - loss: 0.1137 - acc: 0.9636 - val_loss: 0.6914 - val_acc: 0.7795\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9778\n",
      "Epoch 00006: val_loss did not improve from 0.55375\n",
      "91/91 [==============================] - 19s 204ms/step - loss: 0.0702 - acc: 0.9780 - val_loss: 0.7753 - val_acc: 0.7670\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9875\n",
      "Epoch 00007: val_loss did not improve from 0.55375\n",
      "91/91 [==============================] - 18s 199ms/step - loss: 0.0401 - acc: 0.9876 - val_loss: 0.7978 - val_acc: 0.7698\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9934\n",
      "Epoch 00008: val_loss did not improve from 0.55375\n",
      "91/91 [==============================] - 18s 201ms/step - loss: 0.0264 - acc: 0.9931 - val_loss: 0.8458 - val_acc: 0.7753\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 12:55:51.274658\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 8\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8612 - acc: 0.6135\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65910, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 8\n",
      "91/91 [==============================] - 36s 401ms/step - loss: 0.8568 - acc: 0.6158 - val_loss: 0.6591 - val_acc: 0.7295\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4807 - acc: 0.8024\n",
      "Epoch 00002: val_loss improved from 0.65910 to 0.55124, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 8\n",
      "91/91 [==============================] - 19s 206ms/step - loss: 0.4792 - acc: 0.8032 - val_loss: 0.5512 - val_acc: 0.7628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2920 - acc: 0.8941\n",
      "Epoch 00003: val_loss did not improve from 0.55124\n",
      "91/91 [==============================] - 19s 205ms/step - loss: 0.2906 - acc: 0.8946 - val_loss: 0.5978 - val_acc: 0.7573\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9444\n",
      "Epoch 00004: val_loss did not improve from 0.55124\n",
      "91/91 [==============================] - 19s 210ms/step - loss: 0.1616 - acc: 0.9437 - val_loss: 0.7099 - val_acc: 0.7503\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9753\n",
      "Epoch 00005: val_loss did not improve from 0.55124\n",
      "91/91 [==============================] - 19s 211ms/step - loss: 0.0816 - acc: 0.9756 - val_loss: 0.7759 - val_acc: 0.7684\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9892\n",
      "Epoch 00006: val_loss did not improve from 0.55124\n",
      "91/91 [==============================] - 19s 209ms/step - loss: 0.0389 - acc: 0.9887 - val_loss: 0.8908 - val_acc: 0.7601\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9951\n",
      "Epoch 00007: val_loss did not improve from 0.55124\n",
      "91/91 [==============================] - 19s 212ms/step - loss: 0.0230 - acc: 0.9952 - val_loss: 0.9522 - val_acc: 0.7614\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 12:58:34.525680\n",
      "../Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 9\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8261 - acc: 0.6375\n",
      "Epoch 00001: val_loss improved from inf to 0.61297, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 9\n",
      "91/91 [==============================] - 37s 407ms/step - loss: 0.8233 - acc: 0.6395 - val_loss: 0.6130 - val_acc: 0.7365\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4662 - acc: 0.8198\n",
      "Epoch 00002: val_loss improved from 0.61297 to 0.60880, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TAET_E Probability 1/normal restaurant 9\n",
      "91/91 [==============================] - 19s 214ms/step - loss: 0.4679 - acc: 0.8187 - val_loss: 0.6088 - val_acc: 0.7393\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2777 - acc: 0.8958\n",
      "Epoch 00003: val_loss did not improve from 0.60880\n",
      "91/91 [==============================] - 19s 212ms/step - loss: 0.2768 - acc: 0.8956 - val_loss: 0.6114 - val_acc: 0.7628\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9458\n",
      "Epoch 00004: val_loss did not improve from 0.60880\n",
      "91/91 [==============================] - 19s 211ms/step - loss: 0.1527 - acc: 0.9454 - val_loss: 0.6515 - val_acc: 0.7684\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9764\n",
      "Epoch 00005: val_loss did not improve from 0.60880\n",
      "91/91 [==============================] - 19s 207ms/step - loss: 0.0808 - acc: 0.9766 - val_loss: 0.7405 - val_acc: 0.7712\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9872\n",
      "Epoch 00006: val_loss did not improve from 0.60880\n",
      "91/91 [==============================] - 19s 213ms/step - loss: 0.0420 - acc: 0.9873 - val_loss: 0.8326 - val_acc: 0.7725\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9969\n",
      "Epoch 00007: val_loss did not improve from 0.60880\n",
      "91/91 [==============================] - 20s 217ms/step - loss: 0.0178 - acc: 0.9969 - val_loss: 0.8887 - val_acc: 0.7781\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 13:03:24.665526\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 0\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9793 - acc: 0.5847\n",
      "Epoch 00001: val_loss improved from inf to 0.84191, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 0\n",
      "91/91 [==============================] - 39s 431ms/step - loss: 0.9771 - acc: 0.5852 - val_loss: 0.8419 - val_acc: 0.6227\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.6733 - acc: 0.7156\n",
      "Epoch 00002: val_loss improved from 0.84191 to 0.63305, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 0\n",
      "91/91 [==============================] - 20s 225ms/step - loss: 0.6759 - acc: 0.7144 - val_loss: 0.6330 - val_acc: 0.7406\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4621 - acc: 0.8108\n",
      "Epoch 00003: val_loss improved from 0.63305 to 0.59710, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 0\n",
      "91/91 [==============================] - 21s 228ms/step - loss: 0.4619 - acc: 0.8105 - val_loss: 0.5971 - val_acc: 0.7393\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3443 - acc: 0.8708\n",
      "Epoch 00004: val_loss did not improve from 0.59710\n",
      "91/91 [==============================] - 21s 227ms/step - loss: 0.3431 - acc: 0.8712 - val_loss: 0.6072 - val_acc: 0.7712\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9049\n",
      "Epoch 00005: val_loss did not improve from 0.59710\n",
      "91/91 [==============================] - 21s 226ms/step - loss: 0.2461 - acc: 0.9056 - val_loss: 0.6316 - val_acc: 0.7725\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9347\n",
      "Epoch 00006: val_loss did not improve from 0.59710\n",
      "91/91 [==============================] - 21s 227ms/step - loss: 0.1804 - acc: 0.9347 - val_loss: 0.7355 - val_acc: 0.7642\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9549\n",
      "Epoch 00007: val_loss did not improve from 0.59710\n",
      "91/91 [==============================] - 20s 224ms/step - loss: 0.1353 - acc: 0.9550 - val_loss: 0.7697 - val_acc: 0.7767\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9760\n",
      "Epoch 00008: val_loss did not improve from 0.59710\n",
      "91/91 [==============================] - 21s 230ms/step - loss: 0.0852 - acc: 0.9763 - val_loss: 0.8041 - val_acc: 0.7850\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 13:06:41.980017\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 1\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9920 - acc: 0.5687\n",
      "Epoch 00001: val_loss improved from inf to 0.82905, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 1\n",
      "91/91 [==============================] - 40s 442ms/step - loss: 0.9888 - acc: 0.5705 - val_loss: 0.8291 - val_acc: 0.6061\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.6892 - acc: 0.7156\n",
      "Epoch 00002: val_loss improved from 0.82905 to 0.62781, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 1\n",
      "91/91 [==============================] - 21s 232ms/step - loss: 0.6871 - acc: 0.7170 - val_loss: 0.6278 - val_acc: 0.7393\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4691 - acc: 0.8059\n",
      "Epoch 00003: val_loss improved from 0.62781 to 0.59186, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 1\n",
      "91/91 [==============================] - 21s 234ms/step - loss: 0.4694 - acc: 0.8061 - val_loss: 0.5919 - val_acc: 0.7587\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3406 - acc: 0.8670\n",
      "Epoch 00004: val_loss did not improve from 0.59186\n",
      "91/91 [==============================] - 21s 233ms/step - loss: 0.3403 - acc: 0.8661 - val_loss: 0.6480 - val_acc: 0.7573\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.9069\n",
      "Epoch 00005: val_loss did not improve from 0.59186\n",
      "91/91 [==============================] - 22s 236ms/step - loss: 0.2430 - acc: 0.9076 - val_loss: 0.6592 - val_acc: 0.7587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1694 - acc: 0.9399\n",
      "Epoch 00006: val_loss did not improve from 0.59186\n",
      "91/91 [==============================] - 21s 231ms/step - loss: 0.1694 - acc: 0.9396 - val_loss: 0.7049 - val_acc: 0.7656\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9635\n",
      "Epoch 00007: val_loss did not improve from 0.59186\n",
      "91/91 [==============================] - 21s 231ms/step - loss: 0.1172 - acc: 0.9639 - val_loss: 0.7468 - val_acc: 0.7698\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9750\n",
      "Epoch 00008: val_loss did not improve from 0.59186\n",
      "91/91 [==============================] - 21s 228ms/step - loss: 0.0827 - acc: 0.9746 - val_loss: 0.8404 - val_acc: 0.7601\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 13:10:04.854794\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 2\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.8922 - acc: 0.6007\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.76372, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 2\n",
      "91/91 [==============================] - 44s 484ms/step - loss: 0.8910 - acc: 0.6021 - val_loss: 0.7637 - val_acc: 0.6852\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.5747 - acc: 0.7649\n",
      "Epoch 00002: val_loss improved from 0.76372 to 0.59161, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 2\n",
      "91/91 [==============================] - 21s 235ms/step - loss: 0.5735 - acc: 0.7647 - val_loss: 0.5916 - val_acc: 0.7587\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3925 - acc: 0.8458\n",
      "Epoch 00003: val_loss did not improve from 0.59161\n",
      "91/91 [==============================] - 22s 238ms/step - loss: 0.3921 - acc: 0.8461 - val_loss: 0.6053 - val_acc: 0.7601\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2791 - acc: 0.8948\n",
      "Epoch 00004: val_loss did not improve from 0.59161\n",
      "91/91 [==============================] - 21s 233ms/step - loss: 0.2789 - acc: 0.8946 - val_loss: 0.6348 - val_acc: 0.7587\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9302\n",
      "Epoch 00005: val_loss did not improve from 0.59161\n",
      "91/91 [==============================] - 22s 238ms/step - loss: 0.1966 - acc: 0.9310 - val_loss: 0.7108 - val_acc: 0.7684\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9476\n",
      "Epoch 00006: val_loss did not improve from 0.59161\n",
      "91/91 [==============================] - 21s 236ms/step - loss: 0.1409 - acc: 0.9478 - val_loss: 0.7719 - val_acc: 0.7545\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9688\n",
      "Epoch 00007: val_loss did not improve from 0.59161\n",
      "91/91 [==============================] - 21s 236ms/step - loss: 0.0970 - acc: 0.9684 - val_loss: 0.8763 - val_acc: 0.7559\n",
      "Epoch 00007: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 13:13:12.686514\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 3\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9920 - acc: 0.5837\n",
      "Epoch 00001: val_loss improved from inf to 0.88253, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 3\n",
      "91/91 [==============================] - 42s 460ms/step - loss: 0.9918 - acc: 0.5818 - val_loss: 0.8825 - val_acc: 0.6117\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.7059 - acc: 0.6962\n",
      "Epoch 00002: val_loss improved from 0.88253 to 0.63592, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 3\n",
      "91/91 [==============================] - 22s 240ms/step - loss: 0.7158 - acc: 0.6886 - val_loss: 0.6359 - val_acc: 0.7393\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4961 - acc: 0.8083\n",
      "Epoch 00003: val_loss improved from 0.63592 to 0.55061, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 3\n",
      "91/91 [==============================] - 22s 247ms/step - loss: 0.4947 - acc: 0.8084 - val_loss: 0.5506 - val_acc: 0.7781\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3346 - acc: 0.8788\n",
      "Epoch 00004: val_loss did not improve from 0.55061\n",
      "91/91 [==============================] - 22s 244ms/step - loss: 0.3338 - acc: 0.8791 - val_loss: 0.5836 - val_acc: 0.7670\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9045\n",
      "Epoch 00005: val_loss did not improve from 0.55061\n",
      "91/91 [==============================] - 23s 248ms/step - loss: 0.2485 - acc: 0.9049 - val_loss: 0.6058 - val_acc: 0.7906\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9368\n",
      "Epoch 00006: val_loss did not improve from 0.55061\n",
      "91/91 [==============================] - 22s 245ms/step - loss: 0.1749 - acc: 0.9371 - val_loss: 0.6637 - val_acc: 0.7878\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9594\n",
      "Epoch 00007: val_loss did not improve from 0.55061\n",
      "91/91 [==============================] - 22s 243ms/step - loss: 0.1237 - acc: 0.9591 - val_loss: 0.7051 - val_acc: 0.7975\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9736\n",
      "Epoch 00008: val_loss did not improve from 0.55061\n",
      "91/91 [==============================] - 22s 245ms/step - loss: 0.0867 - acc: 0.9736 - val_loss: 0.7718 - val_acc: 0.7850\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 13:16:45.859541\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 4\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9975 - acc: 0.5819\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.84754, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 4\n",
      "91/91 [==============================] - 43s 474ms/step - loss: 0.9984 - acc: 0.5804 - val_loss: 0.8475 - val_acc: 0.6117\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.7029 - acc: 0.6931\n",
      "Epoch 00002: val_loss improved from 0.84754 to 0.64872, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 4\n",
      "91/91 [==============================] - 23s 248ms/step - loss: 0.7007 - acc: 0.6948 - val_loss: 0.6487 - val_acc: 0.7226\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4655 - acc: 0.8240\n",
      "Epoch 00003: val_loss improved from 0.64872 to 0.59168, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 4\n",
      "91/91 [==============================] - 23s 251ms/step - loss: 0.4682 - acc: 0.8231 - val_loss: 0.5917 - val_acc: 0.7587\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3281 - acc: 0.8833\n",
      "Epoch 00004: val_loss improved from 0.59168 to 0.58998, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 4\n",
      "91/91 [==============================] - 23s 252ms/step - loss: 0.3280 - acc: 0.8836 - val_loss: 0.5900 - val_acc: 0.7739\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.9163\n",
      "Epoch 00005: val_loss did not improve from 0.58998\n",
      "91/91 [==============================] - 23s 248ms/step - loss: 0.2335 - acc: 0.9159 - val_loss: 0.6411 - val_acc: 0.7559\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9378\n",
      "Epoch 00006: val_loss did not improve from 0.58998\n",
      "91/91 [==============================] - 23s 250ms/step - loss: 0.1668 - acc: 0.9371 - val_loss: 0.6565 - val_acc: 0.7739\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/91 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9611\n",
      "Epoch 00007: val_loss did not improve from 0.58998\n",
      "91/91 [==============================] - 22s 247ms/step - loss: 0.1194 - acc: 0.9612 - val_loss: 0.7023 - val_acc: 0.7892\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9750\n",
      "Epoch 00008: val_loss did not improve from 0.58998\n",
      "91/91 [==============================] - 22s 247ms/step - loss: 0.0846 - acc: 0.9742 - val_loss: 0.7975 - val_acc: 0.7850\n",
      "Epoch 9/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9826\n",
      "Epoch 00009: val_loss did not improve from 0.58998\n",
      "91/91 [==============================] - 22s 246ms/step - loss: 0.0558 - acc: 0.9821 - val_loss: 0.8424 - val_acc: 0.7795\n",
      "Epoch 00009: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 13:20:45.630363\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 5\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9393 - acc: 0.6021\n",
      "Epoch 00001: val_loss improved from inf to 0.79731, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 5\n",
      "91/91 [==============================] - 44s 485ms/step - loss: 0.9382 - acc: 0.6024 - val_loss: 0.7973 - val_acc: 0.6616\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.6443 - acc: 0.7372\n",
      "Epoch 00002: val_loss improved from 0.79731 to 0.61869, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 5\n",
      "91/91 [==============================] - 23s 257ms/step - loss: 0.6442 - acc: 0.7369 - val_loss: 0.6187 - val_acc: 0.7476\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4395 - acc: 0.8281\n",
      "Epoch 00003: val_loss improved from 0.61869 to 0.58725, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 5\n",
      "91/91 [==============================] - 23s 256ms/step - loss: 0.4391 - acc: 0.8283 - val_loss: 0.5873 - val_acc: 0.7573\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3151 - acc: 0.8823\n",
      "Epoch 00004: val_loss did not improve from 0.58725\n",
      "91/91 [==============================] - 23s 254ms/step - loss: 0.3149 - acc: 0.8822 - val_loss: 0.5892 - val_acc: 0.7725\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9149\n",
      "Epoch 00005: val_loss did not improve from 0.58725\n",
      "91/91 [==============================] - 23s 255ms/step - loss: 0.2235 - acc: 0.9152 - val_loss: 0.6143 - val_acc: 0.7850\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1566 - acc: 0.9441\n",
      "Epoch 00006: val_loss did not improve from 0.58725\n",
      "91/91 [==============================] - 23s 253ms/step - loss: 0.1569 - acc: 0.9440 - val_loss: 0.7093 - val_acc: 0.7906\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9639\n",
      "Epoch 00007: val_loss did not improve from 0.58725\n",
      "91/91 [==============================] - 23s 252ms/step - loss: 0.1049 - acc: 0.9636 - val_loss: 0.7577 - val_acc: 0.7753\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9767\n",
      "Epoch 00008: val_loss did not improve from 0.58725\n",
      "91/91 [==============================] - 23s 253ms/step - loss: 0.0727 - acc: 0.9766 - val_loss: 0.8586 - val_acc: 0.7628\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 13:24:27.824388\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 6\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9433 - acc: 0.5743\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.77638, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 6\n",
      "91/91 [==============================] - 44s 481ms/step - loss: 0.9394 - acc: 0.5770 - val_loss: 0.7764 - val_acc: 0.6671\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.6359 - acc: 0.7340\n",
      "Epoch 00002: val_loss improved from 0.77638 to 0.69151, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 6\n",
      "91/91 [==============================] - 24s 260ms/step - loss: 0.6365 - acc: 0.7333 - val_loss: 0.6915 - val_acc: 0.7060\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4169 - acc: 0.8295\n",
      "Epoch 00003: val_loss improved from 0.69151 to 0.60616, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 6\n",
      "91/91 [==============================] - 24s 262ms/step - loss: 0.4192 - acc: 0.8290 - val_loss: 0.6062 - val_acc: 0.7601\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3079 - acc: 0.8823\n",
      "Epoch 00004: val_loss did not improve from 0.60616\n",
      "91/91 [==============================] - 23s 257ms/step - loss: 0.3070 - acc: 0.8825 - val_loss: 0.6155 - val_acc: 0.7628\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9240\n",
      "Epoch 00005: val_loss did not improve from 0.60616\n",
      "91/91 [==============================] - 24s 262ms/step - loss: 0.2186 - acc: 0.9231 - val_loss: 0.6812 - val_acc: 0.7822\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9483\n",
      "Epoch 00006: val_loss did not improve from 0.60616\n",
      "91/91 [==============================] - 23s 256ms/step - loss: 0.1458 - acc: 0.9481 - val_loss: 0.7097 - val_acc: 0.7822\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9639\n",
      "Epoch 00007: val_loss did not improve from 0.60616\n",
      "91/91 [==============================] - 24s 259ms/step - loss: 0.1070 - acc: 0.9643 - val_loss: 0.7606 - val_acc: 0.7850\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9788\n",
      "Epoch 00008: val_loss did not improve from 0.60616\n",
      "91/91 [==============================] - 24s 260ms/step - loss: 0.0655 - acc: 0.9787 - val_loss: 0.8434 - val_acc: 0.7850\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 13:28:13.004675\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 7\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9996 - acc: 0.5802\n",
      "Epoch 00001: val_loss improved from inf to 0.89286, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 7\n",
      "91/91 [==============================] - 45s 494ms/step - loss: 0.9993 - acc: 0.5791 - val_loss: 0.8929 - val_acc: 0.6130\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.7874 - acc: 0.6542\n",
      "Epoch 00002: val_loss improved from 0.89286 to 0.70107, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 7\n",
      "91/91 [==============================] - 24s 267ms/step - loss: 0.7838 - acc: 0.6567 - val_loss: 0.7011 - val_acc: 0.7060\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.5690 - acc: 0.7642\n",
      "Epoch 00003: val_loss improved from 0.70107 to 0.60612, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 7\n",
      "91/91 [==============================] - 24s 268ms/step - loss: 0.5679 - acc: 0.7652 - val_loss: 0.6061 - val_acc: 0.7462\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3824 - acc: 0.8538\n",
      "Epoch 00004: val_loss did not improve from 0.60612\n",
      "91/91 [==============================] - 24s 264ms/step - loss: 0.3827 - acc: 0.8530 - val_loss: 0.6235 - val_acc: 0.7531\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.8955\n",
      "Epoch 00005: val_loss did not improve from 0.60612\n",
      "91/91 [==============================] - 24s 264ms/step - loss: 0.2954 - acc: 0.8857 - val_loss: 0.6418 - val_acc: 0.7725\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/91 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9250\n",
      "Epoch 00006: val_loss did not improve from 0.60612\n",
      "91/91 [==============================] - 24s 264ms/step - loss: 0.2073 - acc: 0.9241 - val_loss: 0.6560 - val_acc: 0.7781\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9490\n",
      "Epoch 00007: val_loss did not improve from 0.60612\n",
      "91/91 [==============================] - 24s 266ms/step - loss: 0.1532 - acc: 0.9492 - val_loss: 0.7102 - val_acc: 0.7781\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9569\n",
      "Epoch 00008: val_loss did not improve from 0.60612\n",
      "91/91 [==============================] - 24s 262ms/step - loss: 0.1290 - acc: 0.9567 - val_loss: 0.7762 - val_acc: 0.7753\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 13:32:03.507064\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 8\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9558 - acc: 0.5986\n",
      "Epoch 00001: val_loss improved from inf to 0.77223, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 8\n",
      "91/91 [==============================] - 46s 501ms/step - loss: 0.9532 - acc: 0.5997 - val_loss: 0.7722 - val_acc: 0.6546\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.6605 - acc: 0.7285\n",
      "Epoch 00002: val_loss improved from 0.77223 to 0.65052, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 8\n",
      "91/91 [==============================] - 25s 269ms/step - loss: 0.6593 - acc: 0.7288 - val_loss: 0.6505 - val_acc: 0.7323\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4410 - acc: 0.8313\n",
      "Epoch 00003: val_loss improved from 0.65052 to 0.61074, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 8\n",
      "91/91 [==============================] - 25s 273ms/step - loss: 0.4420 - acc: 0.8310 - val_loss: 0.6107 - val_acc: 0.7698\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3211 - acc: 0.8826\n",
      "Epoch 00004: val_loss did not improve from 0.61074\n",
      "91/91 [==============================] - 24s 267ms/step - loss: 0.3214 - acc: 0.8825 - val_loss: 0.6597 - val_acc: 0.7545\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9208\n",
      "Epoch 00005: val_loss did not improve from 0.61074\n",
      "91/91 [==============================] - 24s 268ms/step - loss: 0.2232 - acc: 0.9203 - val_loss: 0.7142 - val_acc: 0.7601\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9483\n",
      "Epoch 00006: val_loss did not improve from 0.61074\n",
      "91/91 [==============================] - 24s 269ms/step - loss: 0.1552 - acc: 0.9481 - val_loss: 0.7397 - val_acc: 0.7573\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9694\n",
      "Epoch 00007: val_loss did not improve from 0.61074\n",
      "91/91 [==============================] - 24s 268ms/step - loss: 0.1037 - acc: 0.9691 - val_loss: 0.8044 - val_acc: 0.7684\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9760\n",
      "Epoch 00008: val_loss did not improve from 0.61074\n",
      "91/91 [==============================] - 25s 274ms/step - loss: 0.0735 - acc: 0.9756 - val_loss: 0.8560 - val_acc: 0.7725\n",
      "Epoch 00008: early stopping\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant\n",
      "2018-10-24 13:35:57.990914\n",
      "../Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 9\n",
      "Epoch 1/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9807 - acc: 0.5740\n",
      "Epoch 00001: val_loss improved from inf to 0.83724, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 9\n",
      "91/91 [==============================] - 47s 512ms/step - loss: 0.9780 - acc: 0.5753 - val_loss: 0.8372 - val_acc: 0.6200\n",
      "Epoch 2/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.7074 - acc: 0.6937\n",
      "Epoch 00002: val_loss improved from 0.83724 to 0.63117, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 9\n",
      "91/91 [==============================] - 25s 276ms/step - loss: 0.7044 - acc: 0.6948 - val_loss: 0.6312 - val_acc: 0.7517\n",
      "Epoch 3/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4706 - acc: 0.8132\n",
      "Epoch 00003: val_loss improved from 0.63117 to 0.62338, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 9\n",
      "91/91 [==============================] - 26s 282ms/step - loss: 0.4717 - acc: 0.8132 - val_loss: 0.6234 - val_acc: 0.7420\n",
      "Epoch 4/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.8722\n",
      "Epoch 00004: val_loss improved from 0.62338 to 0.60682, saving model to /home/andrew/Desktop/Bella/Data Augmentation/Models Probability 1/TWT_TAET_E Probability 1/normal restaurant 9\n",
      "91/91 [==============================] - 25s 275ms/step - loss: 0.3397 - acc: 0.8726 - val_loss: 0.6068 - val_acc: 0.7642\n",
      "Epoch 5/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9170\n",
      "Epoch 00005: val_loss did not improve from 0.60682\n",
      "91/91 [==============================] - 25s 274ms/step - loss: 0.2350 - acc: 0.9172 - val_loss: 0.7060 - val_acc: 0.7420\n",
      "Epoch 6/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9410\n",
      "Epoch 00006: val_loss did not improve from 0.60682\n",
      "91/91 [==============================] - 25s 278ms/step - loss: 0.1650 - acc: 0.9409 - val_loss: 0.7140 - val_acc: 0.7656\n",
      "Epoch 7/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9642\n",
      "Epoch 00007: val_loss did not improve from 0.60682\n",
      "91/91 [==============================] - 26s 282ms/step - loss: 0.1086 - acc: 0.9643 - val_loss: 0.7810 - val_acc: 0.7739\n",
      "Epoch 8/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9736\n",
      "Epoch 00008: val_loss did not improve from 0.60682\n",
      "91/91 [==============================] - 25s 271ms/step - loss: 0.0872 - acc: 0.9736 - val_loss: 0.7751 - val_acc: 0.7878\n",
      "Epoch 9/100\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9840\n",
      "Epoch 00009: val_loss did not improve from 0.60682\n",
      "91/91 [==============================] - 25s 277ms/step - loss: 0.0606 - acc: 0.9842 - val_loss: 0.8728 - val_acc: 0.7725\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "rest_only = {'restaurant': domain_data['restaurant']}\n",
    "#model_domain_predictions = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "probabilities = [0.2, 0.5, 0.7, 1]\n",
    "for probability in probabilities:\n",
    "    model_dir = Path('..', 'Data Augmentation', f'Models Probability {probability}')\n",
    "    predict_dir = Path('..', 'Data Augmentation', f'Predictions Probability {probability}')\n",
    "    \n",
    "    target_trans = TargetAugmentation(target_to_targets, probability=probability)\n",
    "    model_settings = [(f'TAET Probability {probability}', \n",
    "                       {'include_target_in_sequence': False,\n",
    "                        'transformers': [target_trans]}, \n",
    "                       'tclstm'),\n",
    "                      (f'TWT Probability {probability}', \n",
    "                       {'include_target_in_batches': False, \n",
    "                        'transformers': [target_trans]},\n",
    "                       'tdlstm'),\n",
    "                      (f'TWT_TAET Probability {probability}', \n",
    "                       {'transformers': [target_trans]}, \n",
    "                       'tclstm'),\n",
    "                      (f'TAET_E Probability {probability}', \n",
    "                       {'include_target_in_sequence': False, \n",
    "                        'transformers': [target_trans]},\n",
    "                       'tclstm'),\n",
    "                      (f'TWT_TAET_E Probability {probability}', \n",
    "                       {'transformers': [target_trans]}, \n",
    "                       'tclstm')]\n",
    "    \n",
    "    data_saved = return_save_data(model_settings, model_dir, predict_dir, 10,\n",
    "                                  model_domain_predictions, rest_only)\n",
    "    if data_saved:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    model_domain_predictions = multi_save_and_predict(model_settings, model_dir, \n",
    "                                                      predict_dir, rest_only, \n",
    "                                                      model_domain_predictions,\n",
    "                                                      num_times=10, \n",
    "                                                      dataloader_class=LeftRightAugmentSequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_preds(model_settings, model_dir, predict_dir, num_copies,\n",
    "                         model_domain_predictions, dataloader_class,\n",
    "                         domain_data, **model_kwargs):\n",
    "    for model_type_name, data_settings, model_name in model_settings:\n",
    "        model_type_dir = Path(model_dir, model_type_name)\n",
    "\n",
    "        for domain_name, data in domain_data.items():\n",
    "            model_type_path = Path(model_type_dir, f'normal {domain_name}')\n",
    "            \n",
    "            for copy_num in range(num_copies):\n",
    "                model_path_name = f'{model_type_path.name} {copy_num}'\n",
    "                model_path = model_type_path.with_name(model_path_name)\n",
    "                temp_model_path = model_path\n",
    "                \n",
    "                temp_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                if temp_model_path.suffix == '':\n",
    "                    temp_model_path = temp_model_path.with_suffix('.index')\n",
    "                print(temp_model_path)\n",
    "                if not temp_model_path.exists():\n",
    "                    model_func = getattr(keras_models, model_name)\n",
    "                    model = model_func(domain_data[domain_name][2].embedding, **model_kwargs)\n",
    "                    model.load_weights(str(model_path.resolve()))\n",
    "                    _, val, test = train_val_test_loader(dataloader_class, \n",
    "                                                          domain_data[domain_name],\n",
    "                                                          **data_settings)\n",
    "                    print(model.summary())\n",
    "                    predictions = model.predict_generator(val, use_multiprocessing=True, workers=4, verbose=1)\n",
    "                    print(predictions)\n",
    "                    predictions = np.argmax(predictions, axis=1)\n",
    "                    # Map the sentiment back to its original labels\n",
    "                    sent_conv = {2: 1, 1: 0, 0: -1}\n",
    "                    predictions = np.array([sent_conv[pred] for pred in predictions])\n",
    "                    #print(model_path_name)\n",
    "                    print(accuracy_score(rest_dev.sentiment_data(), predictions))\n",
    "                    with tf.Session() as sess:\n",
    "                        model_func = getattr(keras_models, model_name)\n",
    "                        model = model_func(domain_data[domain_name][2].embedding, **model_kwargs)\n",
    "                        from tensorflow.train import Optimizer, AdamOptimizer\n",
    "                        model.compile(optimizer=AdamOptimizer(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "                        print(model)\n",
    "                        print(model_path)\n",
    "                        b = model.load_weights(str(model_path.resolve())).assert_consumed()\n",
    "                        print(b)\n",
    "                        print(b.__dir__())\n",
    "                        print(b.initialize_or_restore())\n",
    "\n",
    "                        _, val, test = train_val_test_loader(dataloader_class, \n",
    "                                                          domain_data[domain_name],\n",
    "                                                          **data_settings)\n",
    "                        print(model.summary())\n",
    "                        print(val[0])\n",
    "                        print(rest_dev.sentiment_data()[:32])\n",
    "                        predictions = model.predict_generator(val, use_multiprocessing=True, workers=4, verbose=1)\n",
    "                        print(predictions)\n",
    "                        predictions = np.argmax(predictions, axis=1)\n",
    "                        # Map the sentiment back to its original labels\n",
    "                        sent_conv = {2: 1, 1: 0, 0: -1}\n",
    "                        predictions = np.array([sent_conv[pred] for pred in predictions])\n",
    "                        #print(model_path_name)\n",
    "                        print(accuracy_score(rest_dev.sentiment_data(), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 0.index\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "left_text (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_text (InputLayer)         (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "shared_embedding (Embedding)    (None, None, 300)    1355400     left_text[0][0]                  \n",
      "                                                                 right_text[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 300)          721200      shared_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 300)          721200      shared_embedding[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 3)            1803        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,799,603\n",
      "Trainable params: 2,799,603\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "23/23 [==============================] - 2s 91ms/step\n",
      "[[0.01209613 0.01937496 0.9685289 ]\n",
      " [0.29249516 0.5413819  0.16612294]\n",
      " [0.00616129 0.01887025 0.97496843]\n",
      " ...\n",
      " [0.02621311 0.24846189 0.72532505]\n",
      " [0.11712272 0.58507884 0.29779842]\n",
      " [0.02754235 0.04422798 0.92822963]]\n",
      "0.6296809986130375\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fe6ebf5f847d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m c=get_validation_preds(model_settings, model_dir, predict_dir, 1,\n\u001b[1;32m      9\u001b[0m                      \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLeftRightTargetSequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                      rest_only)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-d30b80148778>\u001b[0m in \u001b[0;36mget_validation_preds\u001b[0;34m(model_settings, model_dir, predict_dir, num_copies, model_domain_predictions, dataloader_class, domain_data, **model_kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0;31m#print(model_path_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrest_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                         \u001b[0mmodel_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdomain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "rest_only = {'restaurant': domain_data['restaurant']}\n",
    "model_settings = [(f'TWT Probability 1', \n",
    "                       {'include_target_in_batches': False},\n",
    "                       'tdlstm')]\n",
    "model_dir = Path('..', 'Data Augmentation', f'Models Probability 1')\n",
    "predict_dir = Path('..', 'Data Augmentation', f'Predictions Probability 1')\n",
    "m= {}\n",
    "c=get_validation_preds(model_settings, model_dir, predict_dir, 1,\n",
    "                     m, LeftRightTargetSequence,\n",
    "                     rest_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 0.index\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "left_text (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_text (InputLayer)         (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "shared_embedding (Embedding)    (None, None, 300)    1355400     left_text[0][0]                  \n",
      "                                                                 right_text[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_42 (LSTM)                  (None, 300)          721200      shared_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_43 (LSTM)                  (None, 300)          721200      shared_embedding[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 600)          0           lstm_42[0][0]                    \n",
      "                                                                 lstm_43[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 3)            1803        concatenate_21[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,799,603\n",
      "Trainable params: 2,799,603\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "23/23 [==============================] - 2s 107ms/step\n",
      "[[1.1847446e-01 1.4231370e-01 7.3921180e-01]\n",
      " [1.4781542e-01 4.4064695e-01 4.1153765e-01]\n",
      " [3.9318852e-02 1.5131995e-01 8.0936122e-01]\n",
      " ...\n",
      " [3.3138294e-04 4.4486724e-04 9.9922371e-01]\n",
      " [2.6067105e-01 4.4331938e-01 2.9600957e-01]\n",
      " [1.0892797e-02 1.2510909e-02 9.7659636e-01]]\n",
      "0.7461858529819695\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fe6ebf5f847d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m c=get_validation_preds(model_settings, model_dir, predict_dir, 1,\n\u001b[1;32m      9\u001b[0m                      \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLeftRightTargetSequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                      rest_only)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-f2f2e91c4068>\u001b[0m in \u001b[0;36mget_validation_preds\u001b[0;34m(model_settings, model_dir, predict_dir, num_copies, model_domain_predictions, dataloader_class, domain_data, **model_kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0;31m#print(model_path_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrest_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                         \u001b[0mmodel_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdomain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "rest_only = {'restaurant': domain_data['restaurant']}\n",
    "model_settings = [(f'TWT Probability 1', \n",
    "                       {'include_target_in_batches': False},\n",
    "                       'tdlstm')]\n",
    "model_dir = Path('..', 'Data Augmentation', f'Models Probability 1')\n",
    "predict_dir = Path('..', 'Data Augmentation', f'Predictions Probability 1')\n",
    "m= {}\n",
    "c=get_validation_preds(model_settings, model_dir, predict_dir, 1,\n",
    "                     m, LeftRightTargetSequence,\n",
    "                     rest_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 0.index\n",
      "<tensorflow.python.keras.engine.training.Model object at 0x7f23ed7bbb70>\n",
      "../Data Augmentation/Models Probability 1/TWT Probability 1/normal restaurant 0\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "left_text (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_text (InputLayer)         (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "shared_embedding (Embedding)    (None, None, 300)    1355400     left_text[0][0]                  \n",
      "                                                                 right_text[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_54 (LSTM)                  (None, 300)          721200      shared_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_55 (LSTM)                  (None, 300)          721200      shared_embedding[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 600)          0           lstm_54[0][0]                    \n",
      "                                                                 lstm_55[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 3)            1803        concatenate_27[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,799,603\n",
      "Trainable params: 2,799,603\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "([array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,   12,  503,    2,   10],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    2, 3331],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,   21,  183,    2,   10],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    2,  486,    5,  833,   19,   85,    5,   55,   65],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,   85,   60,   24,  187, 2243,   18,  366,   78,   96],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,   21,   31,   20,  277],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  736,\n",
      "          52,   78, 2539,   89,   16,   86,    3,    5,  969,    2,  103],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    2,  155],\n",
      "       [   0,    0,    2,  641, 1059,    7,  106,   20,    3,    5,   41,\n",
      "         427,  137, 1692,  114,   15,  915, 2503,   24,   79,   63,  913],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,   33,    7,    6,  436,   38],\n",
      "       [   0,    0,    0,    0,    0,    0,  127,   21,  122,   36,   30,\n",
      "           6,  284,    3,   21,  134,   91,  297,   15,    2,  145,  514],\n",
      "       [   0,   33,    7,    2,   58,  741,   38,   15,  653,  214,    3,\n",
      "          12,  781,   24,   47,    2,  254,    3,  173,  360, 1602,   10],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    2,  103,   60],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0, 3346,   34,   56,    3,  361,  524],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0, 4149,    7,    6,  223,   13,  572,  137, 3045, 2631],\n",
      "       [   0,    0,    0,    0,    0,    0,  173,   98,  712, 1255,   32,\n",
      "           2,  101,   21,  518,  227,   32,    6,   87,    5,   31,   71],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    5,  753,  108,   18, 1767, 1872,  117,    3,    2,   77],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0, 1577,    3,    2,   23, 1750],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,   70,  192,  759,\n",
      "         122,    3,  192,    9, 2093,    8,  142,    6, 3120,    5, 2094],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    2,  322,  323],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,  206,    3,   12,    9,   54,   13,    6,  426,   71],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    2,  322,  323],\n",
      "       [   0,    0,    2,  648,  622,   45,    7,  222,  337, 2584,    5,\n",
      "          18,   59,  100,  107,  302,   25,  154,   15,  592,    5,  131],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,   16,   37,    6, 2126,   24,    6,   67,    3, 2599,   38],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          41,  558,    5,   12,  177,    8, 1273,   13, 4150,   37,   71],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    2, 1973],\n",
      "       [ 308,  399,    3,    2,  304,    9,  586, 1907,    2,  164,  150,\n",
      "          21,  591,  633,   25, 1908,   15,    6,   66,   14,  704,  229],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,   41, 1983,    5,   12,\n",
      "         217,  826,    8,  140,   54,    6,  240,  127,   11, 1294, 1468],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,   18,  349, 2595,   29,    2,  732],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    2,  112,    9,  472,  131,    5,    9,  288],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,  123,    2, 4151,  473],\n",
      "       [   0,    0,    0,    0,    0,    0,    0,  239,  251,    3,   12,\n",
      "         591,    6,  309,  250,   11,  175, 1715,  712,    6,  462,   96]],\n",
      "      dtype=int32), array([[  10,    3,   17, ...,    0,    0,    0],\n",
      "       [3331,  430,   44, ...,    0,    0,    0],\n",
      "       [  10,    3,   92, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 288,   43, 2611, ...,    0,    0,    0],\n",
      "       [4151,  473,    3, ...,    0,    0,    0],\n",
      "       [  96,   15,    6, ...,    0,    0,    0]], dtype=int32)], array([[0., 0., 1.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 1., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.]], dtype=float32))\n",
      "[1, 0, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 0, 0, 1, 0, 1, 0, -1, 1, 1, 0, 1, 0, 1, -1, -1, 1, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 3s 121ms/step\n",
      "[[0.01011116 0.02553935 0.96434945]\n",
      " [0.14572896 0.10322724 0.7510438 ]\n",
      " [0.00273844 0.00965479 0.9876067 ]\n",
      " ...\n",
      " [0.02050343 0.04486562 0.93463093]\n",
      " [0.05018631 0.10435712 0.84545654]\n",
      " [0.00866583 0.0272201  0.96411407]]\n",
      "0.7614424410540915\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_settings = [(f'TWT Probability 1', \n",
    "                       {'include_target_in_batches': False},\n",
    "                       'tdlstm')]\n",
    "model_dir = Path('..', 'Data Augmentation', f'Models Probability 1')\n",
    "predict_dir = Path('..', 'Data Augmentation', f'Predictions Probability 1')\n",
    "m= {}\n",
    "c=get_validation_preds(model_settings, model_dir, predict_dir, 1,\n",
    "                     m, LeftRightTargetSequence,\n",
    "                     rest_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'restaurant': ([PosixPath('/home/andrew/.Bella/Datasets/restaurants train'),\n",
       "   PosixPath('/home/andrew/.Bella/Datasets/restaurants dev'),\n",
       "   PosixPath('/home/andrew/.Bella/Datasets/restaurants test')],\n",
       "  <keras_preprocessing.text.Tokenizer at 0x7f6102bad8d0>,\n",
       "  <bella.word_embeddings.GloveCommonEmbedding at 0x7f6102bad860>)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type_name, _, _ in model_settings:\n",
    "        model_type_dir = Path(model_dir, model_type_name)\n",
    "        predict_type_dir = Path(predict_dir, model_type_name)\n",
    "\n",
    "        for domain_name, _ in domain_data.items():\n",
    "            model_type_path = Path(model_type_dir, f'normal {domain_name}')\n",
    "            predict_type_path = Path(predict_type_dir, f'normal {domain_name}')\n",
    "            \n",
    "            for copy_num in range(num_copies):\n",
    "                predict_path_name = f'{predict_type_path.name} {copy_num}'\n",
    "                temp_predict_path = predict_type_path.with_name(predict_path_name)\n",
    "                model_path_name = f'{model_type_path.name} {copy_num}'\n",
    "                temp_model_path = model_type_path.with_name(model_path_name)\n",
    "                \n",
    "                temp_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                temp_predict_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    \n",
    "                if temp_predict_path.suffix == '':\n",
    "                    temp_predict_path = temp_predict_path.with_suffix('.npy')\n",
    "                if temp_model_path.suffix == '':\n",
    "                    temp_model_path = temp_model_path.with_suffix('.index')\n",
    "                if temp_model_path.exists() and temp_predict_path.exists():\n",
    "                    predictions = np.load(temp_predict_path)\n",
    "                    pred_dict[model_type_name][domain_name].append(predictions)\n",
    "                    continue\n",
    "                return False\n",
    "    return pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "restuarant_aug_predictions = domain_predictions(model_domain_predictions, 'restaurant')\n",
    "restuarant_aug_collections = dict_to_collection(restuarant_aug_predictions, rest_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f797cf5b7b8>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rest_results_prob = model_results_prob(restuarant_aug_collections, metrics)\n",
    "plot_probability(rest_results_prob, metric='Acc', \n",
    "                 bar_plot=False, box_plot=False, cat_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do next is to do the following:\n",
    "1. Be able to plot the validation accuracy\n",
    "2. Want to compare the best on the validation with each other\n",
    "3. Probably want to plot over more probabilities\n",
    "4. Want to compare the best probability with the original for the different sentiment categories\n",
    "5. Try a different technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe3b15400f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "restaurant_predictions = domain_predictions(model_domain_predictions, 'restaurant')\n",
    "restaurant_colls = dict_to_collection(restaurant_predictions, rest_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from bella.evaluation import datasets_df, plot_acc_f1\n",
    "metrics = [('Accuracy', accuracy_score)]\n",
    "res = datasets_df(restaurant_colls, metrics)\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.boxplot(x='Model', y='Score', data=res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Same One</th>\n",
       "      <th>Same Multi</th>\n",
       "      <th>Similar</th>\n",
       "      <th>Different</th>\n",
       "      <th>Unknown</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TAET Probability 0.2</th>\n",
       "      <td>96.44</td>\n",
       "      <td>81.65</td>\n",
       "      <td>78.99</td>\n",
       "      <td>56.59</td>\n",
       "      <td>74.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TWT Probability 0.2</th>\n",
       "      <td>96.00</td>\n",
       "      <td>81.08</td>\n",
       "      <td>78.99</td>\n",
       "      <td>57.05</td>\n",
       "      <td>71.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TWT_TAET Probability 0.2</th>\n",
       "      <td>94.22</td>\n",
       "      <td>79.61</td>\n",
       "      <td>77.54</td>\n",
       "      <td>51.82</td>\n",
       "      <td>71.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAET_E Probability 0.2</th>\n",
       "      <td>95.56</td>\n",
       "      <td>81.68</td>\n",
       "      <td>77.98</td>\n",
       "      <td>56.82</td>\n",
       "      <td>74.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TWT_TAET_E Probability 0.2</th>\n",
       "      <td>96.00</td>\n",
       "      <td>80.66</td>\n",
       "      <td>79.31</td>\n",
       "      <td>54.55</td>\n",
       "      <td>71.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAET Probability 0.5</th>\n",
       "      <td>94.00</td>\n",
       "      <td>81.35</td>\n",
       "      <td>79.28</td>\n",
       "      <td>58.86</td>\n",
       "      <td>74.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TWT Probability 0.5</th>\n",
       "      <td>94.00</td>\n",
       "      <td>80.93</td>\n",
       "      <td>78.93</td>\n",
       "      <td>61.14</td>\n",
       "      <td>72.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TWT_TAET Probability 0.5</th>\n",
       "      <td>92.44</td>\n",
       "      <td>80.93</td>\n",
       "      <td>78.61</td>\n",
       "      <td>56.82</td>\n",
       "      <td>71.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAET_E Probability 0.5</th>\n",
       "      <td>94.00</td>\n",
       "      <td>82.13</td>\n",
       "      <td>78.73</td>\n",
       "      <td>60.23</td>\n",
       "      <td>73.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TWT_TAET_E Probability 0.5</th>\n",
       "      <td>94.22</td>\n",
       "      <td>80.90</td>\n",
       "      <td>77.98</td>\n",
       "      <td>57.50</td>\n",
       "      <td>71.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAET Probability 0.7</th>\n",
       "      <td>94.44</td>\n",
       "      <td>82.43</td>\n",
       "      <td>78.44</td>\n",
       "      <td>59.32</td>\n",
       "      <td>73.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TWT Probability 0.7</th>\n",
       "      <td>93.33</td>\n",
       "      <td>81.35</td>\n",
       "      <td>78.27</td>\n",
       "      <td>60.45</td>\n",
       "      <td>73.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TWT_TAET Probability 0.7</th>\n",
       "      <td>91.33</td>\n",
       "      <td>80.27</td>\n",
       "      <td>77.69</td>\n",
       "      <td>57.50</td>\n",
       "      <td>71.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAET_E Probability 0.7</th>\n",
       "      <td>92.89</td>\n",
       "      <td>81.83</td>\n",
       "      <td>78.47</td>\n",
       "      <td>61.14</td>\n",
       "      <td>73.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TWT_TAET_E Probability 0.7</th>\n",
       "      <td>92.67</td>\n",
       "      <td>80.42</td>\n",
       "      <td>78.70</td>\n",
       "      <td>57.27</td>\n",
       "      <td>70.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Same One Same Multi Similar Different Unknown\n",
       "                                Acc        Acc     Acc       Acc     Acc\n",
       "TAET Probability 0.2          96.44      81.65   78.99     56.59   74.91\n",
       "TWT Probability 0.2           96.00      81.08   78.99     57.05   71.99\n",
       "TWT_TAET Probability 0.2      94.22      79.61   77.54     51.82   71.70\n",
       "TAET_E Probability 0.2        95.56      81.68   77.98     56.82   74.01\n",
       "TWT_TAET_E Probability 0.2    96.00      80.66   79.31     54.55   71.53\n",
       "TAET Probability 0.5          94.00      81.35   79.28     58.86   74.23\n",
       "TWT Probability 0.5           94.00      80.93   78.93     61.14   72.76\n",
       "TWT_TAET Probability 0.5      92.44      80.93   78.61     56.82   71.82\n",
       "TAET_E Probability 0.5        94.00      82.13   78.73     60.23   73.86\n",
       "TWT_TAET_E Probability 0.5    94.22      80.90   77.98     57.50   71.08\n",
       "TAET Probability 0.7          94.44      82.43   78.44     59.32   73.78\n",
       "TWT Probability 0.7           93.33      81.35   78.27     60.45   73.38\n",
       "TWT_TAET Probability 0.7      91.33      80.27   77.69     57.50   71.68\n",
       "TAET_E Probability 0.7        92.89      81.83   78.47     61.14   73.81\n",
       "TWT_TAET_E Probability 0.7    92.67      80.42   78.70     57.27   70.82"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant_predictions = domain_predictions(model_domain_predictions, 'restaurant')\n",
    "restaurant_colls = dict_to_collection(restaurant_predictions, rest_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from bella.evaluation import summary_errors\n",
    "from bella.error_analysis import same_one_sentiment, same_multi_sentiment,\\\n",
    "                                 similar_sentiment, different_sentiment, unknown_targets\n",
    "error_funcs = [('Same One', same_one_sentiment), ('Same Multi', same_multi_sentiment),\n",
    "               ('Similar', similar_sentiment), ('Different', different_sentiment),\n",
    "               ('Unknown', unknown_targets)]\n",
    "metrics = [('Acc', accuracy_score)]\n",
    "error_func_kwargs = {error_name: {'train_dataset': rest_train} for error_name, _ in error_funcs}\n",
    "summary_errors(restaurant_colls, metrics=metrics,\n",
    "               error_funcs=error_funcs, error_funcs_kwargs=error_func_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = rest_test.sentiment_data()\n",
    "a = model_domain_predictions['TAET Probability 0.2']['restaurant']\n",
    "b = np.array(a)\n",
    "from bella.data_types import TargetCollection\n",
    "another = TargetCollection(rest_test.data(), name='TWT 0.7')\n",
    "another.add_pred_sentiment(b.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6757439072784288"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "np.mean(another.dataset_metric_scores(f1_score, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6670682384379305"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(another.dataset_metric_scores(f1_score, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6681699955116985"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(another.dataset_metric_scores(f1_score, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6525055204414827"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(another.dataset_metric_scores(f1_score, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6600232964728401"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(another.dataset_metric_scores(f1_score, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6577000935474204"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(another.dataset_metric_scores(f1_score, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
