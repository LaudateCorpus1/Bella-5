{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bella\n",
    "from bella import syntactic_contexts\n",
    "from bella.dependency_parsers import tweebo, stanford\n",
    "import networkx as nx\n",
    "from networkx.algorithms import traversal\n",
    "\n",
    "def repro_traversaltree(conll, target):\n",
    "    '''\n",
    "    This is an adapted version of:\n",
    "    https://github.com/bluemonk482/tdparse/blob/master/src/utilities.py\n",
    "    traversaltree method\n",
    "    \n",
    "    This was the original method used to find dependency connected words \n",
    "    in the TDParse method.\n",
    "    \n",
    "    The parts that had to change was the traversal.bfs_successors \n",
    "    outputs a generator and therefore no longer has a values() method\n",
    "    thus had to gues to some degree what was happening here of which we \n",
    "    assume that the values are those in the second half of the tuple as \n",
    "    if you apply dict to the list of tuples those would be the values\n",
    "    '''\n",
    "    G = nx.Graph()\n",
    "    for position, token, tag, parser, rel in conll:\n",
    "        G.add_node(position)\n",
    "        for position1, token1, tag1, parser1, rel1 in conll:\n",
    "            if position1 == parser:\n",
    "               head = position1\n",
    "        if (parser == 0) or (parser == -1):\n",
    "           pass\n",
    "        else:\n",
    "           try:\n",
    "               G.add_edge(position, head, label=rel)\n",
    "           except:\n",
    "               print(token)\n",
    "               print(conll)\n",
    "    target_positions = []\n",
    "    for position, token, tag, parser, rel in conll:\n",
    "      if token == target:\n",
    "        target_positions.append(position)\n",
    "    positions = [[item for sublist in traversal.bfs_successors(G, target_position) for item in sublist] for target_position in target_positions]\n",
    "    words = []\n",
    "    for position in positions:\n",
    "        for i in position:\n",
    "            if isinstance(i, str):\n",
    "                continue\n",
    "            for d in i:\n",
    "                d = int(d)\n",
    "                words.append(conll[d-1][1])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency conntected words within the TDParse method\n",
    "\n",
    "In the TDParse method they use the words connected within the same syntactic path as the target. Further in the footnote of the paper they state that they do not take proximity into account. Thus they use the whole syntactic tree. The whole dependency parse tree in a normal text is still the whole text but when using the [Tweebo Parser](https://www.aclweb.org/anthology/D14-1108.pdf) which is the parser used in this paper it actually splits the text into multiple syntactic trees thus the syntactic path for which the target is in is a utterances of the original text.\n",
    "\n",
    "We show below that this is how the [original work by TDParse](https://www.aclweb.org/anthology/E17-1046.pdf) handled the text through using an adaptation of their code which is shown above in the `repro_traversaltree` method which is an adaptation of their [traversaltree](https://github.com/bluemonk482/tdparse/blob/master/src/utilities.py) method from their codebase.\n",
    "\n",
    "Creating CONLL formatted text for the sentence `This bread is tasty but so is sour dough I think`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tThis\t_\tD\tD\t_\t2\t_\t_\t_\n",
      "2\tbread\t_\tN\tN\t_\t3\t_\t_\t_\n",
      "3\tis\t_\tV\tV\t_\t5\tCONJ\t_\t_\n",
      "4\ttasty\t_\tA\tA\t_\t3\t_\t_\t_\n",
      "5\tbut\t_\t&\t&\t_\t0\t_\t_\t_\n",
      "6\tso\t_\tR\tR\t_\t7\t_\t_\t_\n",
      "7\tis\t_\tV\tV\t_\t5\tCONJ\t_\t_\n",
      "8\tsour\t_\tA\tA\t_\t9\t_\t_\t_\n",
      "9\tdough\t_\tN\tN\t_\t7\t_\t_\t_\n",
      "10\tI\t_\tO\tO\t_\t11\t_\t_\t_\n",
      "11\tthink\t_\tV\tV\t_\t0\t_\t_\t_\n"
     ]
    }
   ],
   "source": [
    "from bella.dependency_parsers import TweeboParser, stanford\n",
    "example_text = ['This bread is tasty but so is sour dough I think']\n",
    "tweebo_api = TweeboParser()\n",
    "tweebo_output = tweebo_api.parse_conll(example_text)[0]\n",
    "tweebo_conll = []\n",
    "for output in tweebo_output.split('\\n'):\n",
    "    print(output)\n",
    "    output = output.split('\\t')\n",
    "    tweebo_conll.append((output[0], output[1], output[3], output[6], output[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above CONLL formatted text we put it through the adapted TDParse method to find the connect words to the target `bread`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'but', 'tasty', 'is', 'so', 'dough', 'sour']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repro_traversaltree(tweebo_conll, 'bread')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown we get all the words that are in the same syntactic tree as the word `bread` except for the word `bread` as that is the target word.\n",
    "\n",
    "## Stanford Parser\n",
    "Now to demonstrate the point we use the Stanford CoreNLP dependency parser that does not split the text into multiple syntactic trees like most dependency parsers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tThis\t_\t_\tDT\t_\t2\tdet\t_\t_\n",
      "2\tbread\t_\t_\tNN\t_\t4\tnsubj\t_\t_\n",
      "3\tis\t_\t_\tVBZ\t_\t4\tcop\t_\t_\n",
      "4\ttasty\t_\t_\tJJ\t_\t0\troot\t_\t_\n",
      "5\tbut\t_\t_\tCC\t_\t4\tcc\t_\t_\n",
      "6\tso\t_\t_\tRB\t_\t10\tadvmod\t_\t_\n",
      "7\tis\t_\t_\tVBZ\t_\t10\tcop\t_\t_\n",
      "8\tsour\t_\t_\tJJ\t_\t10\tamod\t_\t_\n",
      "9\tdough\t_\t_\tNN\t_\t10\tcompound\t_\t_\n",
      "10\tI\t_\t_\tPRP\t_\t4\tconj\t_\t_\n",
      "11\tthink\t_\t_\tVBP\t_\t4\tdep\t_\t_\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This', 'tasty', 'is', 'but', 'I', 'think', 'so', 'is', 'sour', 'dough']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This has been generated from running stanford corenlp through it's java\n",
    "# command line interface and specifying a conll output\n",
    "# https://stanfordnlp.github.io/CoreNLP/cmdline.html\n",
    "stanford_output = '''1\tThis\t_\t_\tDT\t_\t2\tdet\t_\t_\n",
    "2\tbread\t_\t_\tNN\t_\t4\tnsubj\t_\t_\n",
    "3\tis\t_\t_\tVBZ\t_\t4\tcop\t_\t_\n",
    "4\ttasty\t_\t_\tJJ\t_\t0\troot\t_\t_\n",
    "5\tbut\t_\t_\tCC\t_\t4\tcc\t_\t_\n",
    "6\tso\t_\t_\tRB\t_\t10\tadvmod\t_\t_\n",
    "7\tis\t_\t_\tVBZ\t_\t10\tcop\t_\t_\n",
    "8\tsour\t_\t_\tJJ\t_\t10\tamod\t_\t_\n",
    "9\tdough\t_\t_\tNN\t_\t10\tcompound\t_\t_\n",
    "10\tI\t_\t_\tPRP\t_\t4\tconj\t_\t_\n",
    "11\tthink\t_\t_\tVBP\t_\t4\tdep\t_\t_'''\n",
    "stanford_conll = []\n",
    "for output in stanford_output.split('\\n'):\n",
    "    print(output)\n",
    "    output = output.split('\\t')\n",
    "    stanford_conll.append((output[0], output[1], output[4], output[6], output[7]))\n",
    "repro_traversaltree(stanford_conll, 'bread')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above all the words in the list are all the words from the text apart from the target word `bread` as expected. Thus using any other parser but the Tweebo parser with the TDParse methods will not make use of the dependency context window as the window will create a whole context view just removing the target, thus will have no syntactic knowledge added to the feature space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
