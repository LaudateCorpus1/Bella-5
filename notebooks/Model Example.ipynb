{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from tdparse.parsers import semeval_14\n",
    "from tdparse.helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use the pre-trained models\n",
    "\n",
    "A lot of the models trained in the model zoo took a long time to train espically the Neural Network models which include LSTM, TDLSTM, and TCLSTM. In this notebook we are going to show you how to:\n",
    "1. Load the models\n",
    "2. Use the models to predict on some of the test data\n",
    "\n",
    "First we need the path to the model zoo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Target Dependent Plus SemEval 14 Laptop',\n",
       " 'LSTM Dong Twitter weights.h5',\n",
       " 'LSTM SemEval 14 Restaurant architecture.yaml',\n",
       " 'Target Dependent Plus YouTuBean',\n",
       " 'TDLSTM Dong Twitter architecture.yaml',\n",
       " 'Target Dependent Plus SemEval 14 Restaurant',\n",
       " 'TDParse YouTuBean',\n",
       " 'Target Dependent Plus Mitchel',\n",
       " 'TDParse Plus Mitchel',\n",
       " 'LSTM SemEval 14 Restaurant weights.h5',\n",
       " 'TDLSTM Dong Twitter weights.h5',\n",
       " 'TDParse Election Twitter',\n",
       " 'TDLSTM SemEval 14 Laptop weights.h5',\n",
       " 'TDLSTM YouTuBean architecture.yaml',\n",
       " 'TDLSTM SemEval 14 Laptop architecture.yaml',\n",
       " 'Target Dependent SemEval 14 Laptop',\n",
       " 'LSTM Election Twitter weights.h5',\n",
       " 'TDLSTM Election Twitter weights.h5',\n",
       " 'TDParse SemEval 14 Restaurant',\n",
       " 'TDLSTM YouTuBean weights.h5',\n",
       " 'TCLSTM SemEval 14 Restaurant architecture.yaml',\n",
       " 'TCLSTM SemEval 14 Laptop weights.h5',\n",
       " 'Target Dependent Mitchel',\n",
       " 'LSTM Dong Twitter architecture.yaml',\n",
       " 'Target Dependent SemEval 14 Restaurant',\n",
       " 'TDParse Plus Dong Twitter',\n",
       " 'TDParse Plus SemEval 14 Laptop',\n",
       " 'Target Dependent Dong Twitter',\n",
       " 'LSTM Election Twitter architecture.yaml',\n",
       " 'TCLSTM Mitchel architecture.yaml',\n",
       " 'Target Dependent Plus Dong Twitter',\n",
       " 'TCLSTM YouTuBean weights.h5',\n",
       " 'Target Dependent Election Twitter',\n",
       " 'TDParse Plus SemEval 14 Restaurant',\n",
       " 'LSTM SemEval 14 Laptop architecture.yaml',\n",
       " 'TDParse SemEval 14 Laptop',\n",
       " 'TCLSTM SemEval 14 Laptop architecture.yaml',\n",
       " 'Target Dependent Plus Election Twitter',\n",
       " 'Target Dependent YouTuBean',\n",
       " 'TDParse Plus YouTuBean',\n",
       " 'TDLSTM SemEval 14 Restaurant weights.h5',\n",
       " 'TDLSTM Election Twitter architecture.yaml',\n",
       " 'TDLSTM SemEval 14 Restaurant architecture.yaml',\n",
       " 'TCLSTM Mitchel weights.h5',\n",
       " 'LSTM YouTuBean architecture.yaml',\n",
       " 'TDParse Dong Twitter',\n",
       " 'TDParse Mitchel',\n",
       " 'LSTM SemEval 14 Laptop weights.h5',\n",
       " 'TCLSTM YouTuBean architecture.yaml',\n",
       " 'LSTM YouTuBean weights.h5',\n",
       " 'TCLSTM SemEval 14 Restaurant weights.h5',\n",
       " 'TDLSTM Mitchel architecture.yaml',\n",
       " 'TDLSTM Mitchel weights.h5',\n",
       " 'TDParse Plus Election Twitter',\n",
       " 'LSTM Mitchel architecture.yaml',\n",
       " 'LSTM Mitchel weights.h5',\n",
       " 'TCLSTM Election Twitter architecture.yaml',\n",
       " 'TCLSTM Election Twitter weights.h5',\n",
       " 'TCLSTM Dong Twitter architecture.yaml',\n",
       " 'TCLSTM Dong Twitter weights.h5']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change this to the directory of your model zoo\n",
    "model_zoo_dir = os.path.abspath('../model zoo')\n",
    "os.listdir(model_zoo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model zoo you should be able to see the list of models as shown above. The models in there can be broken up into different methods which are the following: 1. Target Dependent, 2. Target Dependent Plus, 3. TDParse, 4. TDParse Plus, 5. LSTM, 6. TDLSTM, 7. TCLSTM. Each one of these methods was trained on the datasets which can be found in the [Dataset notebook](./datasets.ipynb). Therefore we have a model for each method on each dataset, if you would like to see how these models were trained look at the following Mass Evaluation notebooks: 1. [Target Dependent methods](./Mass Evaluation - Target Dependent.ipynb), 2. [TDParse methods](./Mass Evaluation - TDParse.ipynb), 3. [LSTM methods](./Mass Evaluation LSTM.ipynb)\n",
    "\n",
    "We are going to show how to load a model from each of the methods on the [SemEval 2014 Resturant dataset](http://alt.qcri.org/semeval2014/task4/index.php?id=data-and-tools).\n",
    "\n",
    "We are going to load one model at a time as each model can take:\n",
    "1. A long time to load\n",
    "2. Take up a lot of memory\n",
    "\n",
    "For each model we will:\n",
    "1. Load it\n",
    "2. Predict the sentiment of the test data on the SemEval 2014 Resturant dataset\n",
    "3. Evaluate the performance using Accuracy and Macro F1 score.\n",
    "\n",
    "But before going through each model we need to load the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = semeval_14(full_path(read_config('semeval_2014_rest_test')))\n",
    "y_test = test_data.sentiment_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the test data has been loaded all the cells grouped by models can be run independently.\n",
    "\n",
    "## Target Dependent and Target Dependent Plus Models\n",
    "\n",
    "These two methods come from [Vo and Zhang's paper](https://www.ijcai.org/Proceedings/15/Papers/194.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /mnt/silo/andrew/Sentiment_Models/tdparse/model zoo/Target Dependent SemEval 14 Restaurant\n",
      "Model successfully loaded. Load time 347.4\n",
      "Target Dependent scores:\n",
      "Macro F1 score: 59.87172828635391\n",
      "Accuracy: 74.82142857142857\n"
     ]
    }
   ],
   "source": [
    "from tdparse.models.target import TargetDep\n",
    "\n",
    "target_dep = TargetDep()\n",
    "model_path = [model_zoo_dir, \n",
    "              'Target Dependent SemEval 14 Restaurant']\n",
    "target_dep_model_path = os.path.join(*model_path)\n",
    "# Loads the model\n",
    "target_dep.load_model(target_dep_model_path, verbose=1)\n",
    "# Predicts the class on the test data\n",
    "target_dep_preds = target_dep.predict(test_data.data())\n",
    "# Evaluates the results\n",
    "target_dep_f1 = f1_score(y_test, target_dep_preds, average='macro')\n",
    "target_dep_acc = accuracy_score(y_test, target_dep_preds)\n",
    "print('Target Dependent scores:\\nMacro F1 score: {}\\n'\n",
    "      'Accuracy: {}'\n",
    "      .format(target_dep_f1 * 100, target_dep_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /mnt/silo/andrew/Sentiment_Models/tdparse/model zoo/Target Dependent Plus SemEval 14 Restaurant\n",
      "Model successfully loaded. Load time 381.32\n",
      "Target Dependent Plus scores:\n",
      "Macro F1 score: 62.604305872524044\n",
      "Accuracy: 76.33928571428571\n"
     ]
    }
   ],
   "source": [
    "from tdparse.models.target import TargetDepSent\n",
    "\n",
    "target_dep_plus = TargetDepSent()\n",
    "model_path = [model_zoo_dir, \n",
    "              'Target Dependent Plus SemEval 14 Restaurant']\n",
    "target_dep_plus_model_path = os.path.join(*model_path)\n",
    "# Loads the model\n",
    "target_dep_plus.load_model(target_dep_plus_model_path, verbose=1)\n",
    "# Predicts the class on the test data\n",
    "target_dep_plus_preds = target_dep_plus.predict(test_data.data())\n",
    "# Evaluates the results\n",
    "target_dep_plus_f1 = f1_score(y_test, target_dep_plus_preds, average='macro')\n",
    "target_dep_plus_acc = accuracy_score(y_test, target_dep_plus_preds)\n",
    "print('Target Dependent Plus scores:\\nMacro F1 score: {}\\n'\n",
    "      'Accuracy: {}'\n",
    "      .format(target_dep_plus_f1 * 100, target_dep_plus_acc * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDParse and TDParse Plus Models\n",
    "\n",
    "These two methods come from [Wang et al.'s paper](https://www.aclweb.org/anthology/E17-1046). Just a note that these models use the Tweebo dependency parser. If they are not working this could be due to Tweebo not being installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /mnt/silo/andrew/Sentiment_Models/tdparse/model zoo/TDParse SemEval 14 Restaurant\n",
      "Model successfully loaded. Load time 344.03\n",
      "TDParse scores:\n",
      "Macro F1 score: 61.34130989699303\n",
      "Accuracy: 75.89285714285714\n"
     ]
    }
   ],
   "source": [
    "from tdparse.models.tdparse import TDParse\n",
    "\n",
    "tdparse = TDParse()\n",
    "model_path = [model_zoo_dir, \n",
    "              'TDParse SemEval 14 Restaurant']\n",
    "tdparse_model_path = os.path.join(*model_path)\n",
    "# Loads the model\n",
    "tdparse.load_model(tdparse_model_path, verbose=1)\n",
    "# Predicts the class on the test data\n",
    "tdparse_preds = tdparse.predict(test_data.data())\n",
    "# Evaluates the results\n",
    "tdparse_f1 = f1_score(y_test, tdparse_preds, average='macro')\n",
    "tdparse_acc = accuracy_score(y_test, tdparse_preds)\n",
    "print('TDParse scores:\\nMacro F1 score: {}\\n'\n",
    "      'Accuracy: {}'\n",
    "      .format(tdparse_f1 * 100, tdparse_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /mnt/silo/andrew/Sentiment_Models/tdparse/model zoo/TDParse Plus SemEval 14 Restaurant\n",
      "Model successfully loaded. Load time 383.82\n",
      "TDParse Plus scores:\n",
      "Macro F1 score: 63.22342142275521\n",
      "Accuracy: 76.875\n"
     ]
    }
   ],
   "source": [
    "from tdparse.models.tdparse import TDParsePlus\n",
    "\n",
    "tdparse_plus = TDParsePlus()\n",
    "model_path = [model_zoo_dir, \n",
    "              'TDParse Plus SemEval 14 Restaurant']\n",
    "tdparse_plus_model_path = os.path.join(*model_path)\n",
    "# Loads the model\n",
    "tdparse_plus.load_model(tdparse_plus_model_path, verbose=1)\n",
    "# Predicts the class on the test data\n",
    "tdparse_plus_preds = tdparse_plus.predict(test_data.data())\n",
    "# Evaluates the results\n",
    "tdparse_plus_f1 = f1_score(y_test, tdparse_plus_preds, average='macro')\n",
    "tdparse_plus_acc = accuracy_score(y_test, tdparse_plus_preds)\n",
    "print('TDParse Plus scores:\\nMacro F1 score: {}\\n'\n",
    "      'Accuracy: {}'\n",
    "      .format(tdparse_plus_f1 * 100, tdparse_plus_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM, TDLSTM and TCLSTM Models\n",
    "\n",
    "These three methods come from [Tang et al.'s paper](https://aclweb.org/anthology/C16-1311). These models do require you knowing what embeddings they were trained on, this can only be done currently by looking at the relevent methods word vector results file e.g. for the LSTM method the word vector results file can be found [here](../results/TDLstm/lstm/word vector results.json). **NOTE** all the methods were trained using the Ark Tokeniser which is a Twitter specific tokeniser and all words lower cased\n",
    "\n",
    "**NOTE** Padding value when loading the word vectors means that when converting words into word vectors if there is a word that is not in the word embedding it gets that embedding value of <unk> which in the case of SSWE has been learnt and the value of the padding vectors will be equal to the Padding value. If this is not set then the value of the unknown vectors will be used for both padding and unknown words. **We are not in this case going to use the padding value as we did not use it while training the networks**\n",
    "    \n",
    "All the neural network models require two file paths: \n",
    "1. To the network architecture\n",
    "2. To the network's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture loaded /mnt/silo/andrew/Sentiment_Models/tdparse/model zoo/LSTM SemEval 14 Restaurant architecture.yaml\n",
      "Model weights saved to /mnt/silo/andrew/Sentiment_Models/tdparse/model zoo/LSTM SemEval 14 Restaurant weights.h5\n",
      "Load time 1.56\n",
      "LSTM scores:\n",
      "Macro F1 score: 48.35577315381478\n",
      "Accuracy: 73.03571428571428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/Envs/tdparse/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from tdparse.models.tdlstm import LSTM\n",
    "from tdparse.word_vectors import PreTrained\n",
    "from tdparse.tokenisers import ark_twokenize\n",
    "\n",
    "# Load the SSWE word vector\n",
    "sswe_path = full_path(read_config('sswe_files')['vo_zhang'])\n",
    "sswe = PreTrained(sswe_path, name='sswe')\n",
    "\n",
    "lstm = LSTM(tokeniser=ark_twokenize, embeddings=sswe, lower=True)\n",
    "model_arch_path = [model_zoo_dir, \n",
    "                   'LSTM SemEval 14 Restaurant architecture']\n",
    "model_weights_path = [model_zoo_dir, \n",
    "                      'LSTM SemEval 14 Restaurant weights']\n",
    "# Loads the model\n",
    "lstm.load_model(model_arch_fp=os.path.join(*model_arch_path), \n",
    "                model_weights_fp=os.path.join(*model_weights_path), \n",
    "                verbose=1)\n",
    "lstm.test_pad_size = 78\n",
    "# Predicts the class on the test data\n",
    "sentiment_mapper = {0 : 0, 1 : 1, 2 : -1}\n",
    "\n",
    "lstm_preds = lstm.predict(test_data.data_dict())\n",
    "lstm_preds =  lstm.prediction_to_cats(y_test, lstm_preds, \n",
    "                                      mapper=sentiment_mapper)\n",
    "# Evaluates the results\n",
    "lstm_f1 = f1_score(y_test, lstm_preds, average='macro')\n",
    "lstm_acc = accuracy_score(y_test, lstm_preds)\n",
    "print('LSTM scores:\\nMacro F1 score: {}\\n'\n",
    "      'Accuracy: {}'\n",
    "      .format(lstm_f1 * 100, lstm_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture loaded /mnt/silo/andrew/Sentiment_Models/tdparse/model zoo/TDLSTM SemEval 14 Restaurant architecture.yaml\n",
      "Model weights saved to /mnt/silo/andrew/Sentiment_Models/tdparse/model zoo/TDLSTM SemEval 14 Restaurant weights.h5\n",
      "Load time 139.73\n",
      "TDLSTM scores:\n",
      "Macro F1 score: 56.74201784497086\n",
      "Accuracy: 72.94642857142857\n"
     ]
    }
   ],
   "source": [
    "from tdparse.models.tdlstm import TDLSTM\n",
    "from tdparse.word_vectors import GloveCommonCrawl\n",
    "from tdparse.tokenisers import ark_twokenize\n",
    "\n",
    "# Load the Glove word vector\n",
    "glove_300 = GloveCommonCrawl(version=42)\n",
    "\n",
    "tdlstm = TDLSTM(tokeniser=ark_twokenize, embeddings=glove_300, lower=True)\n",
    "model_arch_path = [model_zoo_dir, \n",
    "                   'TDLSTM SemEval 14 Restaurant architecture']\n",
    "model_weights_path = [model_zoo_dir, \n",
    "                      'TDLSTM SemEval 14 Restaurant weights']\n",
    "# Loads the model\n",
    "tdlstm.load_model(model_arch_fp=os.path.join(*model_arch_path), \n",
    "                  model_weights_fp=os.path.join(*model_weights_path), \n",
    "                  verbose=1)\n",
    "# Predicts the class on the test data\n",
    "sentiment_mapper = {0 : 0, 1 : 1, 2 : -1}\n",
    "\n",
    "tdlstm_preds = tdlstm.predict(test_data.data_dict())\n",
    "tdlstm_preds =  tdlstm.prediction_to_cats(y_test, tdlstm_preds, \n",
    "                                          mapper=sentiment_mapper)\n",
    "# Evaluates the results\n",
    "tdlstm_f1 = f1_score(y_test, tdlstm_preds, average='macro')\n",
    "tdlstm_acc = accuracy_score(y_test, tdlstm_preds)\n",
    "print('TDLSTM scores:\\nMacro F1 score: {}\\n'\n",
    "      'Accuracy: {}'\n",
    "      .format(tdlstm_f1 * 100, tdlstm_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture loaded /mnt/silo/andrew/Sentiment_Models/tdparse/model zoo/TCLSTM SemEval 14 Restaurant architecture.yaml\n",
      "Model weights saved to /mnt/silo/andrew/Sentiment_Models/tdparse/model zoo/TCLSTM SemEval 14 Restaurant weights.h5\n",
      "Load time 58.77\n",
      "TCLSTM scores:\n",
      "Macro F1 score: 47.97914029915253\n",
      "Accuracy: 68.21428571428572\n"
     ]
    }
   ],
   "source": [
    "from tdparse.models.tdlstm import TCLSTM\n",
    "from tdparse.word_vectors import GloveCommonCrawl\n",
    "from tdparse.tokenisers import ark_twokenize\n",
    "\n",
    "# Load the Glove word vector\n",
    "glove_300 = GloveCommonCrawl(version=42)\n",
    "\n",
    "tclstm = TCLSTM(tokeniser=ark_twokenize, embeddings=glove_300, lower=True)\n",
    "model_arch_path = [model_zoo_dir, \n",
    "                   'TCLSTM SemEval 14 Restaurant architecture']\n",
    "model_weights_path = [model_zoo_dir, \n",
    "                      'TCLSTM SemEval 14 Restaurant weights']\n",
    "# Loads the model\n",
    "tclstm.load_model(model_arch_fp=os.path.join(*model_arch_path), \n",
    "                        model_weights_fp=os.path.join(*model_weights_path), \n",
    "                        verbose=1)\n",
    "# Predicts the class on the test data\n",
    "sentiment_mapper = {0 : 0, 1 : 1, 2 : -1}\n",
    "\n",
    "tclstm_preds = tclstm.predict(test_data.data_dict())\n",
    "tclstm_preds =  tclstm.prediction_to_cats(y_test, tclstm_preds, \n",
    "                                                mapper=sentiment_mapper)\n",
    "# Evaluates the results\n",
    "tclstm_f1 = f1_score(y_test, tclstm_preds, average='macro')\n",
    "tclstm_acc = accuracy_score(y_test, tclstm_preds)\n",
    "print('TCLSTM scores:\\nMacro F1 score: {}\\n'\n",
    "      'Accuracy: {}'\n",
    "      .format(tclstm_f1 * 100, tclstm_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
