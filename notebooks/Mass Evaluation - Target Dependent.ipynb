{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "**Please ensure that you have ran the *Mitchel and YouTuBean train test split* notebooks first so that all of the datasets are avaliable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Helper functions\n",
    "from bella.notebook_helper import write_json_data, get_json_data\n",
    "# Models\n",
    "from bella.models.target import TargetDep\n",
    "from bella.models.target import TargetDepSent\n",
    "# Word Vector methods\n",
    "from bella.word_vectors import GloveCommonCrawl, SSWE\n",
    "from bella.helper import read_config\n",
    "# Sentiment lexicons\n",
    "from bella import lexicons\n",
    "# Get the data\n",
    "from bella.parsers import semeval_14, dong, election\n",
    "from bella.data_types import TargetCollection\n",
    "# Evaluation methods\n",
    "from bella.evaluation import evaluation_results, get_results, get_raw_data\n",
    "# Tokenisers\n",
    "from bella.tokenisers import ark_twokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_values(c_values):\n",
    "    fine_values = []\n",
    "    for c_value in c_values:\n",
    "        if '35' in c_value or '7' in c_value:\n",
    "            fine_values.append(float(c_value))\n",
    "    fine_values = sorted(fine_values)\n",
    "    best_coarse_c_value = fine_values[3] / 7\n",
    "    fine_values.append(best_coarse_c_value)\n",
    "    return fine_values, best_coarse_c_value\n",
    "def coarse_tune_values(c_values):\n",
    "    coarse_values = []\n",
    "    fine_values = fine_tune_values(c_values)\n",
    "    for c_value in c_values:\n",
    "        c_value = float(c_value)\n",
    "        if c_value not in fine_values:\n",
    "            coarse_values.append(c_value)\n",
    "    return coarse_values\n",
    "def best_c_value(c_values):\n",
    "    best = 0\n",
    "    best_c = 0\n",
    "    for c_value, acc in c_values.items():\n",
    "        if acc > best:\n",
    "            best_c = c_value\n",
    "            best = acc\n",
    "    return best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#  ADD YOUR CONFIG FILE PATH HERE \n",
    "##\n",
    "CONFIG_FP = Path('..', 'config.yaml')\n",
    "\n",
    "# Load all of the datasets\n",
    "youtubean_train = semeval_14(read_config('youtubean_train', CONFIG_FP))\n",
    "youtubean_test = semeval_14(read_config('youtubean_test', CONFIG_FP))\n",
    "semeval_14_rest_train = semeval_14(read_config('semeval_2014_rest_train', CONFIG_FP))\n",
    "semeval_14_lap_train = semeval_14(read_config('semeval_2014_lap_train', CONFIG_FP))\n",
    "semeval_14_rest_test = semeval_14(read_config('semeval_2014_rest_test', CONFIG_FP))\n",
    "semeval_14_lap_test = semeval_14(read_config('semeval_2014_lap_test', CONFIG_FP))\n",
    "dong_train = dong(read_config('dong_twit_train_data', CONFIG_FP))\n",
    "dong_test = dong(read_config('dong_twit_test_data', CONFIG_FP))\n",
    "election_train, election_test = election(read_config('election_folder_dir', CONFIG_FP))\n",
    "mitchel_train = semeval_14(read_config('mitchel_train', CONFIG_FP))\n",
    "mitchel_test = semeval_14(read_config('mitchel_test', CONFIG_FP))\n",
    "\n",
    "\n",
    "dataset_train_test = {'SemEval 14 Laptop' : (semeval_14_lap_train, semeval_14_lap_test),\n",
    "                      'SemEval 14 Restaurant' : (semeval_14_rest_train, semeval_14_rest_test),\n",
    "                      'Dong Twitter' : (dong_train, dong_test),\n",
    "                      'Election Twitter' : (election_train, election_test),\n",
    "                      'YouTuBean' : (youtubean_train, youtubean_test),\n",
    "                      'Mitchel' : (mitchel_train, mitchel_test)\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentiment lexicons and remove all words that are not associated\n",
    "# to the Positive or Negative class.\n",
    "\n",
    "lexicon_data = read_config('lexicons', CONFIG_FP)\n",
    "hu_liu_fp = Path(lexicon_data['hu_liu'])\n",
    "mpqa_fp = Path(lexicon_data['mpqa'])\n",
    "nrc_fp = Path(lexicon_data['nrc_emotion'])\n",
    "\n",
    "subset_cats = {'positive', 'negative'}\n",
    "\n",
    "# Load the sentiment lexicons but lower case all the words\n",
    "mpqa_low = lexicons.Mpqa(mpqa_fp, subset_cats=subset_cats, lower=True)\n",
    "nrc_low = lexicons.NRC(nrc_fp, subset_cats=subset_cats, lower=True)\n",
    "hu_liu_low = lexicons.HuLiu(hu_liu_fp, subset_cats=subset_cats, lower=True)\n",
    "mpqa_huliu_low = lexicons.Lexicon.combine_lexicons(mpqa_low, hu_liu_low)\n",
    "all_three_low = lexicons.Lexicon.combine_lexicons(mpqa_huliu_low, nrc_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_predictions(train, test, dataset_name, model,\n",
    "                        random_state,c_file_path, word_vector_file_path, model_dir,\n",
    "                        sentiment_lexicon=None, result_file_path=None,\n",
    "                        re_write=True, save_raw_data=True):\n",
    "    # Gets the results dataframe if it already exists\n",
    "    if not re_write and result_file_path is not None:\n",
    "        results_df = get_results(result_file_path, name)\n",
    "        if save_raw_data and results_df is not None:\n",
    "            if get_raw_data(result_file_path, name, test):\n",
    "                return results_df\n",
    "        elif results_df is not None:\n",
    "            return results_df\n",
    "    # loading the data\n",
    "    data_train = train.data()\n",
    "    y_train = train.sentiment_data()\n",
    "    data_test = test.data()\n",
    "    y_test = test.sentiment_data()\n",
    "    # Reduce the word vector size by filtering by the word in the train\n",
    "    # and test dataset. Which is still as fair in practice as loading the whole\n",
    "    # word vectors\n",
    "\n",
    "    train_words = train.word_list(ark_twokenize)\n",
    "    test_words = test.word_list(ark_twokenize)\n",
    "    all_words = list(set(train_words + test_words))\n",
    "    glove_300 = GloveCommonCrawl(version=42, filter_words=all_words)\n",
    "    sswe = SSWE(filter_words=all_words)\n",
    "    \n",
    "    # CV grid params\n",
    "    c_grid_params = {'word_vectors' : [[sswe]], 'random_state' : random_state,\n",
    "                     'tokenisers' : [ark_twokenize]}\n",
    "    if sentiment_lexicon is not None:\n",
    "        c_grid_params['senti_lexicons'] = [sentiment_lexicon]\n",
    "        \n",
    "    best_c, c_scores = model.find_best_c(data_train, y_train, c_grid_params, \n",
    "                                         save_file=c_file_path, dataset_name=dataset_name, \n",
    "                                         re_write=False, n_jobs=7, cv=5)\n",
    "    \n",
    "    # Search over the different word vectors \n",
    "    word_vectors = [[sswe], [glove_300]]\n",
    "    word_vector_grid_params = {**c_grid_params}\n",
    "    word_vector_grid_params['C'] = [best_c]\n",
    "    word_vector_grid_params['word_vectors'] = word_vectors\n",
    "    import time\n",
    "    t = time.time()\n",
    "    best_word_vector = model.save_grid_search(data_train, y_train, word_vector_grid_params, \n",
    "                                              'word_vectors', dataset_name, word_vector_file_path, \n",
    "                                              re_write=False, n_jobs=5, cv=5)\n",
    "    print('{} {}'.format(best_word_vector, time.time() - t))\n",
    "    \n",
    "    parameters = {'word_vector' : best_word_vector, 'random_state' : random_state, \n",
    "                  'C' : best_c, 'tokeniser' : ark_twokenize}\n",
    "    if sentiment_lexicon is not None:\n",
    "        parameters['senti_lexicon'] = sentiment_lexicon\n",
    "    best_params = model.get_params(**parameters)\n",
    "    print('Best parameters for dataset {} are: {}'.format(dataset_name, parameters))\n",
    "    model.fit(data_train, y_train, params=best_params)\n",
    "    predicted_values = model.predict(data_test)\n",
    "    # Save the model to the model zoo\n",
    "    model_file_name = '{} {}'.format(model, dataset_name)\n",
    "    model_file_path = os.path.join(model_dir, model_file_name)\n",
    "    model.save_model(model_file_path, verbose=1)\n",
    "    # Return the results\n",
    "    if result_file_path is not None:\n",
    "        return evaluation_results(predicted_values, test, dataset_name, \n",
    "                                  file_name=result_file_path, \n",
    "                                  save_raw_data=save_raw_data, re_write=True)\n",
    "    else:\n",
    "        return evaluation_results(predicted_values, test, dataset_name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances of the models\n",
    "target_dep = TargetDep()\n",
    "target_dep_plus = TargetDepSent()\n",
    "models = [target_dep, target_dep_plus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target dependent model mass evaluation\n",
    "\n",
    "The above code loads all of the data, models, and lexicons we are going to use in this notebook\n",
    "\n",
    "We are going to use three different models.\n",
    "1. target_dep -- Target Dependent model that uses no sentiment lexicons\n",
    "2. target_dep_plus -- Target Dependent model that uses only the Hu & Liu lexicon\n",
    "3. target_dep_plus_all -- Target Dependent model that uses all three lexicons from the original paper\n",
    "\n",
    "Each model gets it's own results file where it will store the results from each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Target Dependent: ({'random_state': 42},\n",
       "  '/mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/results_alt/Target Dependent/Target Dependent.tsv',\n",
       "  '/mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/results_alt/Target Dependent/Target Dependent C.json',\n",
       "  '/mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/results_alt/Target Dependent/Target Dependent word vector.json',\n",
       "  '/mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/model zoo alt'),\n",
       " Target Dependent Plus: ({'random_state': 42,\n",
       "   'sentiment_lexicon': <bella.lexicons.Lexicon at 0x7f84ad0a3358>},\n",
       "  '/mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/results_alt/Target Dependent/Target Dependent+.tsv',\n",
       "  '/mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/results_alt/Target Dependent/Target Dependent+ C.json',\n",
       "  '/mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/results_alt/Target Dependent/Target Dependent+ word vector.json',\n",
       "  '/mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/model zoo alt')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the result files\n",
    "result_folder = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results_alt', 'Target Dependent'))\n",
    "model_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'model zoo alt'))\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_result_files = ['Target Dependent.tsv', 'Target Dependent+.tsv']\n",
    "model_result_files = [os.path.join(result_folder, result_file) for result_file in model_result_files]\n",
    "C_result_files = ['Target Dependent C.json', 'Target Dependent+ C.json']\n",
    "C_result_files = [os.path.join(result_folder, result_file) for result_file in C_result_files]\n",
    "word_vector_result_files = ['Target Dependent word vector.json', 'Target Dependent+ word vector.json']\n",
    "word_vector_result_files = [os.path.join(result_folder, result_file) for result_file in word_vector_result_files]\n",
    "# Parameters for each model\n",
    "std_model_parameters = {'random_state' : 42}\n",
    "all_senti_model_parameters = {**std_model_parameters, 'sentiment_lexicon' : all_three_low}\n",
    "model_parameters = [std_model_parameters, all_senti_model_parameters]\n",
    "# Combining parameters and result files\n",
    "parameters_files = list(zip(model_parameters, model_result_files, C_result_files, \n",
    "                            word_vector_result_files, [model_dir]*2))\n",
    "\n",
    "model_files = dict(zip(models, parameters_files))\n",
    "model_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset SemEval 14 Laptop\n",
      "Processing model Target Dependent\n",
      "Loading glove 300d 42b common crawl from file\n",
      "[glove 300d 42b common crawl] 99.16763806343079\n",
      "Best parameters for dataset SemEval 14 Laptop are: {'word_vector': [glove 300d 42b common crawl], 'random_state': 42, 'C': 0.01, 'tokeniser': <function ark_twokenize at 0x7f84b9358840>}\n",
      "Model saved to /mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/model zoo alt/Target Dependent SemEval 14 Laptop. Save time 0.64\n",
      "saving raw data\n",
      "Processing model Target Dependent Plus\n",
      "Loading glove 300d 42b common crawl from file\n",
      "[glove 300d 42b common crawl] 117.39861941337585\n",
      "Best parameters for dataset SemEval 14 Laptop are: {'word_vector': [glove 300d 42b common crawl], 'random_state': 42, 'C': 0.0035, 'tokeniser': <function ark_twokenize at 0x7f84b9358840>, 'senti_lexicon': <bella.lexicons.Lexicon object at 0x7f84ad0a3358>}\n",
      "Model saved to /mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/model zoo alt/Target Dependent Plus SemEval 14 Laptop. Save time 0.96\n",
      "saving raw data\n",
      "Processing dataset SemEval 14 Restaurant\n",
      "Processing model Target Dependent\n",
      "Loading glove 300d 42b common crawl from file\n",
      "[sswe] 267.82217478752136\n",
      "Best parameters for dataset SemEval 14 Restaurant are: {'word_vector': [sswe], 'random_state': 42, 'C': 0.035, 'tokeniser': <function ark_twokenize at 0x7f84b9358840>}\n",
      "Model saved to /mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/model zoo alt/Target Dependent SemEval 14 Restaurant. Save time 0.67\n",
      "saving raw data\n",
      "Processing model Target Dependent Plus\n",
      "Loading glove 300d 42b common crawl from file\n",
      "[sswe] 245.91290092468262\n",
      "Best parameters for dataset SemEval 14 Restaurant are: {'word_vector': [sswe], 'random_state': 42, 'C': 0.01, 'tokeniser': <function ark_twokenize at 0x7f84b9358840>, 'senti_lexicon': <bella.lexicons.Lexicon object at 0x7f84ad0a3358>}\n",
      "Model saved to /mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/model zoo alt/Target Dependent Plus SemEval 14 Restaurant. Save time 0.99\n",
      "saving raw data\n",
      "Processing dataset Dong Twitter\n",
      "Processing model Target Dependent\n",
      "Loading glove 300d 42b common crawl from file\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time_to_process = time.time()\n",
    "for dataset_name, train_test in dataset_train_test.items():\n",
    "    print('Processing dataset {}'.format(dataset_name))\n",
    "    train, test = train_test\n",
    "    for model, parameter_file_paths in model_files.items():\n",
    "        print('Processing model {}'.format(model))\n",
    "        params_files = parameter_file_paths\n",
    "        parameters = params_files[0]\n",
    "        result_file_path = params_files[1]\n",
    "        c_fp = params_files[2]\n",
    "        word_vectors_fp = params_files[3]\n",
    "        model_dir = params_files[4]\n",
    "        dataset_predictions(train, test, dataset_name, model, \n",
    "                            result_file_path=result_file_path,\n",
    "                            re_write=True, save_raw_data=True,\n",
    "                            c_file_path=c_fp,\n",
    "                            word_vector_file_path=word_vectors_fp, \n",
    "                            model_dir=model_dir,\n",
    "                            **parameters)\n",
    "\n",
    "time_to_process = time.time() - time_to_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time it took to process all the datasets {{round(time_to_process / 3600, 2)}} hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The affect of the C value\n",
    "\n",
    "We take a look at the affect of tunning for the C-Value. We tune the C-Value in two steps:\n",
    "1. Coarse grain search over the following values: from 0.00001 to 10 going up by a factor of 10. Once the best best coarse grain value is found we fine tune it.\n",
    "2. We multiple the best coarse grain value by the following value and search over these values: 0.35, 0.7, 1, 3.5, and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values_target_dep_plus = C_result_files[1]\n",
    "c_values_target_dep_plus = get_json_data(c_values_target_dep_plus, 'Dong Twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stats = defaultdict(lambda: dict())\n",
    "\n",
    "mean_error = lambda acc_values: sum(max(acc_values) - acc_values) / (len(acc_values) - 1)\n",
    "for dataset_name in dataset_train_test:\n",
    "    # Target Dependent Plus using the Hu and Liu Lexicon C file\n",
    "    c_values_target_dep_plus = C_result_files[1]\n",
    "    c_values_target_dep_plus = get_json_data(c_values_target_dep_plus, dataset_name)\n",
    "    model_results_file = model_result_files[1]\n",
    "    fine_c_values, best_coarse_c_value = fine_tune_values(c_values_target_dep_plus)\n",
    "    coarse_c_values = coarse_tune_values(c_values_target_dep_plus)\n",
    "    all_c_values = {float(c_value) : acc for c_value, acc in c_values_target_dep_plus.items()}\n",
    "    fine_acc = np.array([all_c_values[c_value] for c_value in fine_c_values])\n",
    "    coarse_acc = np.array([all_c_values[c_value] for c_value in coarse_c_values])\n",
    "    all_acc = np.array(list(c_values_target_dep_plus.values()))\n",
    "    best_c = best_c_value(all_c_values)\n",
    "    test_accuracy = get_results(model_results_file, dataset_name)['Accuracy']\n",
    "    dataset_stats[dataset_name]['Fine Tune MAE (Accuracy %)'] = mean_error(fine_acc) * 100\n",
    "    dataset_stats[dataset_name]['Coarse Tune MAE (Accuracy %)'] = mean_error(coarse_acc) * 100\n",
    "    dataset_stats[dataset_name]['Best Fine C value'] = best_c\n",
    "    dataset_stats[dataset_name]['Best Coarse C value'] = best_coarse_c_value\n",
    "    dataset_stats[dataset_name]['Test score Accuracy (%)'] = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = list(dataset_train_test)\n",
    "columns = ['Fine Tune MAE (Accuracy %)', 'Coarse Tune MAE (Accuracy %)', 'Best Fine C value', \n",
    "           'Best Coarse C value', 'Dataset', 'Test score Accuracy (%)']\n",
    "c_value_stats = pd.DataFrame(np.zeros((len(dataset_train_test), 6)), columns=columns)\n",
    "c_value_stats['Dataset'] = index\n",
    "c_value_stats = c_value_stats.set_index('Dataset')\n",
    "\n",
    "# Add the data to the DataFrame\n",
    "for dataset_name, col_value in dataset_stats.items():\n",
    "    for column, value in col_value.items():\n",
    "        c_value_stats[column][dataset_name] = value\n",
    "c_value_stats = c_value_stats.round({'Fine Tune Accuracy std (%)' : 2, \n",
    "                                     'Coarse Tune Accuracy std (%)' : 2})\n",
    "c_value_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
