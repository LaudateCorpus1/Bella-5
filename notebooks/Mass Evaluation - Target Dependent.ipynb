{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "**Please ensure that you have ran the *Mitchel and YouTuBean train test split* notebooks first so that all of the datasets are avaliable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Helper functions\n",
    "from tdparse.notebook_helper import write_json_data, get_json_data\n",
    "# Models\n",
    "from tdparse.models.target import TargetDep\n",
    "from tdparse.models.target import TargetDepSent\n",
    "# Word Vector methods\n",
    "from tdparse.word_vectors import GloveCommonCrawl, PreTrained\n",
    "from tdparse.helper import read_config, full_path\n",
    "# Sentiment lexicons\n",
    "from tdparse import lexicons\n",
    "# Get the data\n",
    "from tdparse.parsers import semeval_14, dong, election\n",
    "from tdparse.data_types import TargetCollection\n",
    "# Evaluation methods\n",
    "from tdparse.evaluation import evaluation_results, scores, get_results, \\\n",
    "                               save_results, combine_results, get_raw_data\n",
    "# Tokenisers\n",
    "from tdparse.tokenisers import ark_twokenize, whitespace, stanford\n",
    "from tdparse.stanford_tools import constituency_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_values(c_values):\n",
    "    fine_values = []\n",
    "    for c_value in c_values:\n",
    "        if '35' in c_value or '7' in c_value:\n",
    "            fine_values.append(float(c_value))\n",
    "    fine_values = sorted(fine_values)\n",
    "    best_coarse_c_value = fine_values[3] / 7\n",
    "    fine_values.append(best_coarse_c_value)\n",
    "    return fine_values, best_coarse_c_value\n",
    "def coarse_tune_values(c_values):\n",
    "    coarse_values = []\n",
    "    fine_values = fine_tune_values(c_values)\n",
    "    for c_value in c_values:\n",
    "        c_value = float(c_value)\n",
    "        if c_value not in fine_values:\n",
    "            coarse_values.append(c_value)\n",
    "    return coarse_values\n",
    "def best_c_value(c_values):\n",
    "    best = 0\n",
    "    best_c = 0\n",
    "    for c_value, acc in c_values.items():\n",
    "        if acc > best:\n",
    "            best_c = c_value\n",
    "            best = acc\n",
    "    return best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of the datasets\n",
    "youtubean_train = semeval_14(full_path(read_config('youtubean_train')))\n",
    "youtubean_test = semeval_14(full_path(read_config('youtubean_test')))\n",
    "semeval_14_rest_train = semeval_14(full_path(read_config('semeval_2014_rest_train')))\n",
    "semeval_14_lap_train = semeval_14(full_path(read_config('semeval_2014_lap_train')))\n",
    "semeval_14_rest_test = semeval_14(full_path(read_config('semeval_2014_rest_test')))\n",
    "semeval_14_lap_test = semeval_14(full_path(read_config('semeval_2014_lap_test')))\n",
    "dong_train = dong(full_path(read_config('dong_twit_train_data')))\n",
    "dong_test = dong(full_path(read_config('dong_twit_test_data')))\n",
    "election_train, election_test = election(full_path(read_config('election_folder_dir')))\n",
    "mitchel_train = semeval_14(full_path(read_config('mitchel_train')))\n",
    "mitchel_test = semeval_14(full_path(read_config('mitchel_test')))\n",
    "\n",
    "\n",
    "dataset_train_test = {'SemEval 14 Laptop' : (semeval_14_lap_train, semeval_14_lap_test),\n",
    "                      'SemEval 14 Restaurant' : (semeval_14_rest_train, semeval_14_rest_test),\n",
    "                      'Dong Twitter' : (dong_train, dong_test),\n",
    "                      'Election Twitter' : (election_train, election_test),\n",
    "                      'YouTuBean' : (youtubean_train, youtubean_test),\n",
    "                      'Mitchel' : (mitchel_train, mitchel_test)\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word vectors\n",
    "sswe_path = full_path(read_config('sswe_files')['vo_zhang'])\n",
    "sswe = PreTrained(sswe_path, name='sswe')\n",
    "\n",
    "glove_300 = GloveCommonCrawl(version=42)\n",
    "\n",
    "\n",
    "# Load the sentiment lexicons and remove all words that are not associated\n",
    "# to the Positive or Negative class.\n",
    "subset_cats = {'positive', 'negative'}\n",
    "mpqa_low = lexicons.Mpqa(subset_cats=subset_cats, lower=True)\n",
    "nrc_low = lexicons.NRC(subset_cats=subset_cats, lower=True)\n",
    "hu_liu_low = lexicons.HuLiu(subset_cats=subset_cats, lower=True)\n",
    "mpqa_huliu_low = lexicons.Lexicon.combine_lexicons(mpqa_low, hu_liu_low)\n",
    "all_three_low = lexicons.Lexicon.combine_lexicons(mpqa_huliu_low, nrc_low)\n",
    "all_lexicons = [mpqa_low, nrc_low, hu_liu_low, mpqa_huliu_low, all_three_low]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_predictions(train, test, dataset_name, model,\n",
    "                        word_vector, random_state,\n",
    "                        c_file_path, word_vector_file_path, model_dir,\n",
    "                        sentiment_lexicon=None, result_file_path=None,\n",
    "                        re_write=True, save_raw_data=True):\n",
    "    # Gets the results dataframe if it already exists\n",
    "    if not re_write and result_file_path is not None:\n",
    "        results_df = get_results(result_file_path, name)\n",
    "        if save_raw_data and results_df is not None:\n",
    "            if get_raw_data(result_file_path, name, test):\n",
    "                return results_df\n",
    "        elif results_df is not None:\n",
    "            return results_df\n",
    "    # loading the data\n",
    "    data_train = train.data()\n",
    "    y_train = train.sentiment_data()\n",
    "    data_test = test.data()\n",
    "    y_test = test.sentiment_data()\n",
    "    \n",
    "    # CV grid params\n",
    "    c_grid_params = {'word_vectors' : [word_vector], 'random_state' : random_state,\n",
    "                     'tokenisers' : [ark_twokenize]}\n",
    "    if sentiment_lexicon is not None:\n",
    "        c_grid_params['senti_lexicons'] = [sentiment_lexicon]\n",
    "        \n",
    "    best_c, c_scores = model.find_best_c(data_train, y_train, c_grid_params, \n",
    "                                         save_file=c_file_path, dataset_name=dataset_name, \n",
    "                                         re_write=False, n_jobs=7, cv=5)\n",
    "    \n",
    "    # Search over the different word vectors \n",
    "    word_vectors = [[sswe]]\n",
    "    word_vector_grid_params = {**c_grid_params}\n",
    "    word_vector_grid_params['C'] = [best_c]\n",
    "    word_vector_grid_params['word_vectors'] = word_vectors\n",
    "    import time\n",
    "    t = time.time()\n",
    "    best_word_vector = model.save_grid_search(data_train, y_train, word_vector_grid_params, \n",
    "                                              'word_vectors', dataset_name, word_vector_file_path, \n",
    "                                              re_write=False, n_jobs=5, cv=5)\n",
    "    print('{} {}'.format(best_word_vector, time.time() - t))\n",
    "    t = time.time()\n",
    "    # Word Vector is too large to multi-process\n",
    "    word_vectors.extend([[glove_300]])\n",
    "    best_word_vector = model.save_grid_search(data_train, y_train, word_vector_grid_params, \n",
    "                                              'word_vectors', dataset_name, word_vector_file_path, \n",
    "                                              re_write=False, n_jobs=1, cv=5)\n",
    "    print('{} {}'.format(best_word_vector, time.time() - t))\n",
    "    \n",
    "    parameters = {'word_vector' : best_word_vector, 'random_state' : random_state, \n",
    "                  'C' : best_c, 'tokeniser' : ark_twokenize}\n",
    "    if sentiment_lexicon is not None:\n",
    "        parameters['senti_lexicon'] = sentiment_lexicon\n",
    "    best_params = model.get_params(**parameters)\n",
    "    print('Best parameters for dataset {} are: {}'.format(dataset_name, parameters))\n",
    "    model.fit(data_train, y_train, params=best_params)\n",
    "    predicted_values = model.predict(data_test)\n",
    "    # Save the model to the model zoo\n",
    "    model_file_name = '{} {}'.format(model, dataset_name)\n",
    "    model_file_path = os.path.join(model_dir, model_file_name)\n",
    "    model.save_model(model_file_path, verbose=1)\n",
    "    # Return the results\n",
    "    if result_file_path is not None:\n",
    "        return evaluation_results(predicted_values, test, dataset_name, \n",
    "                                  file_name=result_file_path, \n",
    "                                  save_raw_data=save_raw_data, re_write=True)\n",
    "    else:\n",
    "        return evaluation_results(predicted_values, test, dataset_name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances of the models\n",
    "target_dep = TargetDep()\n",
    "target_dep_plus = TargetDepSent()\n",
    "models = [target_dep, target_dep_plus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target dependent model mass evaluation\n",
    "\n",
    "The above code loads all of the data, models, and lexicons we are going to use in this notebook\n",
    "\n",
    "We are going to use three different models.\n",
    "1. target_dep -- Target Dependent model that uses no sentiment lexicons\n",
    "2. target_dep_plus -- Target Dependent model that uses only the Hu & Liu lexicon\n",
    "3. target_dep_plus_all -- Target Dependent model that uses all three lexicons from the original paper\n",
    "\n",
    "Each model gets it's own results file where it will store the results from each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Target Dependent: ({'random_state': 42, 'word_vector': [sswe]},\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent.tsv',\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent C.json',\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent word vector.json',\n",
       "  '/home/moorea/tdparse/model zoo'),\n",
       " Target Dependent Plus: ({'random_state': 42,\n",
       "   'sentiment_lexicon': <tdparse.lexicons.Lexicon at 0x7f9453f990b8>,\n",
       "   'word_vector': [sswe]},\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent+.tsv',\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent+ C.json',\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent+ word vector.json',\n",
       "  '/home/moorea/tdparse/model zoo')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the result files\n",
    "result_folder = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results', 'Target Dependent Models'))\n",
    "model_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'model zoo'))\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_result_files = ['Target Dependent.tsv', 'Target Dependent+.tsv']\n",
    "model_result_files = [os.path.join(result_folder, result_file) for result_file in model_result_files]\n",
    "C_result_files = ['Target Dependent C.json', 'Target Dependent+ C.json']\n",
    "C_result_files = [os.path.join(result_folder, result_file) for result_file in C_result_files]\n",
    "word_vector_result_files = ['Target Dependent word vector.json', 'Target Dependent+ word vector.json']\n",
    "word_vector_result_files = [os.path.join(result_folder, result_file) for result_file in word_vector_result_files]\n",
    "# Parameters for each model\n",
    "std_model_parameters = {'word_vector' : [sswe], 'random_state' : 42}\n",
    "all_senti_model_parameters = {**std_model_parameters, 'sentiment_lexicon' : all_three_low}\n",
    "model_parameters = [std_model_parameters, all_senti_model_parameters]\n",
    "# Combining parameters and result files\n",
    "parameters_files = list(zip(model_parameters, model_result_files, C_result_files, \n",
    "                            word_vector_result_files, [model_dir]*2))\n",
    "\n",
    "model_files = dict(zip(models, parameters_files))\n",
    "model_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset SemEval 14 Laptop\n",
      "Processing model Target Dependent\n",
      "[sswe] 62.69611859321594\n",
      "[glove 300d 42b common crawl] 335.0806725025177\n",
      "Best parameters for dataset SemEval 14 Laptop are: {'word_vector': [glove 300d 42b common crawl], 'random_state': 42, 'C': 0.0035, 'tokeniser': <function ark_twokenize at 0x7f95b335e9d8>}\n",
      "Model saved to /home/moorea/tdparse/model zoo/Target Dependent SemEval 14 Laptop. Save time 401.49\n",
      "saving raw data\n",
      "Processing model Target Dependent Plus\n",
      "[sswe] 87.44546413421631\n",
      "[glove 300d 42b common crawl] 609.4112818241119\n",
      "Best parameters for dataset SemEval 14 Laptop are: {'word_vector': [glove 300d 42b common crawl], 'random_state': 42, 'C': 0.0035, 'tokeniser': <function ark_twokenize at 0x7f95b335e9d8>, 'senti_lexicon': <tdparse.lexicons.Lexicon object at 0x7f9453f990b8>}\n",
      "Model saved to /home/moorea/tdparse/model zoo/Target Dependent Plus SemEval 14 Laptop. Save time 416.15\n",
      "saving raw data\n",
      "Processing dataset SemEval 14 Restaurant\n",
      "Processing model Target Dependent\n",
      "[sswe] 90.60265254974365\n",
      "[glove 300d 42b common crawl] 512.3801245689392\n",
      "Best parameters for dataset SemEval 14 Restaurant are: {'word_vector': [glove 300d 42b common crawl], 'random_state': 42, 'C': 0.007, 'tokeniser': <function ark_twokenize at 0x7f95b335e9d8>}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time_to_process = time.time()\n",
    "for dataset_name, train_test in dataset_train_test.items():\n",
    "    print('Processing dataset {}'.format(dataset_name))\n",
    "    train, test = train_test\n",
    "    for model, parameter_file_paths in model_files.items():\n",
    "        print('Processing model {}'.format(model))\n",
    "        params_files = parameter_file_paths\n",
    "        parameters = params_files[0]\n",
    "        result_file_path = params_files[1]\n",
    "        c_fp = params_files[2]\n",
    "        word_vectors_fp = params_files[3]\n",
    "        model_dir = params_files[4]\n",
    "        dataset_predictions(train, test, dataset_name, model, \n",
    "                            result_file_path=result_file_path,\n",
    "                            re_write=True, save_raw_data=True,\n",
    "                            c_file_path=c_fp,\n",
    "                            word_vector_file_path=word_vectors_fp, \n",
    "                            model_dir=model_dir,\n",
    "                            **parameters)\n",
    "\n",
    "time_to_process = time.time() - time_to_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time it took to process all the datasets {{round(time_to_process / 3600, 2)}} hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The affect of the C value\n",
    "\n",
    "We take a look at the affect of tunning for the C-Value. We tune the C-Value in two steps:\n",
    "1. Coarse grain search over the following values: from 0.00001 to 10 going up by a factor of 10. Once the best best coarse grain value is found we fine tune it.\n",
    "2. We multiple the best coarse grain value by the following value and search over these values: 0.35, 0.7, 1, 3.5, and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values_target_dep_plus = C_result_files[1]\n",
    "c_values_target_dep_plus = get_json_data(c_values_target_dep_plus, 'Dong Twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stats = defaultdict(lambda: dict())\n",
    "\n",
    "mean_error = lambda acc_values: sum(max(acc_values) - acc_values) / (len(acc_values) - 1)\n",
    "for dataset_name in dataset_train_test:\n",
    "    # Target Dependent Plus using the Hu and Liu Lexicon C file\n",
    "    c_values_target_dep_plus = C_result_files[1]\n",
    "    c_values_target_dep_plus = get_json_data(c_values_target_dep_plus, dataset_name)\n",
    "    model_results_file = model_result_files[1]\n",
    "    fine_c_values, best_coarse_c_value = fine_tune_values(c_values_target_dep_plus)\n",
    "    coarse_c_values = coarse_tune_values(c_values_target_dep_plus)\n",
    "    all_c_values = {float(c_value) : acc for c_value, acc in c_values_target_dep_plus.items()}\n",
    "    fine_acc = np.array([all_c_values[c_value] for c_value in fine_c_values])\n",
    "    coarse_acc = np.array([all_c_values[c_value] for c_value in coarse_c_values])\n",
    "    all_acc = np.array(list(c_values_target_dep_plus.values()))\n",
    "    best_c = best_c_value(all_c_values)\n",
    "    test_accuracy = get_results(model_results_file, dataset_name)['Accuracy']\n",
    "    dataset_stats[dataset_name]['Fine Tune MAE (Accuracy %)'] = mean_error(fine_acc) * 100\n",
    "    dataset_stats[dataset_name]['Coarse Tune MAE (Accuracy %)'] = mean_error(coarse_acc) * 100\n",
    "    dataset_stats[dataset_name]['Best Fine C value'] = best_c\n",
    "    dataset_stats[dataset_name]['Best Coarse C value'] = best_coarse_c_value\n",
    "    dataset_stats[dataset_name]['Test score Accuracy (%)'] = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = list(dataset_train_test)\n",
    "columns = ['Fine Tune MAE (Accuracy %)', 'Coarse Tune MAE (Accuracy %)', 'Best Fine C value', \n",
    "           'Best Coarse C value', 'Dataset', 'Test score Accuracy (%)']\n",
    "c_value_stats = pd.DataFrame(np.zeros((len(dataset_train_test), 6)), columns=columns)\n",
    "c_value_stats['Dataset'] = index\n",
    "c_value_stats = c_value_stats.set_index('Dataset')\n",
    "\n",
    "# Add the data to the DataFrame\n",
    "for dataset_name, col_value in dataset_stats.items():\n",
    "    for column, value in col_value.items():\n",
    "        c_value_stats[column][dataset_name] = value\n",
    "c_value_stats = c_value_stats.round({'Fine Tune Accuracy std (%)' : 2, \n",
    "                                     'Coarse Tune Accuracy std (%)' : 2})\n",
    "c_value_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
