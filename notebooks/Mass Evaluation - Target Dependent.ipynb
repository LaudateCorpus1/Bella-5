{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "**Please ensure that you have ran the *Product and YouTuBean train test split* notebook first so that all of the datasets are avaliable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Helper functions\n",
    "from tdparse.notebook_helper import write_json_data, get_json_data\n",
    "# Models\n",
    "from tdparse.models.target import TargetInd\n",
    "from tdparse.models.target import TargetDepC\n",
    "from tdparse.models.target import TargetDep\n",
    "from tdparse.models.target import TargetDepSent\n",
    "# Word Vector methods\n",
    "from tdparse.word_vectors import GensimVectors\n",
    "from tdparse.word_vectors import PreTrained\n",
    "from tdparse.helper import read_config, full_path\n",
    "# Sentiment lexicons\n",
    "from tdparse import lexicons\n",
    "# Get the data\n",
    "from tdparse.parsers import semeval_14, semeval_15_16, dong, election\n",
    "from tdparse.data_types import TargetCollection\n",
    "# Evaluation methods\n",
    "from tdparse.evaluation import evaluation_results, scores, get_results, \\\n",
    "                               save_results, combine_results, get_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of the datasets\n",
    "youtubean_train = semeval_14(full_path(read_config('youtubean_train')))\n",
    "youtubean_test = semeval_14(full_path(read_config('youtubean_test')))\n",
    "semeval_14_rest_train = semeval_14(full_path(read_config('semeval_2014_rest_train')))\n",
    "semeval_14_lap_train = semeval_14(full_path(read_config('semeval_2014_lap_train')))\n",
    "semeval_14_rest_test = semeval_14(full_path(read_config('semeval_2014_rest_test')))\n",
    "semeval_14_lap_test = semeval_14(full_path(read_config('semeval_2014_lap_test')))\n",
    "semeval_15_rest_test = semeval_15_16(full_path(read_config('semeval_2015_rest_test')))\n",
    "semeval_16_rest_test = semeval_15_16(full_path(read_config('semeval_2016_rest_test')),\n",
    "                                     sep_16_from_15=True)\n",
    "dong_train = dong(full_path(read_config('dong_twit_train_data')))\n",
    "dong_test = dong(full_path(read_config('dong_twit_test_data')))\n",
    "election_train, election_test = election(full_path(read_config('election_folder_dir')))\n",
    "product_reviews_train = semeval_14(full_path(read_config('product_train')))\n",
    "product_reviews_test = semeval_14(full_path(read_config('product_test')))\n",
    "# Combine semeval 14 resturant train and test\n",
    "semeval_14_rest_all = TargetCollection.combine_collections(semeval_14_rest_train,\n",
    "                                                           semeval_14_rest_test)\n",
    "# Combine semeval 14 resturant all with 15 test\n",
    "semeval_14_15 = TargetCollection.combine_collections(semeval_14_rest_all,\n",
    "                                                     semeval_15_rest_test)\n",
    "\n",
    "dataset_train_test = {'SemEval 14 Laptop' : (semeval_14_lap_train, semeval_14_lap_test),\n",
    "                      'SemEval 14 Restaurant' : (semeval_14_rest_train, semeval_14_rest_test),\n",
    "                      'SemEval 16 Restaurant 14 Train' : (semeval_14_rest_train, semeval_16_rest_test),\n",
    "                      'SemEval 16 Restaurant 14 All' : (semeval_14_rest_all, semeval_16_rest_test),\n",
    "                      'SemEval 16 Restaurant 15&14' : (semeval_14_15, semeval_16_rest_test),\n",
    "                      'Dong Twitter' : (dong_train, dong_test),\n",
    "                      'Election Twitter' : (election_train, election_test),\n",
    "                      'Product Reviews' : (product_reviews_train, product_reviews_test),\n",
    "                      'YouTuBean' : (youtubean_train, youtubean_test)\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word vectors\n",
    "w2v_path = full_path(read_config('word2vec_files')['vo_zhang'])\n",
    "w2v = GensimVectors(w2v_path, None, model='word2vec', name='w2v')\n",
    "sswe_path = full_path(read_config('sswe_files')['vo_zhang'])\n",
    "sswe = PreTrained(sswe_path, name='sswe')\n",
    "\n",
    "# Load the sentiment lexicons and remove all words that are not associated\n",
    "# to the Positive or Negative class.\n",
    "subset_cats = {'positive', 'negative'}\n",
    "mpqa_low = lexicons.Mpqa(subset_cats=subset_cats, lower=True)\n",
    "nrc_low = lexicons.NRC(subset_cats=subset_cats, lower=True)\n",
    "hu_liu_low = lexicons.HuLiu(subset_cats=subset_cats, lower=True)\n",
    "mpqa_huliu_low = lexicons.Lexicon.combine_lexicons(mpqa_low, hu_liu_low)\n",
    "all_three_low = lexicons.Lexicon.combine_lexicons(mpqa_huliu_low, nrc_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_predictions(train, test, name, model, word_vector, random_state, \n",
    "                        sentiment_lexicon=None, result_file_path=None,\n",
    "                        c_file_path=None, re_write=True, save_raw_data=True):\n",
    "    if not re_write and result_file_path is not None:\n",
    "        results_df = get_results(result_file_path, name)\n",
    "        if save_raw_data and results_df is not None:\n",
    "            if get_raw_data(result_file_path, name, test):\n",
    "                return results_df\n",
    "        elif results_df is not None:\n",
    "            return results_df\n",
    "    # loading the data\n",
    "    data_train = train.data()\n",
    "    y_train = train.sentiment_data()\n",
    "    data_test = test.data()\n",
    "    y_test = test.sentiment_data()\n",
    "\n",
    "    # Finding the best C value for the model on this dataset\n",
    "    c_grid_params = {'word_vectors' : [word_vector], 'random_state' : random_state}\n",
    "    if sentiment_lexicon is not None:\n",
    "        c_grid_params['senti_lexicons'] = [sentiment_lexicon]\n",
    "    best_c, c_scores = model.find_best_c(data_train, y_train, \n",
    "                                         grid_params=c_grid_params, cv=5, n_jobs=6)\n",
    "    if c_file_path is not None:\n",
    "        write_json_data(c_file_path, name, c_scores)\n",
    "    if sentiment_lexicon is not None:\n",
    "        print('The best C value for {} model with sentiment lexicon {}: {}'\\\n",
    "              .format(model, sentiment_lexicon, best_c))\n",
    "    else:\n",
    "        print('The best C value for {} model: {}'.format(model, best_c))\n",
    "    \n",
    "    # Fitting and getting predictions from the model.\n",
    "    parameters = {'word_vector' : word_vector, 'random_state' : random_state, 'C' : best_c}\n",
    "    if sentiment_lexicon is not None:\n",
    "        parameters['senti_lexicon'] = sentiment_lexicon\n",
    "    best_params = model.get_params(**parameters)\n",
    "    model.fit(data_train, y_train, params=best_params)\n",
    "    predicted_values = model.predict(data_test)\n",
    "    # Return the results\n",
    "    if result_file_path is not None:\n",
    "        return evaluation_results(predicted_values, test, name, \n",
    "                                  file_name=result_file_path, \n",
    "                                  save_raw_data=save_raw_data, re_write=re_write)\n",
    "    else:\n",
    "        return evaluation_results(predicted_values, test, name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances of the models\n",
    "target_dep = TargetDep()\n",
    "target_dep_plus = TargetDepSent()\n",
    "target_dep_plus_all = TargetDepSent()\n",
    "models = [target_dep, target_dep_plus, target_dep_plus_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target dependent model mass evaluation\n",
    "\n",
    "The above code loads all of the data, models, and lexicons we are going to use in this notebook\n",
    "\n",
    "We are going to use three different models.\n",
    "1. target_dep -- Target Dependent model that uses no sentiment lexicons\n",
    "2. target_dep_plus -- Target Dependent model that uses only the Hu & Liu lexicon\n",
    "3. target_dep_plus_all -- Target Dependent model that uses all three lexicons from the original paper\n",
    "\n",
    "Each model gets it's own results file where it will store the results from each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Target Dependent: ({'random_state': 42, 'word_vector': [w2v, sswe]},\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent.tsv',\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent C.json'),\n",
       " Target Dependent Plus: ({'random_state': 42,\n",
       "   'sentiment_lexicon': <tdparse.lexicons.HuLiu at 0x7f7fa51d4da0>,\n",
       "   'word_vector': [w2v, sswe]},\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent+ Hu&Liu.tsv',\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent+ Hu&Liu C.json'),\n",
       " Target Dependent Plus: ({'random_state': 42,\n",
       "   'sentiment_lexicon': <tdparse.lexicons.Lexicon at 0x7f7fa3233128>,\n",
       "   'word_vector': [w2v, sswe]},\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent+ All.tsv',\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent+ All C.json')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the result files\n",
    "result_folder = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results', 'Target Dependent Models'))\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "model_result_files = ['Target Dependent.tsv', 'Target Dependent+ Hu&Liu.tsv',\n",
    "                      'Target Dependent+ All.tsv']\n",
    "model_result_files = [os.path.join(result_folder, result_file) for result_file in model_result_files]\n",
    "C_result_files = ['Target Dependent C.json', 'Target Dependent+ Hu&Liu C.json',\n",
    "                      'Target Dependent+ All C.json']\n",
    "C_result_files = [os.path.join(result_folder, result_file) for result_file in C_result_files]\n",
    "# Parameters for each model\n",
    "std_model_parameters = {'word_vector' : [w2v, sswe], 'random_state' : 42}\n",
    "hu_liu_model_parameters = {**std_model_parameters, 'sentiment_lexicon' : hu_liu_low}\n",
    "all_senti_model_parameters = {**std_model_parameters, 'sentiment_lexicon' : all_three_low}\n",
    "model_parameters = [std_model_parameters, hu_liu_model_parameters, all_senti_model_parameters]\n",
    "# Combining parameters and result files\n",
    "parameters_files = list(zip(model_parameters, model_result_files, C_result_files))\n",
    "\n",
    "model_files = dict(zip(models, parameters_files))\n",
    "model_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset SemEval 14 Laptop\n",
      "Processing model Target Dependent\n",
      "Processing model Target Dependent Plus\n",
      "Processing model Target Dependent Plus\n",
      "Processing dataset SemEval 14 Restaurant\n",
      "Processing model Target Dependent\n",
      "Processing model Target Dependent Plus\n",
      "Processing model Target Dependent Plus\n",
      "Processing dataset SemEval 16 Restaurant 14 Train\n",
      "Processing model Target Dependent\n",
      "Processing model Target Dependent Plus\n",
      "Processing model Target Dependent Plus\n",
      "Processing dataset SemEval 16 Restaurant 14 All\n",
      "Processing model Target Dependent\n",
      "Processing model Target Dependent Plus\n",
      "Processing model Target Dependent Plus\n",
      "Processing dataset SemEval 16 Restaurant 15&14\n",
      "Processing model Target Dependent\n",
      "Processing model Target Dependent Plus\n",
      "Processing model Target Dependent Plus\n",
      "Processing dataset Dong Twitter\n",
      "Processing model Target Dependent\n",
      "Processing model Target Dependent Plus\n",
      "Processing model Target Dependent Plus\n",
      "Processing dataset Election Twitter\n",
      "Processing model Target Dependent\n",
      "Processing model Target Dependent Plus\n",
      "Processing model Target Dependent Plus\n",
      "Processing dataset Product Reviews\n",
      "Processing model Target Dependent\n",
      "Processing model Target Dependent Plus\n",
      "Processing model Target Dependent Plus\n",
      "Processing dataset YouTuBean\n",
      "Processing model Target Dependent\n",
      "Processing model Target Dependent Plus\n",
      "Processing model Target Dependent Plus\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time_to_process = time.time()\n",
    "for dataset_name, train_test in dataset_train_test.items():\n",
    "    print('Processing dataset {}'.format(dataset_name))\n",
    "    train, test = train_test\n",
    "    for model, parameter_file_paths in model_files.items():\n",
    "        print('Processing model {}'.format(model))\n",
    "        parameters, result_file_path, c_file_path = parameter_file_paths\n",
    "        dataset_predictions(train, test, dataset_name, model, \n",
    "                            result_file_path=result_file_path,\n",
    "                            c_file_path=c_file_path,\n",
    "                            re_write=False, save_raw_data=True,\n",
    "                            **parameters)\n",
    "\n",
    "time_to_process = time.time() - time_to_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "round(time_to_process / 3600, 2)": {}
    }
   },
   "source": [
    "Time it took to process all the datasets {{round(time_to_process / 3600, 2)}} hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The affect of the C value\n",
    "\n",
    "We take a look at the affect of tunning for the C-Value. We tune the C-Value in two steps:\n",
    "1. Coarse grain search over the following values: from 0.00001 to 10 going up by a factor of 10. Once the best best coarse grain value is found we fine tune it.\n",
    "2. We multiple the best coarse grain value by the following value and search over these values: 0.35, 0.7, 1, 3.5, and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values_target_dep_plus = C_result_files[1]\n",
    "c_values_target_dep_plus = get_json_data(c_values_target_dep_plus, 'Dong Twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_values(c_values):\n",
    "    fine_values = []\n",
    "    for c_value in c_values:\n",
    "        if '35' in c_value or '7' in c_value:\n",
    "            fine_values.append(float(c_value))\n",
    "    fine_values = sorted(fine_values)\n",
    "    best_coarse_c_value = fine_values[3] / 7\n",
    "    fine_values.append(best_coarse_c_value)\n",
    "    return fine_values, best_coarse_c_value\n",
    "def coarse_tune_values(c_values):\n",
    "    coarse_values = []\n",
    "    fine_values = fine_tune_values(c_values)\n",
    "    for c_value in c_values:\n",
    "        c_value = float(c_value)\n",
    "        if c_value not in fine_values:\n",
    "            coarse_values.append(c_value)\n",
    "    return coarse_values\n",
    "def best_c_value(c_values):\n",
    "    best = 0\n",
    "    best_c = 0\n",
    "    for c_value, acc in c_values.items():\n",
    "        if acc > best:\n",
    "            best_c = c_value\n",
    "            best = acc\n",
    "    return best_c\n",
    "\n",
    "dataset_stats = defaultdict(lambda: dict())\n",
    "\n",
    "mean_error = lambda acc_values: sum(max(acc_values) - acc_values) / (len(acc_values) - 1)\n",
    "for dataset_name in dataset_train_test:\n",
    "    # Target Dependent Plus using the Hu and Liu Lexicon C file\n",
    "    c_values_target_dep_plus = C_result_files[1]\n",
    "    c_values_target_dep_plus = get_json_data(c_values_target_dep_plus, dataset_name)\n",
    "    model_results_file = model_result_files[1]\n",
    "    fine_c_values, best_coarse_c_value = fine_tune_values(c_values_target_dep_plus)\n",
    "    coarse_c_values = coarse_tune_values(c_values_target_dep_plus)\n",
    "    all_c_values = {float(c_value) : acc for c_value, acc in c_values_target_dep_plus.items()}\n",
    "    fine_acc = np.array([all_c_values[c_value] for c_value in fine_c_values])\n",
    "    coarse_acc = np.array([all_c_values[c_value] for c_value in coarse_c_values])\n",
    "    all_acc = np.array(list(c_values_target_dep_plus.values()))\n",
    "    best_c = best_c_value(all_c_values)\n",
    "    test_accuracy = get_results(model_results_file, dataset_name)['Accuracy']\n",
    "    dataset_stats[dataset_name]['Fine Tune MAE (Accuracy %)'] = mean_error(fine_acc) * 100\n",
    "    dataset_stats[dataset_name]['Coarse Tune MAE (Accuracy %)'] = mean_error(coarse_acc) * 100\n",
    "    dataset_stats[dataset_name]['Best Fine C value'] = best_c\n",
    "    dataset_stats[dataset_name]['Best Coarse C value'] = best_coarse_c_value\n",
    "    dataset_stats[dataset_name]['Test score Accuracy (%)'] = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fine Tune MAE (Accuracy %)</th>\n",
       "      <th>Coarse Tune MAE (Accuracy %)</th>\n",
       "      <th>Best Fine C value</th>\n",
       "      <th>Best Coarse C value</th>\n",
       "      <th>Test score Accuracy (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SemEval 14 Laptop</th>\n",
       "      <td>1.491569</td>\n",
       "      <td>6.168036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.001</td>\n",
       "      <td>70.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SemEval 14 Restaurant</th>\n",
       "      <td>0.506663</td>\n",
       "      <td>3.846628</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.010</td>\n",
       "      <td>76.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SemEval 16 Restaurant 14 Train</th>\n",
       "      <td>0.506663</td>\n",
       "      <td>3.846628</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.010</td>\n",
       "      <td>81.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SemEval 16 Restaurant 14 All</th>\n",
       "      <td>0.481787</td>\n",
       "      <td>3.924891</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.010</td>\n",
       "      <td>82.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SemEval 16 Restaurant 15&amp;14</th>\n",
       "      <td>0.672119</td>\n",
       "      <td>4.092248</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.010</td>\n",
       "      <td>82.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dong Twitter</th>\n",
       "      <td>1.216389</td>\n",
       "      <td>5.969910</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.010</td>\n",
       "      <td>70.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Election Twitter</th>\n",
       "      <td>0.569032</td>\n",
       "      <td>3.763862</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>54.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Product Reviews</th>\n",
       "      <td>0.747724</td>\n",
       "      <td>3.828926</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.010</td>\n",
       "      <td>83.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YouTuBean</th>\n",
       "      <td>1.120072</td>\n",
       "      <td>2.787734</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>76.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Fine Tune MAE (Accuracy %)  \\\n",
       "Dataset                                                      \n",
       "SemEval 14 Laptop                                 1.491569   \n",
       "SemEval 14 Restaurant                             0.506663   \n",
       "SemEval 16 Restaurant 14 Train                    0.506663   \n",
       "SemEval 16 Restaurant 14 All                      0.481787   \n",
       "SemEval 16 Restaurant 15&14                       0.672119   \n",
       "Dong Twitter                                      1.216389   \n",
       "Election Twitter                                  0.569032   \n",
       "Product Reviews                                   0.747724   \n",
       "YouTuBean                                         1.120072   \n",
       "\n",
       "                                Coarse Tune MAE (Accuracy %)  \\\n",
       "Dataset                                                        \n",
       "SemEval 14 Laptop                                   6.168036   \n",
       "SemEval 14 Restaurant                               3.846628   \n",
       "SemEval 16 Restaurant 14 Train                      3.846628   \n",
       "SemEval 16 Restaurant 14 All                        3.924891   \n",
       "SemEval 16 Restaurant 15&14                         4.092248   \n",
       "Dong Twitter                                        5.969910   \n",
       "Election Twitter                                    3.763862   \n",
       "Product Reviews                                     3.828926   \n",
       "YouTuBean                                           2.787734   \n",
       "\n",
       "                                Best Fine C value  Best Coarse C value  \\\n",
       "Dataset                                                                  \n",
       "SemEval 14 Laptop                          0.0035                0.001   \n",
       "SemEval 14 Restaurant                      0.0070                0.010   \n",
       "SemEval 16 Restaurant 14 Train             0.0070                0.010   \n",
       "SemEval 16 Restaurant 14 All               0.0035                0.010   \n",
       "SemEval 16 Restaurant 15&14                0.0070                0.010   \n",
       "Dong Twitter                               0.0035                0.010   \n",
       "Election Twitter                           0.0007                0.001   \n",
       "Product Reviews                            0.0070                0.010   \n",
       "YouTuBean                                  0.0010                0.001   \n",
       "\n",
       "                                Test score Accuracy (%)  \n",
       "Dataset                                                  \n",
       "SemEval 14 Laptop                                  70.4  \n",
       "SemEval 14 Restaurant                              76.1  \n",
       "SemEval 16 Restaurant 14 Train                     81.4  \n",
       "SemEval 16 Restaurant 14 All                       82.3  \n",
       "SemEval 16 Restaurant 15&14                        82.6  \n",
       "Dong Twitter                                       70.7  \n",
       "Election Twitter                                   54.9  \n",
       "Product Reviews                                    83.1  \n",
       "YouTuBean                                          76.2  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = list(dataset_train_test)\n",
    "columns = ['Fine Tune MAE (Accuracy %)', 'Coarse Tune MAE (Accuracy %)', 'Best Fine C value', \n",
    "           'Best Coarse C value', 'Dataset', 'Test score Accuracy (%)']\n",
    "c_value_stats = pd.DataFrame(np.zeros((len(dataset_train_test), 6)), columns=columns)\n",
    "c_value_stats['Dataset'] = index\n",
    "c_value_stats = c_value_stats.set_index('Dataset')\n",
    "\n",
    "# Add the data to the DataFrame\n",
    "for dataset_name, col_value in dataset_stats.items():\n",
    "    for column, value in col_value.items():\n",
    "        c_value_stats[column][dataset_name] = value\n",
    "c_value_stats = c_value_stats.round({'Fine Tune Accuracy std (%)' : 2, \n",
    "                                     'Coarse Tune Accuracy std (%)' : 2})\n",
    "c_value_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
