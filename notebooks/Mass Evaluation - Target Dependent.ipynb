{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "**Please ensure that you have ran the *Product and YouTuBean train test split* notebook first so that all of the datasets are avaliable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Helper functions\n",
    "from tdparse.notebook_helper import write_json_data, get_json_data\n",
    "# Models\n",
    "from tdparse.models.target import TargetInd\n",
    "from tdparse.models.target import TargetDepC\n",
    "from tdparse.models.target import TargetDep\n",
    "from tdparse.models.target import TargetDepSent\n",
    "# Word Vector methods\n",
    "from tdparse.word_vectors import GloveWikiGiga, GloveTwitterVectors, GensimVectors, GloveCommonCrawl\n",
    "from tdparse.word_vectors import PreTrained\n",
    "from tdparse.helper import read_config, full_path\n",
    "# Sentiment lexicons\n",
    "from tdparse import lexicons\n",
    "# Get the data\n",
    "from tdparse.parsers import semeval_14, semeval_15_16, dong, election\n",
    "from tdparse.data_types import TargetCollection\n",
    "# Evaluation methods\n",
    "from tdparse.evaluation import evaluation_results, scores, get_results, \\\n",
    "                               save_results, combine_results, get_raw_data\n",
    "# Tokenisers\n",
    "from tdparse.tokenisers import ark_twokenize, whitespace, stanford\n",
    "from tdparse.stanford_tools import constituency_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_values(c_values):\n",
    "    fine_values = []\n",
    "    for c_value in c_values:\n",
    "        if '35' in c_value or '7' in c_value:\n",
    "            fine_values.append(float(c_value))\n",
    "    fine_values = sorted(fine_values)\n",
    "    best_coarse_c_value = fine_values[3] / 7\n",
    "    fine_values.append(best_coarse_c_value)\n",
    "    return fine_values, best_coarse_c_value\n",
    "def coarse_tune_values(c_values):\n",
    "    coarse_values = []\n",
    "    fine_values = fine_tune_values(c_values)\n",
    "    for c_value in c_values:\n",
    "        c_value = float(c_value)\n",
    "        if c_value not in fine_values:\n",
    "            coarse_values.append(c_value)\n",
    "    return coarse_values\n",
    "def best_c_value(c_values):\n",
    "    best = 0\n",
    "    best_c = 0\n",
    "    for c_value, acc in c_values.items():\n",
    "        if acc > best:\n",
    "            best_c = c_value\n",
    "            best = acc\n",
    "    return best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of the datasets\n",
    "youtubean_train = semeval_14(full_path(read_config('youtubean_train')))\n",
    "youtubean_test = semeval_14(full_path(read_config('youtubean_test')))\n",
    "semeval_14_rest_train = semeval_14(full_path(read_config('semeval_2014_rest_train')))\n",
    "semeval_14_lap_train = semeval_14(full_path(read_config('semeval_2014_lap_train')))\n",
    "semeval_14_rest_test = semeval_14(full_path(read_config('semeval_2014_rest_test')))\n",
    "semeval_14_lap_test = semeval_14(full_path(read_config('semeval_2014_lap_test')))\n",
    "semeval_15_rest_test = semeval_15_16(full_path(read_config('semeval_2015_rest_test')))\n",
    "semeval_16_rest_test = semeval_15_16(full_path(read_config('semeval_2016_rest_test')),\n",
    "                                     sep_16_from_15=True)\n",
    "dong_train = dong(full_path(read_config('dong_twit_train_data')))\n",
    "dong_test = dong(full_path(read_config('dong_twit_test_data')))\n",
    "election_train, election_test = election(full_path(read_config('election_folder_dir')))\n",
    "product_reviews_train = semeval_14(full_path(read_config('product_train')))\n",
    "product_reviews_test = semeval_14(full_path(read_config('product_test')))\n",
    "# Combine semeval 14 resturant train and test\n",
    "semeval_14_rest_all = TargetCollection.combine_collections(semeval_14_rest_train,\n",
    "                                                           semeval_14_rest_test)\n",
    "# Combine semeval 14 resturant all with 15 test\n",
    "semeval_14_15 = TargetCollection.combine_collections(semeval_14_rest_all,\n",
    "                                                     semeval_15_rest_test)\n",
    "\n",
    "dataset_train_test = {'SemEval 14 Laptop' : (semeval_14_lap_train, semeval_14_lap_test),\n",
    "                      'SemEval 14 Restaurant' : (semeval_14_rest_train, semeval_14_rest_test),\n",
    "                      'SemEval 16 Restaurant 14 Train' : (semeval_14_rest_train, semeval_16_rest_test),\n",
    "                      'SemEval 16 Restaurant 14 All' : (semeval_14_rest_all, semeval_16_rest_test),\n",
    "                      'SemEval 16 Restaurant 15&14' : (semeval_14_15, semeval_16_rest_test),\n",
    "                      'Dong Twitter' : (dong_train, dong_test),\n",
    "                      'Election Twitter' : (election_train, election_test),\n",
    "                      'Product Reviews' : (product_reviews_train, product_reviews_test),\n",
    "                      'YouTuBean' : (youtubean_train, youtubean_test)\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word vectors\n",
    "w2v_path = full_path(read_config('word2vec_files')['vo_zhang'])\n",
    "w2v = GensimVectors(w2v_path, None, model='word2vec', name='w2v')\n",
    "sswe_path = full_path(read_config('sswe_files')['vo_zhang'])\n",
    "sswe = PreTrained(sswe_path, name='sswe')\n",
    "\n",
    "glove_twit_50 = GloveTwitterVectors(50)\n",
    "glove_twit_100 = GloveTwitterVectors(100)\n",
    "glove_twit_200 = GloveTwitterVectors(200)\n",
    "glove_50 = GloveWikiGiga(50)\n",
    "glove_100 = GloveWikiGiga(100)\n",
    "glove_200 = GloveWikiGiga(200)\n",
    "glove_300 = GloveCommonCrawl(version=42)\n",
    "\n",
    "\n",
    "# Load the sentiment lexicons and remove all words that are not associated\n",
    "# to the Positive or Negative class.\n",
    "subset_cats = {'positive', 'negative'}\n",
    "mpqa_low = lexicons.Mpqa(subset_cats=subset_cats, lower=True)\n",
    "nrc_low = lexicons.NRC(subset_cats=subset_cats, lower=True)\n",
    "hu_liu_low = lexicons.HuLiu(subset_cats=subset_cats, lower=True)\n",
    "mpqa_huliu_low = lexicons.Lexicon.combine_lexicons(mpqa_low, hu_liu_low)\n",
    "all_three_low = lexicons.Lexicon.combine_lexicons(mpqa_huliu_low, nrc_low)\n",
    "all_lexicons = [mpqa_low, nrc_low, hu_liu_low, mpqa_huliu_low, all_three_low]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_predictions(train, test, dataset_name, model,\n",
    "                        word_vector, random_state,\n",
    "                        c_file_path, tokeniser_file_path,\n",
    "                        word_vector_file_path, senti_lexicon_file_path,\n",
    "                        model_dir,\n",
    "                        sentiment_lexicon=None, result_file_path=None,\n",
    "                        re_write=True, save_raw_data=True):\n",
    "    # Gets the results dataframe if it already exists\n",
    "    #if not re_write and result_file_path is not None:\n",
    "    #    results_df = get_results(result_file_path, name)\n",
    "    #    if save_raw_data and results_df is not None:\n",
    "    #        if get_raw_data(result_file_path, name, test):\n",
    "    #            return results_df\n",
    "    #    elif results_df is not None:\n",
    "    #        return results_df\n",
    "    # loading the data\n",
    "    data_train = train.data()\n",
    "    y_train = train.sentiment_data()\n",
    "    data_test = test.data()\n",
    "    y_test = test.sentiment_data()\n",
    "    \n",
    "    # CV grid params\n",
    "    c_grid_params = {'word_vectors' : [word_vector], 'random_state' : random_state}\n",
    "    if sentiment_lexicon is not None:\n",
    "        c_grid_params['senti_lexicons'] = [sentiment_lexicon]\n",
    "        \n",
    "    best_c, c_scores = model.find_best_c(data_train, y_train, c_grid_params, \n",
    "                                         save_file=c_file_path, dataset_name=dataset_name, \n",
    "                                         re_write=False, n_jobs=7, cv=5)\n",
    "    # Search over the different tokenizers\n",
    "    tokenisers = [ark_twokenize, whitespace, stanford]\n",
    "    tok_grid_params = {**c_grid_params}\n",
    "    tok_grid_params['tokenisers'] = tokenisers\n",
    "    tok_grid_params['C'] = [best_c]\n",
    "    best_tokeniser = model.save_grid_search(data_train, y_train, tok_grid_params, \n",
    "                                            'tokenisers', dataset_name, tokeniser_file_path, \n",
    "                                            re_write=False, n_jobs=5, cv=5)\n",
    "    if sentiment_lexicon is not None:\n",
    "        # Search over the different lexicons given the best tokeniser\n",
    "        senti_lexicon_grid_params = {**tok_grid_params}\n",
    "        senti_lexicon_grid_params['tokenisers'] = [best_tokeniser]\n",
    "        senti_lexicon_grid_params['senti_lexicons'] = all_lexicons\n",
    "        best_senti_lexicon = model.save_grid_search(data_train, y_train, senti_lexicon_grid_params, \n",
    "                                                    'senti_lexicons', dataset_name, senti_lexicon_file_path, \n",
    "                                                     re_write=False, n_jobs=5, cv=5)\n",
    "    # Search over the different word vectors given the best tokeniser\n",
    "    # and sentiment lexicon\n",
    "    word_vectors = [[glove_twit_50], [glove_50], [glove_200], [sswe], [w2v, sswe]]\n",
    "    word_vector_grid_params = {**tok_grid_params}\n",
    "    word_vector_grid_params['tokenisers'] = [best_tokeniser]\n",
    "    word_vector_grid_params['word_vectors'] = word_vectors\n",
    "    if sentiment_lexicon is not None:\n",
    "        word_vector_grid_params['senti_lexicons'] = [best_senti_lexicon]\n",
    "    import time\n",
    "    t = time.time()\n",
    "    best_word_vector = model.save_grid_search(data_train, y_train, word_vector_grid_params, \n",
    "                                              'word_vectors', dataset_name, word_vector_file_path, \n",
    "                                              re_write=False, n_jobs=5, cv=5)\n",
    "    print('{} {}'.format(best_word_vector, time.time() - t))\n",
    "    t = time.time()\n",
    "    # Word Vector is too large to multi-process\n",
    "    word_vectors.extend([[glove_twit_200], [glove_300]])\n",
    "    best_word_vector = model.save_grid_search(data_train, y_train, word_vector_grid_params, \n",
    "                                              'word_vectors', dataset_name, word_vector_file_path, \n",
    "                                              re_write=False, n_jobs=1, cv=5)\n",
    "    print('{} {}'.format(best_word_vector, time.time() - t))\n",
    "    \n",
    "    parameters = {'word_vector' : best_word_vector, 'random_state' : random_state, \n",
    "                  'C' : 0.01, 'tokeniser' : best_tokeniser}#best_c, 'tokeniser' : best_tokeniser}\n",
    "    if sentiment_lexicon is not None:\n",
    "        parameters['senti_lexicon'] = best_senti_lexicon\n",
    "    best_params = model.get_params(**parameters)\n",
    "    model.fit(data_train, y_train, params=best_params)\n",
    "    predicted_values = model.predict(data_test)\n",
    "    # Save the model to the model zoo\n",
    "    model_file_name = '{} {}'.format(model, dataset_name)\n",
    "    model_file_path = os.path.join(model_dir, model_file_name)\n",
    "    model.save_model(model_file_path, verbose=1)\n",
    "    # Return the results\n",
    "    if result_file_path is not None:\n",
    "        return evaluation_results(predicted_values, test, dataset_name, \n",
    "                                  file_name=result_file_path, \n",
    "                                  save_raw_data=save_raw_data, re_write=True)\n",
    "    else:\n",
    "        return evaluation_results(predicted_values, test, dataset_name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances of the models\n",
    "target_dep = TargetDep()\n",
    "target_dep_plus = TargetDepSent()\n",
    "models = [target_dep, target_dep_plus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target dependent model mass evaluation\n",
    "\n",
    "The above code loads all of the data, models, and lexicons we are going to use in this notebook\n",
    "\n",
    "We are going to use three different models.\n",
    "1. target_dep -- Target Dependent model that uses no sentiment lexicons\n",
    "2. target_dep_plus -- Target Dependent model that uses only the Hu & Liu lexicon\n",
    "3. target_dep_plus_all -- Target Dependent model that uses all three lexicons from the original paper\n",
    "\n",
    "Each model gets it's own results file where it will store the results from each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Target Dependent: ({'random_state': 42, 'word_vector': [sswe]},\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent.tsv',\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent C.json',\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent tokeniser.json',\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent word vector.json',\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent senti lexicon.json',\n",
       "  '/home/moorea/tdparse/model zoo'),\n",
       " Target Dependent Plus: ({'random_state': 42,\n",
       "   'sentiment_lexicon': <tdparse.lexicons.Lexicon at 0x7fadec17a668>,\n",
       "   'word_vector': [sswe]},\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent+.tsv',\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent+ All C.json',\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent+ All tokeniser.json',\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent+ All word vector.json',\n",
       "  '/home/moorea/tdparse/results/Target Dependent Models/Target Dependent+ All senti lexicon.json',\n",
       "  '/home/moorea/tdparse/model zoo')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the result files\n",
    "result_folder = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results', 'Target Dependent Models'))\n",
    "model_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'model zoo'))\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_result_files = ['Target Dependent.tsv', 'Target Dependent+.tsv']\n",
    "model_result_files = [os.path.join(result_folder, result_file) for result_file in model_result_files]\n",
    "C_result_files = ['Target Dependent C.json', 'Target Dependent+ All C.json']\n",
    "C_result_files = [os.path.join(result_folder, result_file) for result_file in C_result_files]\n",
    "tokeniser_result_files = ['Target Dependent tokeniser.json', 'Target Dependent+ All tokeniser.json']\n",
    "tokeniser_result_files = [os.path.join(result_folder, result_file) for result_file in tokeniser_result_files]\n",
    "word_vector_result_files = ['Target Dependent word vector.json', 'Target Dependent+ All word vector.json']\n",
    "word_vector_result_files = [os.path.join(result_folder, result_file) for result_file in word_vector_result_files]\n",
    "senti_lexicon_result_files = ['Target Dependent senti lexicon.json', 'Target Dependent+ All senti lexicon.json']\n",
    "senti_lexicon_result_files = [os.path.join(result_folder, result_file) for result_file in senti_lexicon_result_files]\n",
    "# Parameters for each model\n",
    "std_model_parameters = {'word_vector' : [sswe], 'random_state' : 42}\n",
    "all_senti_model_parameters = {**std_model_parameters, 'sentiment_lexicon' : all_three_low}\n",
    "model_parameters = [std_model_parameters, all_senti_model_parameters]\n",
    "# Combining parameters and result files\n",
    "parameters_files = list(zip(model_parameters, model_result_files, C_result_files, \n",
    "                            tokeniser_result_files, word_vector_result_files, \n",
    "                            senti_lexicon_result_files, [model_dir]*2))\n",
    "\n",
    "model_files = dict(zip(models, parameters_files))\n",
    "model_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset SemEval 14 Laptop\n",
      "Processing model Target Dependent\n",
      "yes ark_twokenize it has continued {'ark_twokenize': 0.6346735840899265, 'whitespace': 0.6221357544314743, 'stanford': 0.6238651102464332}\n",
      "yes whitespace it has continued {'ark_twokenize': 0.6346735840899265, 'whitespace': 0.6221357544314743, 'stanford': 0.6238651102464332}\n",
      "yes stanford it has continued {'ark_twokenize': 0.6346735840899265, 'whitespace': 0.6221357544314743, 'stanford': 0.6238651102464332}\n",
      "best <function ark_twokenize at 0x7fad981d1620> list {'ark_twokenize': <function ark_twokenize at 0x7fad981d1620>, 'whitespace': <function whitespace at 0x7fad9e32fa60>, 'stanford': <function stanford at 0x7fad97aec840>} stored [('ark_twokenize', 0.6346735840899265), ('stanford', 0.6238651102464332), ('whitespace', 0.6221357544314743)]\n",
      "yes glove twitter 50d it has continued {'glove twitter 50d': 0.6446173800259404, 'glove wiki giga 50d': 0.6610462602680501, 'glove wiki giga 200d': 0.6632079550367488, 'sswe': 0.6346735840899265, 'glove twitter 200d': 0.682230869001297, 'w2v sswe': 0.672719412019023, 'glove 300d 42b common crawl': 0.6817985300475573}\n",
      "yes glove wiki giga 50d it has continued {'glove twitter 50d': 0.6446173800259404, 'glove wiki giga 50d': 0.6610462602680501, 'glove wiki giga 200d': 0.6632079550367488, 'sswe': 0.6346735840899265, 'glove twitter 200d': 0.682230869001297, 'w2v sswe': 0.672719412019023, 'glove 300d 42b common crawl': 0.6817985300475573}\n",
      "yes glove wiki giga 200d it has continued {'glove twitter 50d': 0.6446173800259404, 'glove wiki giga 50d': 0.6610462602680501, 'glove wiki giga 200d': 0.6632079550367488, 'sswe': 0.6346735840899265, 'glove twitter 200d': 0.682230869001297, 'w2v sswe': 0.672719412019023, 'glove 300d 42b common crawl': 0.6817985300475573}\n",
      "yes sswe it has continued {'glove twitter 50d': 0.6446173800259404, 'glove wiki giga 50d': 0.6610462602680501, 'glove wiki giga 200d': 0.6632079550367488, 'sswe': 0.6346735840899265, 'glove twitter 200d': 0.682230869001297, 'w2v sswe': 0.672719412019023, 'glove 300d 42b common crawl': 0.6817985300475573}\n",
      "yes w2v sswe it has continued {'glove twitter 50d': 0.6446173800259404, 'glove wiki giga 50d': 0.6610462602680501, 'glove wiki giga 200d': 0.6632079550367488, 'sswe': 0.6346735840899265, 'glove twitter 200d': 0.682230869001297, 'w2v sswe': 0.672719412019023, 'glove 300d 42b common crawl': 0.6817985300475573}\n",
      "best (w2v, sswe) list {'glove twitter 50d': (glove twitter 50d,), 'glove wiki giga 50d': (glove wiki giga 50d,), 'glove wiki giga 200d': (glove wiki giga 200d,), 'sswe': (sswe,), 'w2v sswe': (w2v, sswe)} stored [('glove twitter 200d', 0.682230869001297), ('glove 300d 42b common crawl', 0.6817985300475573), ('w2v sswe', 0.672719412019023), ('glove wiki giga 200d', 0.6632079550367488), ('glove wiki giga 50d', 0.6610462602680501), ('glove twitter 50d', 0.6446173800259404), ('sswe', 0.6346735840899265)]\n",
      "[w2v, sswe] 79.22355651855469\n",
      "yes glove twitter 50d it has continued {'glove twitter 50d': 0.6446173800259404, 'glove wiki giga 50d': 0.6610462602680501, 'glove wiki giga 200d': 0.6632079550367488, 'sswe': 0.6346735840899265, 'glove twitter 200d': 0.682230869001297, 'w2v sswe': 0.672719412019023, 'glove 300d 42b common crawl': 0.6817985300475573}\n",
      "yes glove wiki giga 50d it has continued {'glove twitter 50d': 0.6446173800259404, 'glove wiki giga 50d': 0.6610462602680501, 'glove wiki giga 200d': 0.6632079550367488, 'sswe': 0.6346735840899265, 'glove twitter 200d': 0.682230869001297, 'w2v sswe': 0.672719412019023, 'glove 300d 42b common crawl': 0.6817985300475573}\n",
      "yes glove wiki giga 200d it has continued {'glove twitter 50d': 0.6446173800259404, 'glove wiki giga 50d': 0.6610462602680501, 'glove wiki giga 200d': 0.6632079550367488, 'sswe': 0.6346735840899265, 'glove twitter 200d': 0.682230869001297, 'w2v sswe': 0.672719412019023, 'glove 300d 42b common crawl': 0.6817985300475573}\n",
      "yes sswe it has continued {'glove twitter 50d': 0.6446173800259404, 'glove wiki giga 50d': 0.6610462602680501, 'glove wiki giga 200d': 0.6632079550367488, 'sswe': 0.6346735840899265, 'glove twitter 200d': 0.682230869001297, 'w2v sswe': 0.672719412019023, 'glove 300d 42b common crawl': 0.6817985300475573}\n",
      "yes w2v sswe it has continued {'glove twitter 50d': 0.6446173800259404, 'glove wiki giga 50d': 0.6610462602680501, 'glove wiki giga 200d': 0.6632079550367488, 'sswe': 0.6346735840899265, 'glove twitter 200d': 0.682230869001297, 'w2v sswe': 0.672719412019023, 'glove 300d 42b common crawl': 0.6817985300475573}\n",
      "yes glove twitter 200d it has continued {'glove twitter 50d': 0.6446173800259404, 'glove wiki giga 50d': 0.6610462602680501, 'glove wiki giga 200d': 0.6632079550367488, 'sswe': 0.6346735840899265, 'glove twitter 200d': 0.682230869001297, 'w2v sswe': 0.672719412019023, 'glove 300d 42b common crawl': 0.6817985300475573}\n",
      "yes glove 300d 42b common crawl it has continued {'glove twitter 50d': 0.6446173800259404, 'glove wiki giga 50d': 0.6610462602680501, 'glove wiki giga 200d': 0.6632079550367488, 'sswe': 0.6346735840899265, 'glove twitter 200d': 0.682230869001297, 'w2v sswe': 0.672719412019023, 'glove 300d 42b common crawl': 0.6817985300475573}\n",
      "best (glove twitter 200d,) list {'glove twitter 50d': (glove twitter 50d,), 'glove wiki giga 50d': (glove wiki giga 50d,), 'glove wiki giga 200d': (glove wiki giga 200d,), 'sswe': (sswe,), 'w2v sswe': (w2v, sswe), 'glove twitter 200d': (glove twitter 200d,), 'glove 300d 42b common crawl': (glove 300d 42b common crawl,)} stored [('glove twitter 200d', 0.682230869001297), ('glove 300d 42b common crawl', 0.6817985300475573), ('w2v sswe', 0.672719412019023), ('glove wiki giga 200d', 0.6632079550367488), ('glove wiki giga 50d', 0.6610462602680501), ('glove twitter 50d', 0.6446173800259404), ('sswe', 0.6346735840899265)]\n",
      "[glove twitter 200d] 192.70622491836548\n",
      "Model saved to /home/moorea/tdparse/model zoo/Target Dependent SemEval 14 Laptop. Save time 232.39\n",
      "saving raw data\n",
      "Re-writing over previous results\n",
      "Re-writing over previous RAW results\n",
      "Processing model Target Dependent Plus\n",
      "yes ark_twokenize it has continued {'ark_twokenize': 0.6753134457414613, 'whitespace': 0.6463467358408993, 'stanford': 0.6675313445741461}\n",
      "yes whitespace it has continued {'ark_twokenize': 0.6753134457414613, 'whitespace': 0.6463467358408993, 'stanford': 0.6675313445741461}\n",
      "yes stanford it has continued {'ark_twokenize': 0.6753134457414613, 'whitespace': 0.6463467358408993, 'stanford': 0.6675313445741461}\n",
      "best <function ark_twokenize at 0x7fad981d1620> list {'ark_twokenize': <function ark_twokenize at 0x7fad981d1620>, 'whitespace': <function whitespace at 0x7fad9e32fa60>, 'stanford': <function stanford at 0x7fad97aec840>} stored [('ark_twokenize', 0.6753134457414613), ('stanford', 0.6675313445741461), ('whitespace', 0.6463467358408993)]\n",
      "yes Mpqa it has continued {'Mpqa': 0.6731517509727627, 'NRC': 0.6493731085170774, 'HuLiu': 0.6761781236489408, 'Mpqa HuLiu': 0.6817985300475573, 'Mpqa HuLiu NRC': 0.6753134457414613}\n",
      "yes NRC it has continued {'Mpqa': 0.6731517509727627, 'NRC': 0.6493731085170774, 'HuLiu': 0.6761781236489408, 'Mpqa HuLiu': 0.6817985300475573, 'Mpqa HuLiu NRC': 0.6753134457414613}\n",
      "yes HuLiu it has continued {'Mpqa': 0.6731517509727627, 'NRC': 0.6493731085170774, 'HuLiu': 0.6761781236489408, 'Mpqa HuLiu': 0.6817985300475573, 'Mpqa HuLiu NRC': 0.6753134457414613}\n",
      "yes Mpqa HuLiu it has continued {'Mpqa': 0.6731517509727627, 'NRC': 0.6493731085170774, 'HuLiu': 0.6761781236489408, 'Mpqa HuLiu': 0.6817985300475573, 'Mpqa HuLiu NRC': 0.6753134457414613}\n",
      "yes Mpqa HuLiu NRC it has continued {'Mpqa': 0.6731517509727627, 'NRC': 0.6493731085170774, 'HuLiu': 0.6761781236489408, 'Mpqa HuLiu': 0.6817985300475573, 'Mpqa HuLiu NRC': 0.6753134457414613}\n",
      "best Mpqa HuLiu list {'Mpqa': <tdparse.lexicons.Mpqa object at 0x7faaf6d26b00>, 'NRC': <tdparse.lexicons.NRC object at 0x7fabcb9d9ef0>, 'HuLiu': <tdparse.lexicons.HuLiu object at 0x7faa517d3400>, 'Mpqa HuLiu': <tdparse.lexicons.Lexicon object at 0x7faa517ec048>, 'Mpqa HuLiu NRC': <tdparse.lexicons.Lexicon object at 0x7fadec17a668>} stored [('Mpqa HuLiu', 0.6817985300475573), ('HuLiu', 0.6761781236489408), ('Mpqa HuLiu NRC', 0.6753134457414613), ('Mpqa', 0.6731517509727627), ('NRC', 0.6493731085170774)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes glove twitter 50d it has continued {'glove twitter 50d': 0.6869865974924341, 'glove wiki giga 50d': 0.6913099870298314, 'glove wiki giga 200d': 0.6964980544747081, 'sswe': 0.6817985300475573, 'glove twitter 200d': 0.7042801556420234, 'w2v sswe': 0.6969303934284479, 'glove 300d 42b common crawl': 0.7107652399481194}\n",
      "yes glove wiki giga 50d it has continued {'glove twitter 50d': 0.6869865974924341, 'glove wiki giga 50d': 0.6913099870298314, 'glove wiki giga 200d': 0.6964980544747081, 'sswe': 0.6817985300475573, 'glove twitter 200d': 0.7042801556420234, 'w2v sswe': 0.6969303934284479, 'glove 300d 42b common crawl': 0.7107652399481194}\n",
      "yes glove wiki giga 200d it has continued {'glove twitter 50d': 0.6869865974924341, 'glove wiki giga 50d': 0.6913099870298314, 'glove wiki giga 200d': 0.6964980544747081, 'sswe': 0.6817985300475573, 'glove twitter 200d': 0.7042801556420234, 'w2v sswe': 0.6969303934284479, 'glove 300d 42b common crawl': 0.7107652399481194}\n",
      "yes sswe it has continued {'glove twitter 50d': 0.6869865974924341, 'glove wiki giga 50d': 0.6913099870298314, 'glove wiki giga 200d': 0.6964980544747081, 'sswe': 0.6817985300475573, 'glove twitter 200d': 0.7042801556420234, 'w2v sswe': 0.6969303934284479, 'glove 300d 42b common crawl': 0.7107652399481194}\n",
      "yes w2v sswe it has continued {'glove twitter 50d': 0.6869865974924341, 'glove wiki giga 50d': 0.6913099870298314, 'glove wiki giga 200d': 0.6964980544747081, 'sswe': 0.6817985300475573, 'glove twitter 200d': 0.7042801556420234, 'w2v sswe': 0.6969303934284479, 'glove 300d 42b common crawl': 0.7107652399481194}\n",
      "best (w2v, sswe) list {'glove twitter 50d': (glove twitter 50d,), 'glove wiki giga 50d': (glove wiki giga 50d,), 'glove wiki giga 200d': (glove wiki giga 200d,), 'sswe': (sswe,), 'w2v sswe': (w2v, sswe)} stored [('glove 300d 42b common crawl', 0.7107652399481194), ('glove twitter 200d', 0.7042801556420234), ('w2v sswe', 0.6969303934284479), ('glove wiki giga 200d', 0.6964980544747081), ('glove wiki giga 50d', 0.6913099870298314), ('glove twitter 50d', 0.6869865974924341), ('sswe', 0.6817985300475573)]\n",
      "[w2v, sswe] 75.62552404403687\n",
      "yes glove twitter 50d it has continued {'glove twitter 50d': 0.6869865974924341, 'glove wiki giga 50d': 0.6913099870298314, 'glove wiki giga 200d': 0.6964980544747081, 'sswe': 0.6817985300475573, 'glove twitter 200d': 0.7042801556420234, 'w2v sswe': 0.6969303934284479, 'glove 300d 42b common crawl': 0.7107652399481194}\n",
      "yes glove wiki giga 50d it has continued {'glove twitter 50d': 0.6869865974924341, 'glove wiki giga 50d': 0.6913099870298314, 'glove wiki giga 200d': 0.6964980544747081, 'sswe': 0.6817985300475573, 'glove twitter 200d': 0.7042801556420234, 'w2v sswe': 0.6969303934284479, 'glove 300d 42b common crawl': 0.7107652399481194}\n",
      "yes glove wiki giga 200d it has continued {'glove twitter 50d': 0.6869865974924341, 'glove wiki giga 50d': 0.6913099870298314, 'glove wiki giga 200d': 0.6964980544747081, 'sswe': 0.6817985300475573, 'glove twitter 200d': 0.7042801556420234, 'w2v sswe': 0.6969303934284479, 'glove 300d 42b common crawl': 0.7107652399481194}\n",
      "yes sswe it has continued {'glove twitter 50d': 0.6869865974924341, 'glove wiki giga 50d': 0.6913099870298314, 'glove wiki giga 200d': 0.6964980544747081, 'sswe': 0.6817985300475573, 'glove twitter 200d': 0.7042801556420234, 'w2v sswe': 0.6969303934284479, 'glove 300d 42b common crawl': 0.7107652399481194}\n",
      "yes w2v sswe it has continued {'glove twitter 50d': 0.6869865974924341, 'glove wiki giga 50d': 0.6913099870298314, 'glove wiki giga 200d': 0.6964980544747081, 'sswe': 0.6817985300475573, 'glove twitter 200d': 0.7042801556420234, 'w2v sswe': 0.6969303934284479, 'glove 300d 42b common crawl': 0.7107652399481194}\n",
      "yes glove twitter 200d it has continued {'glove twitter 50d': 0.6869865974924341, 'glove wiki giga 50d': 0.6913099870298314, 'glove wiki giga 200d': 0.6964980544747081, 'sswe': 0.6817985300475573, 'glove twitter 200d': 0.7042801556420234, 'w2v sswe': 0.6969303934284479, 'glove 300d 42b common crawl': 0.7107652399481194}\n",
      "yes glove 300d 42b common crawl it has continued {'glove twitter 50d': 0.6869865974924341, 'glove wiki giga 50d': 0.6913099870298314, 'glove wiki giga 200d': 0.6964980544747081, 'sswe': 0.6817985300475573, 'glove twitter 200d': 0.7042801556420234, 'w2v sswe': 0.6969303934284479, 'glove 300d 42b common crawl': 0.7107652399481194}\n",
      "best (glove 300d 42b common crawl,) list {'glove twitter 50d': (glove twitter 50d,), 'glove wiki giga 50d': (glove wiki giga 50d,), 'glove wiki giga 200d': (glove wiki giga 200d,), 'sswe': (sswe,), 'w2v sswe': (w2v, sswe), 'glove twitter 200d': (glove twitter 200d,), 'glove 300d 42b common crawl': (glove 300d 42b common crawl,)} stored [('glove 300d 42b common crawl', 0.7107652399481194), ('glove twitter 200d', 0.7042801556420234), ('w2v sswe', 0.6969303934284479), ('glove wiki giga 200d', 0.6964980544747081), ('glove wiki giga 50d', 0.6913099870298314), ('glove twitter 50d', 0.6869865974924341), ('sswe', 0.6817985300475573)]\n",
      "[glove 300d 42b common crawl] 195.78226399421692\n",
      "Model saved to /home/moorea/tdparse/model zoo/Target Dependent Plus SemEval 14 Laptop. Save time 385.57\n",
      "saving raw data\n",
      "Re-writing over previous results\n",
      "Re-writing over previous RAW results\n",
      "Processing dataset SemEval 14 Restaurant\n",
      "Processing model Target Dependent\n",
      "yes ark_twokenize it has continued {'ark_twokenize': 0.6848972792892837, 'whitespace': 0.6651860077734592, 'stanford': 0.6843420322043309}\n",
      "yes whitespace it has continued {'ark_twokenize': 0.6848972792892837, 'whitespace': 0.6651860077734592, 'stanford': 0.6843420322043309}\n",
      "yes stanford it has continued {'ark_twokenize': 0.6848972792892837, 'whitespace': 0.6651860077734592, 'stanford': 0.6843420322043309}\n",
      "best <function ark_twokenize at 0x7fad981d1620> list {'ark_twokenize': <function ark_twokenize at 0x7fad981d1620>, 'whitespace': <function whitespace at 0x7fad9e32fa60>, 'stanford': <function stanford at 0x7fad97aec840>} stored [('ark_twokenize', 0.6848972792892837), ('stanford', 0.6843420322043309), ('whitespace', 0.6651860077734592)]\n",
      "yes glove twitter 50d it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes glove wiki giga 50d it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes glove wiki giga 200d it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes sswe it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes w2v sswe it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "best (w2v, sswe) list {'glove twitter 50d': (glove twitter 50d,), 'glove wiki giga 50d': (glove wiki giga 50d,), 'glove wiki giga 200d': (glove wiki giga 200d,), 'sswe': (sswe,), 'w2v sswe': (w2v, sswe)} stored [('glove 300d 42b common crawl', 0.7134925041643532), ('w2v sswe', 0.6998889505830095), ('glove twitter 200d', 0.6907273736812882), ('glove wiki giga 200d', 0.689894503053859), ('sswe', 0.6848972792892837), ('glove wiki giga 50d', 0.6715713492504164), ('glove twitter 50d', 0.6690727373681288)]\n",
      "[w2v, sswe] 78.74981379508972\n",
      "yes glove twitter 50d it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes glove wiki giga 50d it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes glove wiki giga 200d it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes sswe it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes w2v sswe it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes glove twitter 200d it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes glove 300d 42b common crawl it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "best (glove 300d 42b common crawl,) list {'glove twitter 50d': (glove twitter 50d,), 'glove wiki giga 50d': (glove wiki giga 50d,), 'glove wiki giga 200d': (glove wiki giga 200d,), 'sswe': (sswe,), 'w2v sswe': (w2v, sswe), 'glove twitter 200d': (glove twitter 200d,), 'glove 300d 42b common crawl': (glove 300d 42b common crawl,)} stored [('glove 300d 42b common crawl', 0.7134925041643532), ('w2v sswe', 0.6998889505830095), ('glove twitter 200d', 0.6907273736812882), ('glove wiki giga 200d', 0.689894503053859), ('sswe', 0.6848972792892837), ('glove wiki giga 50d', 0.6715713492504164), ('glove twitter 50d', 0.6690727373681288)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[glove 300d 42b common crawl] 207.69602465629578\n",
      "Model saved to /home/moorea/tdparse/model zoo/Target Dependent SemEval 14 Restaurant. Save time 376.72\n",
      "saving raw data\n",
      "Re-writing over previous results\n",
      "Re-writing over previous RAW results\n",
      "Processing model Target Dependent Plus\n",
      "yes ark_twokenize it has continued {'ark_twokenize': 0.7093281510272071, 'whitespace': 0.6901721265963354, 'stanford': 0.7007218212104387}\n",
      "yes whitespace it has continued {'ark_twokenize': 0.7093281510272071, 'whitespace': 0.6901721265963354, 'stanford': 0.7007218212104387}\n",
      "yes stanford it has continued {'ark_twokenize': 0.7093281510272071, 'whitespace': 0.6901721265963354, 'stanford': 0.7007218212104387}\n",
      "best <function ark_twokenize at 0x7fad981d1620> list {'ark_twokenize': <function ark_twokenize at 0x7fad981d1620>, 'whitespace': <function whitespace at 0x7fad9e32fa60>, 'stanford': <function stanford at 0x7fad97aec840>} stored [('ark_twokenize', 0.7093281510272071), ('stanford', 0.7007218212104387), ('whitespace', 0.6901721265963354)]\n",
      "yes Mpqa it has continued {'Mpqa': 0.7090505274847307, 'NRC': 0.6923931149361466, 'HuLiu': 0.7090505274847307, 'Mpqa HuLiu': 0.7082176568573015, 'Mpqa HuLiu NRC': 0.7093281510272071}\n",
      "yes NRC it has continued {'Mpqa': 0.7090505274847307, 'NRC': 0.6923931149361466, 'HuLiu': 0.7090505274847307, 'Mpqa HuLiu': 0.7082176568573015, 'Mpqa HuLiu NRC': 0.7093281510272071}\n",
      "yes HuLiu it has continued {'Mpqa': 0.7090505274847307, 'NRC': 0.6923931149361466, 'HuLiu': 0.7090505274847307, 'Mpqa HuLiu': 0.7082176568573015, 'Mpqa HuLiu NRC': 0.7093281510272071}\n",
      "yes Mpqa HuLiu it has continued {'Mpqa': 0.7090505274847307, 'NRC': 0.6923931149361466, 'HuLiu': 0.7090505274847307, 'Mpqa HuLiu': 0.7082176568573015, 'Mpqa HuLiu NRC': 0.7093281510272071}\n",
      "yes Mpqa HuLiu NRC it has continued {'Mpqa': 0.7090505274847307, 'NRC': 0.6923931149361466, 'HuLiu': 0.7090505274847307, 'Mpqa HuLiu': 0.7082176568573015, 'Mpqa HuLiu NRC': 0.7093281510272071}\n",
      "best Mpqa HuLiu NRC list {'Mpqa': <tdparse.lexicons.Mpqa object at 0x7faaf6d26b00>, 'NRC': <tdparse.lexicons.NRC object at 0x7fabcb9d9ef0>, 'HuLiu': <tdparse.lexicons.HuLiu object at 0x7faa517d3400>, 'Mpqa HuLiu': <tdparse.lexicons.Lexicon object at 0x7faa517ec048>, 'Mpqa HuLiu NRC': <tdparse.lexicons.Lexicon object at 0x7fadec17a668>} stored [('Mpqa HuLiu NRC', 0.7093281510272071), ('Mpqa', 0.7090505274847307), ('HuLiu', 0.7090505274847307), ('Mpqa HuLiu', 0.7082176568573015), ('NRC', 0.6923931149361466)]\n",
      "yes glove twitter 50d it has continued {'glove twitter 50d': 0.6962798445308163, 'glove wiki giga 50d': 0.6851749028317601, 'glove wiki giga 200d': 0.6954469739033871, 'sswe': 0.7093281510272071, 'glove twitter 200d': 0.7001665741254859, 'w2v sswe': 0.7079400333148251, 'glove 300d 42b common crawl': 0.7084952803997779}\n",
      "yes glove wiki giga 50d it has continued {'glove twitter 50d': 0.6962798445308163, 'glove wiki giga 50d': 0.6851749028317601, 'glove wiki giga 200d': 0.6954469739033871, 'sswe': 0.7093281510272071, 'glove twitter 200d': 0.7001665741254859, 'w2v sswe': 0.7079400333148251, 'glove 300d 42b common crawl': 0.7084952803997779}\n",
      "yes glove wiki giga 200d it has continued {'glove twitter 50d': 0.6962798445308163, 'glove wiki giga 50d': 0.6851749028317601, 'glove wiki giga 200d': 0.6954469739033871, 'sswe': 0.7093281510272071, 'glove twitter 200d': 0.7001665741254859, 'w2v sswe': 0.7079400333148251, 'glove 300d 42b common crawl': 0.7084952803997779}\n",
      "yes sswe it has continued {'glove twitter 50d': 0.6962798445308163, 'glove wiki giga 50d': 0.6851749028317601, 'glove wiki giga 200d': 0.6954469739033871, 'sswe': 0.7093281510272071, 'glove twitter 200d': 0.7001665741254859, 'w2v sswe': 0.7079400333148251, 'glove 300d 42b common crawl': 0.7084952803997779}\n",
      "yes w2v sswe it has continued {'glove twitter 50d': 0.6962798445308163, 'glove wiki giga 50d': 0.6851749028317601, 'glove wiki giga 200d': 0.6954469739033871, 'sswe': 0.7093281510272071, 'glove twitter 200d': 0.7001665741254859, 'w2v sswe': 0.7079400333148251, 'glove 300d 42b common crawl': 0.7084952803997779}\n",
      "best (sswe,) list {'glove twitter 50d': (glove twitter 50d,), 'glove wiki giga 50d': (glove wiki giga 50d,), 'glove wiki giga 200d': (glove wiki giga 200d,), 'sswe': (sswe,), 'w2v sswe': (w2v, sswe)} stored [('sswe', 0.7093281510272071), ('glove 300d 42b common crawl', 0.7084952803997779), ('w2v sswe', 0.7079400333148251), ('glove twitter 200d', 0.7001665741254859), ('glove twitter 50d', 0.6962798445308163), ('glove wiki giga 200d', 0.6954469739033871), ('glove wiki giga 50d', 0.6851749028317601)]\n",
      "[sswe] 77.63635897636414\n",
      "yes glove twitter 50d it has continued {'glove twitter 50d': 0.6962798445308163, 'glove wiki giga 50d': 0.6851749028317601, 'glove wiki giga 200d': 0.6954469739033871, 'sswe': 0.7093281510272071, 'glove twitter 200d': 0.7001665741254859, 'w2v sswe': 0.7079400333148251, 'glove 300d 42b common crawl': 0.7084952803997779}\n",
      "yes glove wiki giga 50d it has continued {'glove twitter 50d': 0.6962798445308163, 'glove wiki giga 50d': 0.6851749028317601, 'glove wiki giga 200d': 0.6954469739033871, 'sswe': 0.7093281510272071, 'glove twitter 200d': 0.7001665741254859, 'w2v sswe': 0.7079400333148251, 'glove 300d 42b common crawl': 0.7084952803997779}\n",
      "yes glove wiki giga 200d it has continued {'glove twitter 50d': 0.6962798445308163, 'glove wiki giga 50d': 0.6851749028317601, 'glove wiki giga 200d': 0.6954469739033871, 'sswe': 0.7093281510272071, 'glove twitter 200d': 0.7001665741254859, 'w2v sswe': 0.7079400333148251, 'glove 300d 42b common crawl': 0.7084952803997779}\n",
      "yes sswe it has continued {'glove twitter 50d': 0.6962798445308163, 'glove wiki giga 50d': 0.6851749028317601, 'glove wiki giga 200d': 0.6954469739033871, 'sswe': 0.7093281510272071, 'glove twitter 200d': 0.7001665741254859, 'w2v sswe': 0.7079400333148251, 'glove 300d 42b common crawl': 0.7084952803997779}\n",
      "yes w2v sswe it has continued {'glove twitter 50d': 0.6962798445308163, 'glove wiki giga 50d': 0.6851749028317601, 'glove wiki giga 200d': 0.6954469739033871, 'sswe': 0.7093281510272071, 'glove twitter 200d': 0.7001665741254859, 'w2v sswe': 0.7079400333148251, 'glove 300d 42b common crawl': 0.7084952803997779}\n",
      "yes glove twitter 200d it has continued {'glove twitter 50d': 0.6962798445308163, 'glove wiki giga 50d': 0.6851749028317601, 'glove wiki giga 200d': 0.6954469739033871, 'sswe': 0.7093281510272071, 'glove twitter 200d': 0.7001665741254859, 'w2v sswe': 0.7079400333148251, 'glove 300d 42b common crawl': 0.7084952803997779}\n",
      "yes glove 300d 42b common crawl it has continued {'glove twitter 50d': 0.6962798445308163, 'glove wiki giga 50d': 0.6851749028317601, 'glove wiki giga 200d': 0.6954469739033871, 'sswe': 0.7093281510272071, 'glove twitter 200d': 0.7001665741254859, 'w2v sswe': 0.7079400333148251, 'glove 300d 42b common crawl': 0.7084952803997779}\n",
      "best (sswe,) list {'glove twitter 50d': (glove twitter 50d,), 'glove wiki giga 50d': (glove wiki giga 50d,), 'glove wiki giga 200d': (glove wiki giga 200d,), 'sswe': (sswe,), 'w2v sswe': (w2v, sswe), 'glove twitter 200d': (glove twitter 200d,), 'glove 300d 42b common crawl': (glove 300d 42b common crawl,)} stored [('sswe', 0.7093281510272071), ('glove 300d 42b common crawl', 0.7084952803997779), ('w2v sswe', 0.7079400333148251), ('glove twitter 200d', 0.7001665741254859), ('glove twitter 50d', 0.6962798445308163), ('glove wiki giga 200d', 0.6954469739033871), ('glove wiki giga 50d', 0.6851749028317601)]\n",
      "[sswe] 205.9449999332428\n",
      "Model saved to /home/moorea/tdparse/model zoo/Target Dependent Plus SemEval 14 Restaurant. Save time 31.3\n",
      "saving raw data\n",
      "Re-writing over previous results\n",
      "Re-writing over previous RAW results\n",
      "Processing dataset SemEval 16 Restaurant 14 Train\n",
      "Processing model Target Dependent\n",
      "yes ark_twokenize it has continued {'ark_twokenize': 0.6848972792892837, 'whitespace': 0.6651860077734592, 'stanford': 0.6843420322043309}\n",
      "yes whitespace it has continued {'ark_twokenize': 0.6848972792892837, 'whitespace': 0.6651860077734592, 'stanford': 0.6843420322043309}\n",
      "yes stanford it has continued {'ark_twokenize': 0.6848972792892837, 'whitespace': 0.6651860077734592, 'stanford': 0.6843420322043309}\n",
      "best <function ark_twokenize at 0x7fad981d1620> list {'ark_twokenize': <function ark_twokenize at 0x7fad981d1620>, 'whitespace': <function whitespace at 0x7fad9e32fa60>, 'stanford': <function stanford at 0x7fad97aec840>} stored [('ark_twokenize', 0.6848972792892837), ('stanford', 0.6843420322043309), ('whitespace', 0.6651860077734592)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes glove twitter 50d it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes glove wiki giga 50d it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes glove wiki giga 200d it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes sswe it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes w2v sswe it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "best (w2v, sswe) list {'glove twitter 50d': (glove twitter 50d,), 'glove wiki giga 50d': (glove wiki giga 50d,), 'glove wiki giga 200d': (glove wiki giga 200d,), 'sswe': (sswe,), 'w2v sswe': (w2v, sswe)} stored [('glove 300d 42b common crawl', 0.7134925041643532), ('w2v sswe', 0.6998889505830095), ('glove twitter 200d', 0.6907273736812882), ('glove wiki giga 200d', 0.689894503053859), ('sswe', 0.6848972792892837), ('glove wiki giga 50d', 0.6715713492504164), ('glove twitter 50d', 0.6690727373681288)]\n",
      "[w2v, sswe] 77.21093320846558\n",
      "yes glove twitter 50d it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes glove wiki giga 50d it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes glove wiki giga 200d it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes sswe it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes w2v sswe it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes glove twitter 200d it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "yes glove 300d 42b common crawl it has continued {'glove twitter 50d': 0.6690727373681288, 'glove wiki giga 50d': 0.6715713492504164, 'glove wiki giga 200d': 0.689894503053859, 'sswe': 0.6848972792892837, 'glove twitter 200d': 0.6907273736812882, 'w2v sswe': 0.6998889505830095, 'glove 300d 42b common crawl': 0.7134925041643532}\n",
      "best (glove 300d 42b common crawl,) list {'glove twitter 50d': (glove twitter 50d,), 'glove wiki giga 50d': (glove wiki giga 50d,), 'glove wiki giga 200d': (glove wiki giga 200d,), 'sswe': (sswe,), 'w2v sswe': (w2v, sswe), 'glove twitter 200d': (glove twitter 200d,), 'glove 300d 42b common crawl': (glove 300d 42b common crawl,)} stored [('glove 300d 42b common crawl', 0.7134925041643532), ('w2v sswe', 0.6998889505830095), ('glove twitter 200d', 0.6907273736812882), ('glove wiki giga 200d', 0.689894503053859), ('sswe', 0.6848972792892837), ('glove wiki giga 50d', 0.6715713492504164), ('glove twitter 50d', 0.6690727373681288)]\n",
      "[glove 300d 42b common crawl] 194.05587363243103\n",
      "Model saved to /home/moorea/tdparse/model zoo/Target Dependent SemEval 16 Restaurant 14 Train. Save time 380.32\n",
      "saving raw data\n",
      "Re-writing over previous results\n",
      "Re-writing over previous RAW results\n",
      "Processing model Target Dependent Plus\n",
      "yes ark_twokenize it has continued {'ark_twokenize': 0.7093281510272071, 'whitespace': 0.6901721265963354, 'stanford': 0.7007218212104387}\n",
      "yes whitespace it has continued {'ark_twokenize': 0.7093281510272071, 'whitespace': 0.6901721265963354, 'stanford': 0.7007218212104387}\n",
      "yes stanford it has continued {'ark_twokenize': 0.7093281510272071, 'whitespace': 0.6901721265963354, 'stanford': 0.7007218212104387}\n",
      "best <function ark_twokenize at 0x7fad981d1620> list {'ark_twokenize': <function ark_twokenize at 0x7fad981d1620>, 'whitespace': <function whitespace at 0x7fad9e32fa60>, 'stanford': <function stanford at 0x7fad97aec840>} stored [('ark_twokenize', 0.7093281510272071), ('stanford', 0.7007218212104387), ('whitespace', 0.6901721265963354)]\n",
      "yes Mpqa it has continued {'Mpqa': 0.7090505274847307, 'NRC': 0.6923931149361466, 'HuLiu': 0.7090505274847307, 'Mpqa HuLiu': 0.7082176568573015, 'Mpqa HuLiu NRC': 0.7093281510272071}\n",
      "yes NRC it has continued {'Mpqa': 0.7090505274847307, 'NRC': 0.6923931149361466, 'HuLiu': 0.7090505274847307, 'Mpqa HuLiu': 0.7082176568573015, 'Mpqa HuLiu NRC': 0.7093281510272071}\n",
      "yes HuLiu it has continued {'Mpqa': 0.7090505274847307, 'NRC': 0.6923931149361466, 'HuLiu': 0.7090505274847307, 'Mpqa HuLiu': 0.7082176568573015, 'Mpqa HuLiu NRC': 0.7093281510272071}\n",
      "yes Mpqa HuLiu it has continued {'Mpqa': 0.7090505274847307, 'NRC': 0.6923931149361466, 'HuLiu': 0.7090505274847307, 'Mpqa HuLiu': 0.7082176568573015, 'Mpqa HuLiu NRC': 0.7093281510272071}\n",
      "yes Mpqa HuLiu NRC it has continued {'Mpqa': 0.7090505274847307, 'NRC': 0.6923931149361466, 'HuLiu': 0.7090505274847307, 'Mpqa HuLiu': 0.7082176568573015, 'Mpqa HuLiu NRC': 0.7093281510272071}\n",
      "best Mpqa HuLiu NRC list {'Mpqa': <tdparse.lexicons.Mpqa object at 0x7faaf6d26b00>, 'NRC': <tdparse.lexicons.NRC object at 0x7fabcb9d9ef0>, 'HuLiu': <tdparse.lexicons.HuLiu object at 0x7faa517d3400>, 'Mpqa HuLiu': <tdparse.lexicons.Lexicon object at 0x7faa517ec048>, 'Mpqa HuLiu NRC': <tdparse.lexicons.Lexicon object at 0x7fadec17a668>} stored [('Mpqa HuLiu NRC', 0.7093281510272071), ('Mpqa', 0.7090505274847307), ('HuLiu', 0.7090505274847307), ('Mpqa HuLiu', 0.7082176568573015), ('NRC', 0.6923931149361466)]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time_to_process = time.time()\n",
    "for dataset_name, train_test in dataset_train_test.items():\n",
    "    print('Processing dataset {}'.format(dataset_name))\n",
    "    train, test = train_test\n",
    "    for model, parameter_file_paths in model_files.items():\n",
    "        print('Processing model {}'.format(model))\n",
    "        params_files = parameter_file_paths\n",
    "        parameters = params_files[0]\n",
    "        result_file_path = params_files[1]\n",
    "        c_fp = params_files[2]\n",
    "        tokeniser_fp = params_files[3]\n",
    "        word_vectors_fp = params_files[4]\n",
    "        senti_lexicons_fp = params_files[5]\n",
    "        model_dir = params_files[6]\n",
    "        dataset_predictions(train, test, dataset_name, model, \n",
    "                            result_file_path=result_file_path,\n",
    "                            re_write=True, save_raw_data=True,\n",
    "                            c_file_path=c_fp,\n",
    "                            tokeniser_file_path=tokeniser_fp,\n",
    "                            word_vector_file_path=word_vectors_fp, \n",
    "                            senti_lexicon_file_path=senti_lexicons_fp,\n",
    "                            model_dir=model_dir,\n",
    "                            **parameters)\n",
    "\n",
    "time_to_process = time.time() - time_to_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time it took to process all the datasets {{round(time_to_process / 3600, 2)}} hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The affect of the C value\n",
    "\n",
    "We take a look at the affect of tunning for the C-Value. We tune the C-Value in two steps:\n",
    "1. Coarse grain search over the following values: from 0.00001 to 10 going up by a factor of 10. Once the best best coarse grain value is found we fine tune it.\n",
    "2. We multiple the best coarse grain value by the following value and search over these values: 0.35, 0.7, 1, 3.5, and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values_target_dep_plus = C_result_files[1]\n",
    "c_values_target_dep_plus = get_json_data(c_values_target_dep_plus, 'Dong Twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stats = defaultdict(lambda: dict())\n",
    "\n",
    "mean_error = lambda acc_values: sum(max(acc_values) - acc_values) / (len(acc_values) - 1)\n",
    "for dataset_name in dataset_train_test:\n",
    "    # Target Dependent Plus using the Hu and Liu Lexicon C file\n",
    "    c_values_target_dep_plus = C_result_files[1]\n",
    "    c_values_target_dep_plus = get_json_data(c_values_target_dep_plus, dataset_name)\n",
    "    model_results_file = model_result_files[1]\n",
    "    fine_c_values, best_coarse_c_value = fine_tune_values(c_values_target_dep_plus)\n",
    "    coarse_c_values = coarse_tune_values(c_values_target_dep_plus)\n",
    "    all_c_values = {float(c_value) : acc for c_value, acc in c_values_target_dep_plus.items()}\n",
    "    fine_acc = np.array([all_c_values[c_value] for c_value in fine_c_values])\n",
    "    coarse_acc = np.array([all_c_values[c_value] for c_value in coarse_c_values])\n",
    "    all_acc = np.array(list(c_values_target_dep_plus.values()))\n",
    "    best_c = best_c_value(all_c_values)\n",
    "    test_accuracy = get_results(model_results_file, dataset_name)['Accuracy']\n",
    "    dataset_stats[dataset_name]['Fine Tune MAE (Accuracy %)'] = mean_error(fine_acc) * 100\n",
    "    dataset_stats[dataset_name]['Coarse Tune MAE (Accuracy %)'] = mean_error(coarse_acc) * 100\n",
    "    dataset_stats[dataset_name]['Best Fine C value'] = best_c\n",
    "    dataset_stats[dataset_name]['Best Coarse C value'] = best_coarse_c_value\n",
    "    dataset_stats[dataset_name]['Test score Accuracy (%)'] = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = list(dataset_train_test)\n",
    "columns = ['Fine Tune MAE (Accuracy %)', 'Coarse Tune MAE (Accuracy %)', 'Best Fine C value', \n",
    "           'Best Coarse C value', 'Dataset', 'Test score Accuracy (%)']\n",
    "c_value_stats = pd.DataFrame(np.zeros((len(dataset_train_test), 6)), columns=columns)\n",
    "c_value_stats['Dataset'] = index\n",
    "c_value_stats = c_value_stats.set_index('Dataset')\n",
    "\n",
    "# Add the data to the DataFrame\n",
    "for dataset_name, col_value in dataset_stats.items():\n",
    "    for column, value in col_value.items():\n",
    "        c_value_stats[column][dataset_name] = value\n",
    "c_value_stats = c_value_stats.round({'Fine Tune Accuracy std (%)' : 2, \n",
    "                                     'Coarse Tune Accuracy std (%)' : 2})\n",
    "c_value_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
