{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Helper functions\n",
    "from bella.notebook_helper import write_json_data\n",
    "# Models\n",
    "from bella.models.tdparse import TDParse, TDParsePlus\n",
    "# Word Vector methods\n",
    "from bella.word_vectors import GloveCommonCrawl, SSWE\n",
    "# Dependency Parser\n",
    "from bella.dependency_parsers import tweebo\n",
    "from bella import tokenisers\n",
    "# Sentiment lexicons\n",
    "from bella import lexicons\n",
    "# Get the data\n",
    "from bella.parsers import semeval_14, dong, election\n",
    "from bella.data_types import TargetCollection\n",
    "from bella.helper import read_config\n",
    "# Evaluation methods\n",
    "from bella.evaluation import evaluation_results, get_results, get_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#  ADD YOUR CONFIG FILE PATH HERE \n",
    "##\n",
    "CONFIG_FP = Path('..', 'config.yaml')\n",
    "\n",
    "# Load all of the datasets\n",
    "youtubean_train = semeval_14(read_config('youtubean_train', CONFIG_FP))\n",
    "youtubean_test = semeval_14(read_config('youtubean_test', CONFIG_FP))\n",
    "semeval_14_rest_train = semeval_14(read_config('semeval_2014_rest_train', CONFIG_FP))\n",
    "semeval_14_lap_train = semeval_14(read_config('semeval_2014_lap_train', CONFIG_FP))\n",
    "semeval_14_rest_test = semeval_14(read_config('semeval_2014_rest_test', CONFIG_FP))\n",
    "semeval_14_lap_test = semeval_14(read_config('semeval_2014_lap_test', CONFIG_FP))\n",
    "dong_train = dong(read_config('dong_twit_train_data', CONFIG_FP))\n",
    "dong_test = dong(read_config('dong_twit_test_data', CONFIG_FP))\n",
    "election_train, election_test = election(read_config('election_folder_dir', CONFIG_FP))\n",
    "mitchel_train = semeval_14(read_config('mitchel_train', CONFIG_FP))\n",
    "mitchel_test = semeval_14(read_config('mitchel_test', CONFIG_FP))\n",
    "\n",
    "\n",
    "dataset_train_test = {#'SemEval 14 Laptop' : (semeval_14_lap_train, semeval_14_lap_test)}#,\n",
    "                      'SemEval 14 Restaurant' : (semeval_14_rest_train, semeval_14_rest_test)}#,\n",
    "                      #'Dong Twitter' : (dong_train, dong_test),\n",
    "                      #'Election Twitter' : (election_train, election_test),\n",
    "                      #'YouTuBean' : (youtubean_train, youtubean_test),\n",
    "                      #'Mitchel' : (mitchel_train, mitchel_test)\n",
    "                     #}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentiment lexicons and remove all words that are not associated\n",
    "# to the Positive or Negative class.\n",
    "\n",
    "lexicon_data = read_config('lexicons', CONFIG_FP)\n",
    "hu_liu_fp = Path(lexicon_data['hu_liu'])\n",
    "mpqa_fp = Path(lexicon_data['mpqa'])\n",
    "nrc_fp = Path(lexicon_data['nrc_emotion'])\n",
    "\n",
    "subset_cats = {'positive', 'negative'}\n",
    "\n",
    "# Load the sentiment lexicons but lower case all the words\n",
    "mpqa_low = lexicons.Mpqa(mpqa_fp, subset_cats=subset_cats, lower=True)\n",
    "nrc_low = lexicons.NRC(nrc_fp, subset_cats=subset_cats, lower=True)\n",
    "hu_liu_low = lexicons.HuLiu(hu_liu_fp, subset_cats=subset_cats, lower=True)\n",
    "mpqa_huliu_low = lexicons.Lexicon.combine_lexicons(mpqa_low, hu_liu_low)\n",
    "all_three_low = lexicons.Lexicon.combine_lexicons(mpqa_huliu_low, nrc_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_predictions(train, test, dataset_name, model, random_state,  \n",
    "                        c_file_path, word_vector_file_path, model_dir,\n",
    "                        sentiment_lexicon=None, result_file_path=None,\n",
    "                        re_write=True, save_raw_data=True):\n",
    "    if not re_write and result_file_path is not None:\n",
    "        results_df = get_results(result_file_path, dataset_name)\n",
    "        if save_raw_data and results_df is not None:\n",
    "            if get_raw_data(result_file_path, dataset_name, test):\n",
    "                return results_df\n",
    "        elif results_df is not None:\n",
    "            return results_df\n",
    "    # loading the data\n",
    "    data_train = train.data()\n",
    "    y_train = train.sentiment_data()\n",
    "    data_test = test.data()\n",
    "    y_test = test.sentiment_data()\n",
    "    # Reduce the word vector size by filtering by the word in the train\n",
    "    # and test dataset. Which is still as fair in practice as loading the whole\n",
    "    # word vectors\n",
    "\n",
    "    train_words = train.word_list(tokenisers.ark_twokenize)\n",
    "    test_words = test.word_list(tokenisers.ark_twokenize)\n",
    "    all_words = list(set(train_words + test_words))\n",
    "    glove_300 = GloveCommonCrawl(version=42, filter_words=all_words)\n",
    "    sswe = SSWE(filter_words=all_words)\n",
    "    \n",
    "    \n",
    "    # Finding the best C value for the model on this dataset\n",
    "    c_grid_params = {'word_vectors' : [[sswe]], 'random_state' : random_state,\n",
    "                     'parsers' : [tweebo], 'tokenisers' : [tokenisers.ark_twokenize]}\n",
    "    if sentiment_lexicon is not None:\n",
    "        c_grid_params['senti_lexicons'] = [sentiment_lexicon]\n",
    "    best_c, c_scores = model.find_best_c(data_train, y_train, c_grid_params, \n",
    "                                         save_file=c_file_path, dataset_name=dataset_name, \n",
    "                                         re_write=False, n_jobs=7, cv=5)\n",
    "    # Search over the different word vectors given the best tokeniser\n",
    "    # and sentiment lexicon\n",
    "    word_vectors = [[sswe], [glove_300]]\n",
    "    word_vector_grid_params = {**c_grid_params}\n",
    "    word_vector_grid_params['C'] = [best_c]\n",
    "    word_vector_grid_params['word_vectors'] = word_vectors\n",
    "    import time\n",
    "    t = time.time()\n",
    "    best_word_vector = model.save_grid_search(data_train, y_train, word_vector_grid_params, \n",
    "                                              'word_vectors', dataset_name, word_vector_file_path, \n",
    "                                              re_write=False, n_jobs=7, cv=5)\n",
    "    print('{} {}'.format(best_word_vector, time.time() - t))\n",
    "    parameters = {'word_vector' : best_word_vector, 'random_state' : random_state, \n",
    "                  'C' : best_c, 'tokeniser' : tokenisers.ark_twokenize, 'parser' : tweebo}\n",
    "    print('Best parameters for dataset {} are: {}'.format(dataset_name, parameters))\n",
    "    if sentiment_lexicon is not None:\n",
    "        parameters['senti_lexicon'] = sentiment_lexicon\n",
    "    best_params = model.get_params(**parameters)\n",
    "    model.fit(data_train, y_train, params=best_params)\n",
    "    predicted_values = model.predict(data_test)\n",
    "    # Save the model to the model zoo\n",
    "    model_file_name = '{} {}'.format(model, dataset_name)\n",
    "    model_file_path = os.path.join(model_dir, model_file_name)\n",
    "    model.save_model(model_file_path, verbose=1)\n",
    "    # Return the results\n",
    "    if result_file_path is not None:\n",
    "        return evaluation_results(predicted_values, test, dataset_name, \n",
    "                                  file_name=result_file_path, \n",
    "                                  save_raw_data=save_raw_data, re_write=True)\n",
    "    else:\n",
    "        return evaluation_results(predicted_values, test, dataset_name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances of the models\n",
    "tdparse = TDParse()\n",
    "tdparse_plus = TDParsePlus()\n",
    "models = [tdparse, tdparse_plus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{TDParse: ({'random_state': 42},\n",
       "  '/mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/results alt/TDParse/TDParse.tsv',\n",
       "  '/mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/results alt/TDParse/TDParse C.json',\n",
       "  '/mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/results alt/TDParse/TDParse word vector.json',\n",
       "  '/mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/model zoo alt'),\n",
       " TDParse Plus: ({'random_state': 42,\n",
       "   'sentiment_lexicon': <bella.lexicons.Lexicon at 0x7ff7a75eb0b8>},\n",
       "  '/mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/results alt/TDParse/TDParsePlus.tsv',\n",
       "  '/mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/results alt/TDParse/TDParsePlus C.json',\n",
       "  '/mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/results alt/TDParse/TDParsePlus word vector.json',\n",
       "  '/mnt/silo/andrew/Sentiment_Models/EMNLP/Bella/model zoo alt')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the result files\n",
    "result_folder = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results alt', 'TDParse'))\n",
    "model_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'model zoo alt'))\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_result_files = ['TDParse.tsv', 'TDParsePlus.tsv']\n",
    "model_result_files = [os.path.join(result_folder, result_file) for result_file in model_result_files]\n",
    "C_result_files = ['TDParse C.json', 'TDParsePlus C.json']\n",
    "C_result_files = [os.path.join(result_folder, result_file) for result_file in C_result_files]\n",
    "word_vector_result_files = ['TDParse word vector.json', 'TDParsePlus word vector.json']\n",
    "word_vector_result_files = [os.path.join(result_folder, result_file) for result_file in word_vector_result_files]\n",
    "# Parameters for each model\n",
    "std_model_parameters = {'random_state' : 42}\n",
    "all_senti_model_parameters = {**std_model_parameters, 'sentiment_lexicon' : all_three_low}\n",
    "model_parameters = [std_model_parameters, all_senti_model_parameters]\n",
    "# Combining parameters and result files\n",
    "parameters_files = list(zip(model_parameters, model_result_files, C_result_files, \n",
    "                            word_vector_result_files, [model_dir]*2))\n",
    "\n",
    "model_files = dict(zip(models, parameters_files))\n",
    "model_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset SemEval 14 Restaurant\n",
      "Processing model TDParse\n",
      "Loading glove 300d 42b common crawl from file\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time_to_process = time.time()\n",
    "for dataset_name, train_test in dataset_train_test.items():\n",
    "    print('Processing dataset {}'.format(dataset_name))\n",
    "    train, test = train_test\n",
    "    for model, parameter_file_paths in model_files.items():\n",
    "        print('Processing model {}'.format(model))\n",
    "        params_files = parameter_file_paths\n",
    "        parameters = params_files[0]\n",
    "        result_file_path = params_files[1]\n",
    "        c_fp = params_files[2]\n",
    "        word_vectors_fp = params_files[3]\n",
    "        model_dir = params_files[4]\n",
    "        dataset_predictions(train, test, dataset_name, model, \n",
    "                            result_file_path=result_file_path,\n",
    "                            re_write=True, save_raw_data=True,\n",
    "                            c_file_path=c_fp,\n",
    "                            word_vector_file_path=word_vectors_fp, \n",
    "                            model_dir=model_dir,\n",
    "                            **parameters)\n",
    "\n",
    "time_to_process = time.time() - time_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
