{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Helper functions\n",
    "from tdparse.notebook_helper import write_json_data\n",
    "# Models\n",
    "from tdparse.models.tdparse import TDParse, TDParsePlus\n",
    "# Word Vector methods\n",
    "from tdparse.word_vectors import GensimVectors\n",
    "from tdparse.word_vectors import PreTrained\n",
    "# Dependency Parser\n",
    "from tdparse.dependency_parsers import tweebo\n",
    "# Sentiment lexicons\n",
    "from tdparse import lexicons\n",
    "# Get the data\n",
    "from tdparse.parsers import semeval_14, semeval_15_16, dong, election\n",
    "from tdparse.data_types import TargetCollection\n",
    "from tdparse.helper import read_config, full_path\n",
    "# Evaluation methods\n",
    "from tdparse.evaluation import evaluation_results, scores, get_results, \\\n",
    "                               save_results, combine_results, get_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of the datasets\n",
    "youtubean_train = semeval_14(full_path(read_config('youtubean_train')))\n",
    "youtubean_test = semeval_14(full_path(read_config('youtubean_test')))\n",
    "semeval_14_rest_train = semeval_14(full_path(read_config('semeval_2014_rest_train')))\n",
    "semeval_14_lap_train = semeval_14(full_path(read_config('semeval_2014_lap_train')))\n",
    "semeval_14_rest_test = semeval_14(full_path(read_config('semeval_2014_rest_test')))\n",
    "semeval_14_lap_test = semeval_14(full_path(read_config('semeval_2014_lap_test')))\n",
    "semeval_15_rest_test = semeval_15_16(full_path(read_config('semeval_2015_rest_test')))\n",
    "semeval_16_rest_test = semeval_15_16(full_path(read_config('semeval_2016_rest_test')),\n",
    "                                     sep_16_from_15=True)\n",
    "dong_train = dong(full_path(read_config('dong_twit_train_data')))\n",
    "dong_test = dong(full_path(read_config('dong_twit_test_data')))\n",
    "election_train, election_test = election(full_path(read_config('election_folder_dir')))\n",
    "product_reviews_train = semeval_14(full_path(read_config('product_train')))\n",
    "product_reviews_test = semeval_14(full_path(read_config('product_test')))\n",
    "# Combine semeval 14 resturant train and test\n",
    "semeval_14_rest_all = TargetCollection.combine_collections(semeval_14_rest_train,\n",
    "                                                           semeval_14_rest_test)\n",
    "# Combine semeval 14 resturant all with 15 test\n",
    "semeval_14_15 = TargetCollection.combine_collections(semeval_14_rest_all,\n",
    "                                                     semeval_15_rest_test)\n",
    "\n",
    "train_test = {'SemEval 14 Laptop' : (semeval_14_lap_train, semeval_14_lap_test),\n",
    "              'SemEval 14 Restaurant' : (semeval_14_rest_train, semeval_14_rest_test),\n",
    "              'SemEval 16 Restaurant 14 Train' : (semeval_14_rest_train, semeval_16_rest_test),\n",
    "              'SemEval 16 Restaurant 14 All' : (semeval_14_rest_all, semeval_16_rest_test),\n",
    "              'SemEval 16 Restaurant 15&14' : (semeval_14_15, semeval_16_rest_test),\n",
    "              'Dong Twitter' : (dong_train, dong_test),\n",
    "              'Election Twitter' : (election_train, election_test),\n",
    "              'Product Reviews' : (product_reviews_train, product_reviews_test),\n",
    "              'YouTuBean' : (youtubean_train, youtubean_test)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word vectors\n",
    "w2v_path = full_path(read_config('word2vec_files')['vo_zhang'])\n",
    "w2v = GensimVectors(w2v_path, None, model='word2vec', name='w2v')\n",
    "sswe_path = full_path(read_config('sswe_files')['vo_zhang'])\n",
    "sswe = PreTrained(sswe_path, name='sswe')\n",
    "\n",
    "# Load the sentiment lexicons and remove all words that are not associated\n",
    "# to the Positive or Negative class.\n",
    "subset_cats = {'positive', 'negative'}\n",
    "mpqa_low = lexicons.Mpqa(subset_cats=subset_cats, lower=True)\n",
    "nrc_low = lexicons.NRC(subset_cats=subset_cats, lower=True)\n",
    "hu_liu_low = lexicons.HuLiu(subset_cats=subset_cats, lower=True)\n",
    "mpqa_huliu_low = lexicons.Lexicon.combine_lexicons(mpqa_low, hu_liu_low)\n",
    "all_three_low = lexicons.Lexicon.combine_lexicons(mpqa_huliu_low, nrc_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_predictions(train, test, name, model, word_vector, random_state, \n",
    "                        sentiment_lexicon=None, result_file_path=None, \n",
    "                        c_file_path=None, re_write=True, save_raw_data=True):\n",
    "    if not re_write and result_file_path is not None:\n",
    "        results_df = get_results(result_file_path, name)\n",
    "        if save_raw_data and results_df is not None:\n",
    "            if get_raw_data(result_file_path, name, test):\n",
    "                return results_df\n",
    "        elif results_df is not None:\n",
    "            return results_df\n",
    "    # loading the data\n",
    "    data_train = train.data()\n",
    "    y_train = train.sentiment_data()\n",
    "    data_test = test.data()\n",
    "    y_test = test.sentiment_data()\n",
    "\n",
    "    # Finding the best C value for the model on this dataset\n",
    "    c_grid_params = {'word_vectors' : [word_vector], 'random_state' : random_state,\n",
    "                     'parsers' : [tweebo]}\n",
    "    if sentiment_lexicon is not None:\n",
    "        c_grid_params['senti_lexicons'] = [sentiment_lexicon]\n",
    "    best_c, c_scores = model.find_best_c(data_train, y_train, \n",
    "                                         grid_params=c_grid_params, cv=5, n_jobs=7)\n",
    "    if c_file_path is not None:\n",
    "        write_json_data(c_file_path, name, c_scores)\n",
    "    if sentiment_lexicon is not None:\n",
    "        print('The best C value for {} model with sentiment lexicon {}: {}'\\\n",
    "              .format(model, sentiment_lexicon, best_c))\n",
    "    else:\n",
    "        print('The best C value for {} model: {}'.format(model, best_c))\n",
    "    \n",
    "    # Fitting and getting predictions from the model.\n",
    "    parameters = {'word_vector' : word_vector, 'random_state' : random_state, \n",
    "                  'C' : best_c, 'parser' : tweebo}\n",
    "    if sentiment_lexicon is not None:\n",
    "        parameters['senti_lexicon'] = sentiment_lexicon\n",
    "    best_params = model.get_params(**parameters)\n",
    "    model.fit(data_train, y_train, params=best_params)\n",
    "    predicted_values = model.predict(data_test)\n",
    "    # Return the results\n",
    "    if result_file_path is not None:\n",
    "        return evaluation_results(predicted_values, test, name, \n",
    "                                  file_name=result_file_path, \n",
    "                                  save_raw_data=save_raw_data, re_write=re_write)\n",
    "    else:\n",
    "        return evaluation_results(predicted_values, test, name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances of the models\n",
    "tdparse = TDParse()\n",
    "tdparse_plus = TDParsePlus()\n",
    "models = [tdparse, tdparse_plus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{TDParse: ({'random_state': 42, 'word_vector': [w2v, sswe]},\n",
       "  '/home/moorea/tdparse/results/TDParse Models/TDParse.tsv',\n",
       "  '/home/moorea/tdparse/results/TDParse Models/TDParse C.json'),\n",
       " TDParse Plus: ({'random_state': 42,\n",
       "   'sentiment_lexicon': <tdparse.lexicons.Lexicon at 0x7f458d588f28>,\n",
       "   'word_vector': [w2v, sswe]},\n",
       "  '/home/moorea/tdparse/results/TDParse Models/TDParsePlus.tsv',\n",
       "  '/home/moorea/tdparse/results/TDParse Models/TDParsePlus C.json')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the result files\n",
    "result_folder = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results', 'TDParse Models'))\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "model_result_files = ['TDParse.tsv', 'TDParsePlus.tsv']\n",
    "model_result_files = [os.path.join(result_folder, result_file) for result_file in model_result_files]\n",
    "C_result_files = ['TDParse C.json', 'TDParsePlus C.json']\n",
    "C_result_files = [os.path.join(result_folder, result_file) for result_file in C_result_files]\n",
    "# Parameters for each model\n",
    "std_model_parameters = {'word_vector' : [w2v, sswe], 'random_state' : 42}\n",
    "senti_model_parameters = {**std_model_parameters, 'sentiment_lexicon' : all_three_low}\n",
    "model_parameters = [std_model_parameters, senti_model_parameters]\n",
    "# Combining parameters and result files\n",
    "parameters_files = list(zip(model_parameters, model_result_files, C_result_files))\n",
    "\n",
    "model_result_files = dict(zip(models, parameters_files))\n",
    "model_result_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset Dong Twitter\n",
      "Processing model TDParse\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time_to_process = time.time()\n",
    "for dataset_name, train_test in train_test.items():\n",
    "    print('Processing dataset {}'.format(dataset_name))\n",
    "    train, test = train_test\n",
    "    for model, parameter_file_paths in model_result_files.items():\n",
    "        print('Processing model {}'.format(model))\n",
    "        parameters, result_file_path, c_file_path = parameter_file_paths\n",
    "        dataset_predictions(train, test, dataset_name, model, \n",
    "                            result_file_path=result_file_path,\n",
    "                            c_file_path=c_file_path,\n",
    "                            re_write=False, save_raw_data=True,\n",
    "                            **parameters)\n",
    "\n",
    "time_to_process = time.time() - time_to_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time it took to process all the datasets {{round(time_to_process / 3600, 2)}} hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
